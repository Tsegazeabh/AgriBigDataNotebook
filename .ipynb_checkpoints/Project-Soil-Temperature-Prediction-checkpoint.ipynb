{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a65a55f-f2df-45b2-9119-859ac1e6b455",
   "metadata": {},
   "source": [
    "# Predicting the Soil Temperature at 100cm depth using machine learning\n",
    "### 1. Problem definition\n",
    "> Soil temperature is one of the important parameters of a soil characteristics that contributes information in understanding the nitrate levels of a soil at different depths. This is because the soil temperature affects the soil's microbial activity, plant uptake, volatilization of Nitrogen compounds and leaching.  NMBU measures soil temperature at different depths (2cm, 5cm, 10cm, 20cm, 50cm, 100cm) using platinum resistance thermometers called PT100. Machine learning based predication may help reducing the effort and installation cost required to measure temperature at deeper soil levels such 100cm. This machine learning algorithm will try to predict the soil temperature at 2cm, 5cm, 10cm, 20, 50 and 100cm from the different data features. \n",
    "### 2. Data Source\n",
    "> To start the analysis process, the data source (01 Sep 2000 - 01 Apr 2024) is collected from the Meteorological data for Ås - BIOKLIM  (https://www.nmbu.no/forskning/grupper/meteorologiske-data) and organized into one big dataset which contains nearly 8856 samples. The improvements will be: first, the dataset will be improved to have bigger number of samples (2000-2024); second, multi-sites will be considererd to make the prediction more generic for different types of soils in Norway.\n",
    "> #### There is one dataset inside the data folder\n",
    "\r",
    "### 3. Evaluation metrics\n",
    "> The evaluation metrics such as R-squared (R²) Score, Mean Absolute Error (MAE), Root Mean Square Error (RMSE) will be considered as common regression metrics.\n",
    "> > The goal of this machine learning model is to build a machine leanrning model that minimizes the erros: MAE, RMSE and increase the R-squared (R²) Score.\n",
    "### 4. Data Features\n",
    "> The original features of the dataset are:  month,\tday, mean_air_temperature_2m, min_air_temperature_2m,\tmax_air_temperature_2m\tsoil_temperature_2cm,\tsoil_temperature_5cm,\tsoil_temperature_10cm,\tsoil_temperature_20cm,\tsoil_temperature_50cm,\trelative_humudity_%,\tair_pressure_2m_mbar,\tradiation_balance_w_m2,\talbedo_RR_GR,\tearth_heat_flux_MJ_m2,\tevaporation_mm,\trainfall_mm,\tsnowfall_cm. But through feature importance analysis less number of features will be used for consideration.\n",
    ">> The original dataset is inside the data folder in this project with a file name 'NMBUBigDatasetFinal.csv'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a21156c7-5306-40f0-940b-218f1d869075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "# Import the train_test_split module from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# For z-score calculations\n",
    "from scipy import stats\n",
    "# Import the GridSearchCV and RandomizeSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import mplcursors\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fc883-3f02-48a1-a039-3938d41d6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/NMBUBigDatasetFinal.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a3a8a-5f2d-4726-ab3c-be51d8073afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade8b8a-ee83-41ce-919b-3d0d3a84c239",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing Stage\n",
    "### Excel manipulation to clean the original dataset\n",
    "#### 1. Duplicate values were removed where one date is repeated.\n",
    "#### 2. Some of the snow_depth_cm data read from sensor are replaced by the manually recorded snow_depth_manual_cm data if manual recording data exists\n",
    "#### 3. Unnecessary columns are removed from the original dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf70261-2dae-482b-ba1c-ded372b466f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['snow_depth_cm'] = dataset['snow_depth_cm'].fillna(dataset['snow_depth_manual_cm'])\n",
    "# dataset.drop('snow_depth_manual_cm', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c56206-9383-41db-9d85-63ca2876dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a year, month and day columns by feature engineering to make manipulation of these easy\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "dataset_copied = dataset.copy()\n",
    "dataset_copied['year'] = dataset_copied['date'].dt.year\n",
    "dataset_copied['month'] = dataset_copied['date'].dt.month\n",
    "dataset_copied['day'] = dataset_copied['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec99664-7b61-4abe-9e8f-ea5b258bfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fa1cc-064e-4d6c-9712-c969c1e0e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a6df8-6323-4919-a063-85ddd54a3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb4329-9e1b-4c08-952f-6c03389207b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e51e98-bd0c-4341-a9d2-40def644e188",
   "metadata": {},
   "source": [
    "### NOTE: There are huge missing values for evaporation_mm= 6392, snow_depth_cm=7272 which hugely affect our data if we randomly impute them. So we need to study why these data are missing to decide on either to impute or drop these features from our dataset. To see if the missing values are evenly distributed throughout the period or are missing at specific instants, let us see the time series trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d0ee6-da82-4f73-b586-2d3b3474091c",
   "metadata": {},
   "source": [
    "#### Draw Evaporation(mm) vs Time to see the trend and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f86e3-bd8a-4d52-bbda-3d789a6b3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the scatter plot of the evaporation witht respect to time\n",
    "# Filter out rows where evaporation is not missing\n",
    "valid_evap_data = dataset_copied.dropna(subset=['evaporation_mm'])\n",
    "\n",
    "# Group the valid data by year\n",
    "# valid_evap_data['Year'] = valid_evap_data['year']\n",
    "grouped_evap_data = valid_evap_data.groupby('year')\n",
    "\n",
    "# Draw the scatter plot of snow_depth_cm vs date\n",
    "def scatter_evaporation_timeseries(dataset_name, file='data/results/evaporation_mm_trend.png', bbox_inches='tight'):\n",
    "    # Determine the number of unique groups (years) to adjust the figure size\n",
    "    num_years = len(dataset_name['year'].unique())\n",
    "    \n",
    "    # Base figure height and additional height per year\n",
    "    base_height = 5\n",
    "    extra_height_per_year = 0.2\n",
    "    \n",
    "    # Adjust figure height based on the number of legend items\n",
    "    fig_height = base_height + extra_height_per_year * num_years\n",
    "    fig, ax = plt.subplots(figsize=(15, fig_height))\n",
    "    \n",
    "    for year, group in dataset_name:\n",
    "        ax.scatter(group['date'], group['evaporation_mm'], label=f'{year} ({len(group)}) points')\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    ax.set( xlabel=\"Date\", ylabel=\"Evaporation (mm)\")\n",
    "    # Set font sizes separately\n",
    "    ax.xaxis.label.set_fontsize(18)\n",
    "    ax.yaxis.label.set_fontsize(18)\n",
    "    # Set font sizes for tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    \n",
    "    # Add legend outside the plot area\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout to make room for the legend\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(file, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "scatter_evaporation_timeseries(grouped_evap_data, 'data/results/evaporation_mm_missing_values.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21947cf5-697c-4197-ad21-1fb23b0106ee",
   "metadata": {},
   "source": [
    "### The above evaporation scatter plot shows there is missing data from 2004 - 2010 and 2019 which are in total 8 years. So, we can see at least 8 years data is fully missing. The majority of the evaporation value range from 0 to 10mm but there are three values which tend to be outliers and can be removed from the dataset. Two of them are in 2018 (12.5mm and 15.8mm) and one is in 2023 (20.6mm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074769ae-ca60-45c2-9a95-2b37db5a8496",
   "metadata": {},
   "source": [
    "#### Remove the evaporation outliers (>10mm) from the dataset and redraw the scatter plot of evaporation timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e7f73-375a-450d-9118-cbc61fb7644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'evaporation_mm' is less than or equal to 10 and re-draw\n",
    "dataset_copied = dataset_copied[(dataset_copied['evaporation_mm'] <= 10) | (pd.isna(dataset_copied['evaporation_mm']))]\n",
    "\n",
    "# Filter out rows where evaporation is not missing\n",
    "valid_evap_data = dataset_copied.dropna(subset=['evaporation_mm'])\n",
    "\n",
    "# Group the valid data by year\n",
    "grouped_evap_data = valid_evap_data.groupby('year')\n",
    "# Redraw the evaporation_mm with time for outliers removed\n",
    "scatter_evaporation_timeseries(grouped_evap_data, 'data/results/evaporation_mm_outliers_removed.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4631e-d56d-41c4-ba47-5c55014ac72b",
   "metadata": {},
   "source": [
    "#### Fill the missing values of evaporation by the average of each same day throughout the years for non-empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e823b-af0a-4f55-948b-de32ec2b1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copied.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ede880-f269-46e3-93ef-8d49af99e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copied.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e50bd5-b266-47f1-a862-8ddddbb8f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean for evaporation_mm considering only non-NaN values for each day and month\n",
    "mean_values = dataset_copied.groupby(['day', 'month'])['evaporation_mm'].apply(lambda x: x.dropna().mean()).reset_index(name='evaporation_mm_mean')\n",
    "\n",
    "# Merge mean_values with the original DataFrame to fill missing values\n",
    "dataset_copied = pd.merge(dataset_copied, mean_values, on=['day', 'month'], how='left')\n",
    "\n",
    "# Fill missing values for evaporation_mm with the mean values calculated for the specific range of years\n",
    "def fill_missing_evaporation(row):\n",
    "    if pd.isna(row['evaporation_mm']):\n",
    "        # Calculate the mean value for the specific range of years\n",
    "        filtered_means = mean_values[(mean_values['day'] == row['day']) & (mean_values['month'] == row['month'])]\n",
    "        mean_value = filtered_means['evaporation_mm_mean'].mean() if not filtered_means.empty else None\n",
    "        return mean_value\n",
    "    else:\n",
    "        return row['evaporation_mm']\n",
    "\n",
    "# Fill missing values for evaporation_mm with the mean values calculated for the specific range of years\n",
    "dataset_copied['evaporation_mm'] = dataset_copied.apply(fill_missing_evaporation, axis=1)\n",
    "# Drop auxiliary columns\n",
    "dataset_copied.drop('evaporation_mm_mean', axis=1, inplace=True)\n",
    "# Drop duplicates in case there are overlapping values from the merge\n",
    "dataset_copied.drop_duplicates(inplace=True)\n",
    "\n",
    "# Redraw the scatter plot for evaporation timeseries\n",
    "# Filter out rows where evaporation is not missing\n",
    "valid_evap_data = dataset_copied.dropna(subset=['evaporation_mm'])\n",
    "\n",
    "# Group the valid data by year\n",
    "grouped_evap_data = valid_evap_data.groupby('year')\n",
    "\n",
    "# Draw the scatter plot of snow_depth_cm vs date\n",
    "scatter_evaporation_timeseries(grouped_evap_data, 'data/results/evaporation_mm_missing_values_filled.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c93d83-a99f-4bd4-9f13-444107f81f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copied.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c75bc-de86-4237-a1af-0649bb873c00",
   "metadata": {},
   "source": [
    "#### Let's count and draw the missing values based on monthes to see if the evaporation is missing for winter seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9a19f-9f11-4731-b301-f517a09ab332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to select rows where evaporation_mm is NaN\n",
    "nan_evaporation = dataset_copied[dataset_copied['evaporation_mm'].isna()]\n",
    "\n",
    "# Group by month and count NaN occurrenc\n",
    "nan_evaporation_counts = nan_evaporation.groupby('month').size()\n",
    "\n",
    "# Define the plot\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "# Plotting the bar graph\n",
    "if (nan_evaporation_counts.any()):\n",
    "    nan_evaporation_counts.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Annotating each bar with its count value\n",
    "for i, count in enumerate(nan_evaporation_counts):\n",
    "    ax.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Count of Missing Evaporation')\n",
    "# ax.set_title('Count of Missing Evaporation by Month')\n",
    "# Set X-axis tick labels to be horizontal\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "# Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig('data/results/evaporation_missing_count_by_month.png', bbox_inches='tight')  # Save as PNG format\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6e72b-5d9c-48b2-ba31-9a0353ee478b",
   "metadata": {},
   "source": [
    "#### The above bargraph shows the missing evaporation measurements are mostly from January - April except few in May. This could be due to the winter season that evaporation is less likely. So, we can fill them all by zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397afc09-f2ee-408d-9dbd-5e63c370ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing evaporation_mm values by zero\n",
    "dataset_copied['evaporation_mm'] = dataset_copied['evaporation_mm'].fillna(0.0)\n",
    "dataset_copied.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fff30b-895a-477a-9a32-01420902aa76",
   "metadata": {},
   "source": [
    "### Plot the evaporation data after removing all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62889312-bc6a-4705-9449-5820bde052f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redraw the scatter plot for evaporation timeseries\n",
    "# Filter out rows where evaporation is not missing\n",
    "valid_evap_data_filled = dataset_copied.dropna(subset=['evaporation_mm'])\n",
    "\n",
    "# Group the valid data by year\n",
    "grouped_evap_data_filled = valid_evap_data_filled.groupby('year')\n",
    "\n",
    "# Draw the scatter plot of snow_depth_cm vs date\n",
    "scatter_evaporation_timeseries(grouped_evap_data_filled, 'data/results/evaporation_mm_all_missing_values_filled.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c8730-8599-4186-95c3-b7c6413aab48",
   "metadata": {},
   "source": [
    "#### Draw Snow Depth(cm) vs Time to see the trend and missing values with time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279957f7-73dd-446d-a2a7-7f157a79398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_snow_depth_timeseries(dataset_name, file='data/results/snow_depth_time_series.png', bbox_inches='tight'):\n",
    "    # Draw the scatter plot of the evaporation witht respect to time\n",
    "    # Determine the number of unique groups (years) to adjust the figure size\n",
    "    num_years = len(dataset_name['year'].unique())\n",
    "    \n",
    "    # Base figure height and additional height per year\n",
    "    base_height = 5\n",
    "    extra_height_per_year = 0.2\n",
    "    \n",
    "    # Adjust figure height based on the number of legend items\n",
    "    fig_height = base_height + extra_height_per_year * num_years\n",
    "    fig, ax = plt.subplots(figsize=(15, fig_height))\n",
    "    # Filter out rows where snow_depth_cm is not missing\n",
    "    valid_snow_data = dataset_name.dropna(subset=['snow_depth_cm'])\n",
    "    \n",
    "    # Group the valid data by year\n",
    "    # valid_snow_data['Year'] = valid_snow_data['date'].dt.year\n",
    "    grouped_snow_data = valid_snow_data.groupby('year')\n",
    "    \n",
    "    for year, group in grouped_snow_data:\n",
    "        ax.scatter(group['date'], group['snow_depth_cm'], label=f'{year} ({len(group)}) points')\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    ax.set(xlabel=\"Date\", ylabel=\"Snow Depth (cm)\")\n",
    "    # # Set font sizes separately\n",
    "    # ax.title.set_fontsize(15)\n",
    "    # ax.xaxis.label.set_fontsize(14)\n",
    "    # ax.yaxis.label.set_fontsize(14)\n",
    "    # # Set font sizes for tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    # Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "    plt.savefig(file)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "scatter_snow_depth_timeseries(dataset_copied, 'data/results/snow_depth_time_series_missing_values.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33109fc6-ec50-48f9-bb0a-3d17f8010929",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copied['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17d84c-e3ef-455b-8941-17bcc9645952",
   "metadata": {},
   "source": [
    "### The above snow depth scatter plot shows that snow data is completely missing from 2000-2015 and 2019. The daily snow depth ranges from 0 to about 50 cm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa292e-071f-4b72-a980-798f87a81af1",
   "metadata": {},
   "source": [
    "### Some of the surface snow thickness values missing from the dataset are filled from the data gathered from MET Norway Frost API from 10/01/2016 - 29/03/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c72b47-f031-4508-98fb-2405f5179f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes based on the 'date' column\n",
    "snow_thickness = pd.read_csv('data/surface_snow_daily_2016_2024.csv')\n",
    "snow_thickness['date'] = pd.to_datetime(snow_thickness['date'])\n",
    "# Replace all values of snow_thickness['snow_depth_cm'] equal to -1 with 0 because -1 in MET Frost API indicates it is a very low or zero snow depth\n",
    "snow_thickness.loc[snow_thickness['snow_depth_cm'] == -1, 'snow_depth_cm'] = 0\n",
    "\n",
    "# Merge the two dataframes based on the 'date' column\n",
    "merged_df = pd.merge(dataset_copied, snow_thickness, on='date', suffixes=('_copied', '_thickness'), how='left')\n",
    "\n",
    "# Copy values from 'snow_depth_cm_thickness' to 'snow_depth_cm_copied' where 'snow_depth_cm_copied' is NaN\n",
    "merged_df['snow_depth_cm_copied'] = merged_df['snow_depth_cm_copied'].fillna(merged_df['snow_depth_cm_thickness'])\n",
    "\n",
    "# Drop the 'snow_depth_cm_thickness' column\n",
    "merged_df.drop(['snow_depth_cm_thickness','index','elementId','sourceId'], axis=1, inplace=True)\n",
    "\n",
    "# If needed, you can rename the 'snow_depth_cm_copied' column back to 'snow_depth_cm'\n",
    "merged_df.rename(columns={'snow_depth_cm_copied': 'snow_depth_cm'}, inplace=True)\n",
    "\n",
    "# Now, merged_df contains the updated snow depth values in the dataset_copied dataframe\n",
    "\n",
    "# To overwrite the original dataset_copied with the updated values:\n",
    "dataset_copied = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04692931-dce0-4c16-a5a7-f51aff81f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['albedo_RR_GR'] < 0]['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f996a-f7be-437d-a87c-d4ed6ee8ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['ST2'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369cbab-3252-4376-96e8-9be07320e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copied.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30e49b-234f-4552-9132-1e27e21ee228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redraw the snow depth after some fillna\n",
    "scatter_snow_depth_timeseries(dataset_copied, 'data/results/snow_depth_missing_values_filled_from_MET.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669f61d-ea19-4ee1-86a9-81bbf36f7b08",
   "metadata": {},
   "source": [
    "### Let's count and draw the missing values from 2016 - 2024 based on months to see if the snow depth is missing for non-winter seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5464771-fb7e-402b-ac67-2a8230a97a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to select rows for years 2016 to 2024 because the snow depth before 2016 is completely missing.\n",
    "filtered_data = dataset_copied[(dataset_copied['year'] >= 2016) & (dataset_copied['year'] <= 2024)]\n",
    "\n",
    "# Further filter the dataframe to select rows where snow_depth_cm is NaN\n",
    "nan_snow_depth = filtered_data[filtered_data['snow_depth_cm'].isna()]\n",
    "\n",
    "# Group by year and month and count NaN occurrences\n",
    "nan_snow_depth_counts = nan_snow_depth.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "\n",
    "# Sum the counts across years to get the total missing values for each month\n",
    "nan_snow_depth_counts_by_month = nan_snow_depth_counts.sum(axis=0)\n",
    "\n",
    "# Define the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plotting the bar graph\n",
    "nan_snow_depth_counts_by_month.plot(kind='bar', ax=ax)\n",
    "\n",
    "# Annotating each bar with its count value\n",
    "for i, count in enumerate(nan_snow_depth_counts_by_month):\n",
    "    ax.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Count of Missing Snow Depth')\n",
    "# ax.set_title('Count of Missing Snow Depth by Month (2016-2024)')\n",
    "# Set X-axis tick labels to be horizontal\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "# Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig('data/results/snow_depth_missing_count_by_month.png', bbox_inches='tight')  # Save as PNG format\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978f55a-cf7b-4f51-a781-fe4089f44bb9",
   "metadata": {},
   "source": [
    "### Define a helper function that is used to do the following:\n",
    "##### 1. Fill missing values for each month of the year with the mean value of that month for the same year.\n",
    "##### 2. For months where the mean value is NaN (i.e., all values for that month in the same year are NaN), fill missing values with the mean value of that month across all other years. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5caae-d00e-4e27-ba0b-4b345405730e",
   "metadata": {},
   "source": [
    "### Define helper function that takes mean value of the same day and month across all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca906f27-64c4-4aee-ba6a-545b2410597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic function that is used for filling missing values different columns\n",
    "def fill_missing_values_by_mean_day(row_data, dataset_passed, column_name):\n",
    "    \"\"\"\n",
    "    Fill missing values for a specified column based on conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        row_data (pandas.Series): A single row_data of the DataFrame.\n",
    "        dataset (pandas.DataFrame): The DataFrame containing the dataset.\n",
    "        column_name (str): The name of the column to fill missing values for.\n",
    "    \n",
    "    Returns:\n",
    "        float: The filled value for the specified column.\n",
    "    \"\"\"\n",
    "    if pd.isna(row_data[column_name]):\n",
    "        # Extract year and month from the current row_data\n",
    "        year = row_data['year']\n",
    "        month = row_data['month']\n",
    "        day = row_data['day']\n",
    "        \n",
    "        # Check if there are any non-NaN values for the same month and year\n",
    "        same_month_day = dataset_passed[(dataset_passed['day'] == day) & (dataset_passed['month'] == month)]\n",
    "        valid_day_values = same_month_day.dropna(subset=[column_name])\n",
    "        # Check if there are any non-NaN values for the same month and year\n",
    "        same_month_year = dataset_passed[(dataset_passed['year'] == year) & (dataset_passed['month'] == month)]\n",
    "        valid_month_values = same_month_year.dropna(subset=[column_name])\n",
    "        \n",
    "        if not valid_day_values.empty:\n",
    "            # Calculate the mean of non-NaN values for the same month and year\n",
    "            mean_values = valid_day_values[column_name].mean()\n",
    "        elif not valid_month_values.empty:            \n",
    "            mean_values = valid_month_values[column_name].mean()\n",
    "        else:\n",
    "            # Calculate the mean of non-NaN values for the same month in other years\n",
    "            other_years = dataset_passed[(dataset_passed['month'] == month)]\n",
    "            other_years_valid_value = other_years.dropna(subset=[column_name])\n",
    "            mean_values = other_years_valid_value[column_name].mean()\n",
    "        \n",
    "        return mean_values\n",
    "    else:\n",
    "        # If the value is not NaN, return the original value\n",
    "        return row_data[column_name]\n",
    "\n",
    "def fill_missing_by_daily_mean(column_name, dataset_passed):\n",
    "    dataset_passed[column_name] = dataset_passed.apply(lambda row_data: fill_missing_values_by_mean_day(row_data, dataset_passed, column_name), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1036f73-d845-4b4f-a13c-e055f283688f",
   "metadata": {},
   "source": [
    "### Fill missing values of all features using the daily mean above helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a55d2-104a-46d8-ba56-192951543949",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fill the missing values for all features by the monthly mean of that specific year or mean value of that month across all other years if the month of that specific year is NaN\n",
    "fill_missing_by_daily_mean(\"evaporation_mm\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"ST2\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"ST5\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"ST10\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"ST20\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"ST50\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"ST100\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"relative_humidity\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"air_pressure_2m_mbar\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"radiation_balance_w_m2\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"albedo_RR_GR\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"earth_heat_flux_MJ_m2\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"precipitation_mm\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"snow_depth_cm\", dataset_copied)\n",
    "fill_missing_by_daily_mean(\"phosynthetic_active_radiation_mE_m2\", dataset_copied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14feedf9-c261-4175-a1a9-62f28c465334",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copied.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7845b-00cc-409f-a93d-6670ef4f5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_snow_depth_timeseries(dataset_copied, 'data/results/snow_depth_with_all_missing_values_filled.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917db10-0d16-45ed-bb3a-72041418b057",
   "metadata": {},
   "source": [
    "### Soil temperatures with respect to time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5070b48-c9f5-4261-8833-3f1b45cb1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the list of columns you want to plot\n",
    "columns = [\"ST2\", \"ST5\", \"ST10\", \"ST20\", \"ST50\", \"ST100\"]\n",
    "\n",
    "# Create a figure and a set of subplots with 2 rows and 3 columns\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten the 2D array of axes for easy iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each column in a separate subplot\n",
    "for ax, column in zip(axs, columns):\n",
    "    ax.plot(dataset[\"date\"], dataset[column])\n",
    "    ax.set_title(f\"({column})\", fontsize=16)\n",
    "    ax.set_xlabel(\"Date\", fontsize=14)\n",
    "    ax.set_ylabel(f\"{column} Soil Temperature (°C)\", fontsize=14)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig('data/results/soil_temperature_time_trend_grid.png', bbox_inches='tight')  # Save as PNG format\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f032b-a8ea-4d93-a7e4-1dfb937957d6",
   "metadata": {},
   "source": [
    "#### The time series scatter chart shows that there is similar trend of increase and decrease every year except few times like at around 2016, it showed discontinuous nature  which is due to missing measurement errors. Overall, the dataset shows us that our data is a real timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eede69-dddb-4639-a80e-d0d4e61c8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize =(10,5))\n",
    "ax.hist(dataset[\"ST100\"]);\n",
    "ax.set(title=\"Soil Temperature (100cm) Histogram\", xlabel='Soil Temp (°C)', ylabel=\"Frequency\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44844e-a611-4ee5-beb4-b63f8072879e",
   "metadata": {},
   "source": [
    "### Let's see the scatter plots of the the original values of the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18e23b-2559-4292-9889-24b060cfc76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features list from the dataset columns\n",
    "features = dataset_copied.columns.tolist()\n",
    "# remove the ST100 feature from the features list\n",
    "features.remove('ST100')\n",
    "# reomve the date feature from the features list\n",
    "features.remove('date')\n",
    "# reomve the year feature from the features list\n",
    "features.remove('year')\n",
    "# remove the month feature from the features list\n",
    "features.remove('month')\n",
    "# reomve the day feature from the features list\n",
    "features.remove('day')\n",
    "# reomve the ID from the features list\n",
    "features.remove('ID')\n",
    "\n",
    "# Extract the 'ST100' series\n",
    "original_date = dataset_copied['date']\n",
    "\n",
    "# Extract the important features\n",
    "original_df = dataset_copied[features]\n",
    "\n",
    "# Calculate the number of rows needed for the subplot grid\n",
    "num_features = len(features)\n",
    "num_cols = 2  # Number of columns for subplots\n",
    "num_rows = (num_features + num_cols - 1) // num_cols\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 6 * num_rows))\n",
    "\n",
    "# Plot z-score normalized features against z-score normalized 'ST100' individually\n",
    "for i, feature in enumerate(features):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    ax = axs[row, col]\n",
    "        \n",
    "    ax.scatter(original_date, original_df[feature])\n",
    "    ax.set_title(f'Original {feature} vs. Original ST100', fontsize=16)\n",
    "    ax.set_xlabel('Original ST100', fontsize=16)\n",
    "    ax.set_ylabel(f'Original {feature}', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide any empty subplots\n",
    "for i in range(num_features, num_rows * num_cols):\n",
    "    axs.flatten()[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST100_vs_other_parameters.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd13cd5-4dfc-4aa1-bd2f-d7f0bdc2a81a",
   "metadata": {},
   "source": [
    "### Let's check the outliers by normalizing our features using the z-score method seeing the trend on the scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae5644-1fde-4de1-bfe9-36422d360885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the features list from the dataset columns\n",
    "features = dataset_copied.columns.tolist()\n",
    "# reomve the date feature from the features list\n",
    "features.remove('date')\n",
    "# reomve the year feature from the features list\n",
    "features.remove('year')\n",
    "# remove the month feature from the features list\n",
    "features.remove('month')\n",
    "# reomve the day feature from the features list\n",
    "features.remove('day')\n",
    "# reomve the ID from the features list\n",
    "features.remove('ID')\n",
    "\n",
    "# Calculate Z-score for the target variable 'ST100'\n",
    "# zscore_ST100 = (dataset_copied['ST100'] - dataset_copied['ST100'].mean()) / dataset_copied['ST100'].std()\n",
    "# Date values\n",
    "zscore_date = dataset_copied['date']\n",
    "\n",
    "# Calculate Z-score for all features\n",
    "zscore_df = (dataset_copied[features] - dataset_copied[features].mean()) / dataset_copied[features].std()\n",
    "\n",
    "# Calculate the number of rows needed for the subplot grid\n",
    "num_features = len(features)\n",
    "num_cols = 2  # Number of columns for subplots\n",
    "num_rows = (num_features + num_cols - 1) // num_cols\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 6 * num_rows))\n",
    "\n",
    "# Plot z-score normalized features against z-score normalized 'ST100' individually\n",
    "for i, feature in enumerate(features):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    ax = axs[row, col]\n",
    "        \n",
    "    # ax.scatter(zscore_ST100, zscore_df[feature])\n",
    "    ax.scatter(zscore_date, zscore_df[feature])\n",
    "    # ax.set_title(f'Z-score Normalized {feature} vs. Z-score Normalized ST100')\n",
    "    ax.set_title(f'({feature})', fontsize=14)\n",
    "    # ax.set_xlabel('Z-score Normalized ST100')\n",
    "    ax.set_xlabel('Date', fontsize=14)\n",
    "    ax.set_ylabel(f'{feature}(Normalized)', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "    ax.grid(True)\n",
    "    # Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig(f'data/results/zscore_normalized_all_features_time_series_trend_grid.png', bbox_inches='tight')  # Save as PNG format\n",
    "\n",
    "# Hide any empty subplots\n",
    "for i in range(num_features, num_rows * num_cols):\n",
    "    axs.flatten()[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f89129-0ca8-49c6-9e01-22923d66e078",
   "metadata": {},
   "source": [
    "### The scatter plots above show that there are outliers for some of the features. To remove the outliers, let's first normalize our dataset values to create new normalized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e5987-4831-45f4-a88c-d718c797dbaf",
   "metadata": {},
   "source": [
    "### Define a custom scatter_plot function that takes the features and dataset and plots one feature against the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1acf5-1da5-4241-afe0-e60de923b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(features, df, parameter):\n",
    "    # Calculate the number of rows needed for the subplot grid\n",
    "    num_features = len(features)\n",
    "    num_cols = 2  # Number of columns for subplots\n",
    "    num_rows = (num_features + num_cols - 1) // num_cols\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 6 * num_rows))\n",
    "    \n",
    "    # Plot z-score normalized features against z-score normalized 'ST100' individually\n",
    "    for i, feature in enumerate(features):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axs[row, col]\n",
    "        \n",
    "        ax.scatter(df[parameter], df[feature])\n",
    "        ax.set_title(f'({feature})', fontsize=14)\n",
    "        ax.set_xlabel(f'{parameter}', fontsize=14)\n",
    "        ax.set_ylabel(f'{feature}(Normalized)', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Hide any empty subplots\n",
    "    for i in range(num_features, num_rows * num_cols):\n",
    "        axs.flatten()[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/results/scatter_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72edaf-d3ae-4618-9406-f876037e6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features list from the dataset columns\n",
    "features = dataset_copied.columns.tolist()\n",
    "\n",
    "# Create a local copy of the dataset and drop the date and year features\n",
    "dataset_denormalized_original = dataset_copied.drop(['ID','date', 'year'], axis=1)\n",
    "\n",
    "# Temporary reomve the ID from the features list\n",
    "features.remove('ID')\n",
    "# reomve the date feature from the features list\n",
    "features.remove('date')\n",
    "# reomve the year feature from the features list\n",
    "features.remove('year')\n",
    "\n",
    "# Keep the original dataset's means of each feature for later use in denormalization\n",
    "mean_original = dataset_denormalized_original[features].mean()\n",
    "# Keep the original dataset's standard deviationa of each feature for later use in denormalization\n",
    "std_original = dataset_denormalized_original[features].std()\n",
    "# Calculate Z-score for all features\n",
    "zscore_df = (dataset_denormalized_original[features] - mean_original) / std_original\n",
    "\n",
    "# Create a new DataFrame to store the normalized values\n",
    "dataset_normalized = zscore_df.copy()\n",
    "\n",
    "# copy the ID column from the original dataset to the dataset_denormalized\n",
    "dataset_denormalized_original['ID'] = dataset_copied['ID']\n",
    "# copy the ID column from the original dataset to the dataset_normalized\n",
    "dataset_normalized['ID'] = dataset_copied['ID']\n",
    "dataset_normalized['date'] = dataset_copied['date']\n",
    "\n",
    "dataset_normalized.to_csv('data/dataset_normalized.csv', index=False)\n",
    "dataset_denormalized_original.to_csv('data/dataset_denormalized_original.csv', index=False)\n",
    "\n",
    "# remove the month feature from the features list\n",
    "features.remove('month')\n",
    "# reomve the day feature from the features list\n",
    "features.remove('day')\n",
    "# Call the custom built scatter plot for multiple features\n",
    "scatter_plot(features, dataset_normalized, 'date')\n",
    "# Remove the date column after plotting because it is not used in the modelling\n",
    "dataset_normalized.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb746d98-83ee-45d4-82b4-ee404278dbe7",
   "metadata": {},
   "source": [
    "### Let's check any outliers on our dataset using statistical Z-score and Interquartile Range (IQR) techniques for all the features\n",
    "#### 1. Z-score: Calculate the z-score for each data point, which represents how many standard deviations it is away from the mean. Data points with a z-score beyond a certain threshold can be considered outliers.\n",
    "#### 2. IQR: IQR is the range between the first quartile (Q1) and the third quartile (Q3). Data points outside the range (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR) are typically considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb89197-db71-42e1-bd63-97753b0a25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978d7e5-69bf-4bf6-8537-d381d51d053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d0d03a-08b3-4891-9bec-527b45e10d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized['snow_depth_cm'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b8f41-76a8-448b-8522-bf32d509b8fb",
   "metadata": {},
   "source": [
    "#### Extract the outliers and histplot of each feature in the dataset_normalized\n",
    "##### 1. Descriptive outlier detection is used to determine if there is a problem with the data, for example, if the data is not normally distributed.\n",
    "##### 2. Prescriptive outlier detection is used to determine if a specific action needs to be taken\n",
    "#### Z-Score is a measure of how many standard deviations a data point is away from the mean. Typically, data points with a Z-Score greater than a threshold are considered outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2cceda-4e50-48b7-b4e6-f5dabb934f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of features from our dataset columns\n",
    "features = dataset_normalized.columns.tolist()\n",
    "\n",
    "# remove the month feature from the features list\n",
    "features.remove('month')\n",
    "# remove the day feature from the features list\n",
    "features.remove('day')\n",
    "# remove the ID from the features list\n",
    "features.remove('ID')\n",
    "\n",
    "# Define different Z-score thresholds for each feature\n",
    "zscore_thresholds = {\n",
    "    'mean_air_temperature_2m': (-4, 3),\n",
    "    'min_air_temperature_2m': (-4, 3),\n",
    "    'max_air_temperature_2m': (-4, 3),\n",
    "    'relative_humidity': (-4, 2),\n",
    "    'air_pressure_2m_mbar': (-5, 5),\n",
    "    'precipitation_mm': (-1, 8),\n",
    "    'evaporation_mm': (-1, 4),\n",
    "    'earth_heat_flux_MJ_m2': (-5, 5),\n",
    "    'ST2': (-2, 2.5),\n",
    "    'ST5': (-2, 2.5),\n",
    "    'ST10': (-2, 2.5),\n",
    "    'ST20': (-2, 2.5),\n",
    "    'ST50': (-2, 2.5),\n",
    "    'radiation_balance_w_m2': (-2, 3),\n",
    "    'phosynthetic_active_radiation_mE_m2': (-1.5, 3),\n",
    "    'albedo_RR_GR': (-15, 5),\n",
    "    'snow_depth_cm': (-1, 12),\n",
    "    'ST100': (-2, 2.5)\n",
    "}\n",
    "\n",
    "# Plot histograms of Z-scores for all features\n",
    "num_features = len(features)\n",
    "num_cols = 2\n",
    "num_rows = (num_features + num_cols - 1) // num_cols\n",
    "plt.figure(figsize=(15, 5 * num_rows))\n",
    "for i, feature in enumerate(features, start=1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.histplot(zscore_df[feature], kde=True)\n",
    "    plt.title('Z-score distribution')\n",
    "    plt.xlabel(f'Z-score of {feature}')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify outliers based on Z-score for all features using different thresholds\n",
    "outliers_zscore_dict = {}\n",
    "for feature in features:\n",
    "    lower_threshold, upper_threshold = zscore_thresholds.get(feature)  \n",
    "    if upper_threshold is None or lower_threshold is None:\n",
    "        continue\n",
    "    outliers_zscore = dataset_normalized[(dataset_normalized[feature]> upper_threshold) | (dataset_normalized[feature] < lower_threshold)]\n",
    "    if not outliers_zscore.empty:\n",
    "        outliers_zscore_dict[feature] = outliers_zscore[feature].tolist()\n",
    "\n",
    "# Calculate IQR for all features\n",
    "Q1 = dataset_normalized[features].quantile(0.25)\n",
    "Q3 = dataset_normalized[features].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identify outliers based on IQR for all features\n",
    "outliers_iqr_dict = {}\n",
    "for feature in features:\n",
    "    outliers_iqr = dataset_normalized[(dataset_normalized[feature] < Q1[feature] - 1.5 * IQR[feature]) | \n",
    "                                               (dataset_normalized[feature] > Q3[feature] + 1.5 * IQR[feature])]\n",
    "    if not outliers_iqr.empty:\n",
    "        outliers_iqr_dict[feature] = outliers_iqr[feature].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e4dcb-b902-4057-982b-99a8a6341490",
   "metadata": {},
   "source": [
    "#### Remove the outliers for each feature and generate the filtered new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82576fa-e673-4ce6-9622-9d7b88ee5a0c",
   "metadata": {},
   "source": [
    "### Compare the difference between the original dataset and the copied dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315167b-b61b-413d-b9c5-f44da582b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'ID' is the identifier column\n",
    "id_column = 'ID'\n",
    "\n",
    "# Find IDs in dataset but not in dataset_denormalized_original\n",
    "ids_in_dataset_not_in_denormalized = set(dataset[id_column]) - set(dataset_copied[id_column])\n",
    "\n",
    "# Find IDs in dataset_denormalized_original but not in dataset\n",
    "ids_in_denormalized_not_in_dataset = set(dataset_copied[id_column]) - set(dataset[id_column])\n",
    "\n",
    "# Extract rows with these IDs\n",
    "rows_in_dataset_not_in_denormalized = dataset[dataset[id_column].isin(ids_in_dataset_not_in_denormalized)]\n",
    "rows_in_denormalized_not_in_dataset = dataset_copied[dataset_copied[id_column].isin(ids_in_denormalized_not_in_dataset)]\n",
    "\n",
    "print(\"Rows in dataset but not in dataset_denormalized_original:\")\n",
    "print(rows_in_dataset_not_in_denormalized)\n",
    "\n",
    "print(\"Rows in dataset_denormalized_original but not in dataset:\")\n",
    "print(rows_in_denormalized_not_in_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cf5b3-2aef-4338-821b-4c8fd15d3e89",
   "metadata": {},
   "source": [
    "### Generate and save the Normalized and Denormalized Datasets Outlier Filtered By Z-score and whisker thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992ab4c-94e3-4244-9b34-bb79bfb74c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deifne the function to remove the outliers from the dataset\n",
    "def remove_outliers(df, thresholds):\n",
    "    \"\"\"\n",
    "    Removes outliers from a DataFrame based on z-score thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame (already z-score normalized).\n",
    "        thresholds (dict): A dictionary containing feature names as keys and (lower, upper) z-score thresholds as values.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    for feature, (lower, upper) in thresholds.items():\n",
    "        df = df[(df[feature] >= lower) & (df[feature] <= upper)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# The thresholds of the DataFrame normalized 'dataset_normalized' (already z-score normalized)\n",
    "zscore_thresholds = {\n",
    "    'mean_air_temperature_2m': (-4, 3),\n",
    "    'min_air_temperature_2m': (-4, 3),\n",
    "    'max_air_temperature_2m': (-4, 3),\n",
    "    'relative_humidity': (-4, 2),\n",
    "    'air_pressure_2m_mbar': (-5, 5),\n",
    "    'precipitation_mm': (-1, 4),\n",
    "    'evaporation_mm': (-1, 3),\n",
    "    'earth_heat_flux_MJ_m2': (-5, 5),\n",
    "    'ST2': (-2, 2.5),\n",
    "    'ST5': (-2, 2.5),\n",
    "    'ST10': (-2, 2.5),\n",
    "    'ST20': (-2, 2.5),\n",
    "    'ST50': (-2, 2.5),\n",
    "    'radiation_balance_w_m2': (-2, 3),\n",
    "    'phosynthetic_active_radiation_mE_m2': (-1.5, 3),\n",
    "    'albedo_RR_GR': (-2.5, 2.5),\n",
    "    'snow_depth_cm': (-1, 2),\n",
    "    'ST100': (-2, 2.5)\n",
    "}\n",
    "# Remove the outliers and generate a new normalized filtered dataset without outliers\n",
    "# Step 1: Remove Outliers using Z-score Method\n",
    "dataset_normalized_outlier_filtered = remove_outliers(dataset_normalized, zscore_thresholds)\n",
    "dataset_normalized_outlier_filtered.to_csv('data/dataset_normalized_outlier_filtered.csv', index=False)\n",
    "\n",
    "features = dataset_normalized_outlier_filtered.columns.tolist()\n",
    "# Temporary reomve the ID from the features list\n",
    "features.remove('ID')\n",
    "# Generate denormalized filtered dataset equivalent to the normalized filtered\n",
    "dataset_denormalized_outlier_filtered = (dataset_normalized_outlier_filtered[features] * std_original) + mean_original\n",
    "dataset_denormalized_outlier_filtered['ID'] = dataset_normalized_outlier_filtered['ID']\n",
    "\n",
    "# Step 2: Remove Outliers using Whisker outliers (IQR Method)\n",
    "def remove_outliers_iqr(df, excluded_columns=None):\n",
    "    # Copy the DataFrame\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Exclude specified columns if provided\n",
    "    if excluded_columns:\n",
    "        df_filtered = df_filtered.drop(columns=excluded_columns)\n",
    "    \n",
    "    # Iterate through each remaining column\n",
    "    for column in df_filtered.columns:\n",
    "        Q1 = df_filtered[column].quantile(0.25)\n",
    "        Q3 = df_filtered[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Ensure the boolean indexing uses the DataFrame's index\n",
    "        df_filtered = df_filtered[(df_filtered[column] >= lower_bound) & (df_filtered[column] <= upper_bound)]\n",
    "    \n",
    "    return df_filtered\n",
    "# Specify columns to exclude from outlier detection\n",
    "excluded_columns = ['month', 'day', 'ID']\n",
    "# Remove outliers using the modified function\n",
    "dataset_denormalized_zscore_and_whisker_filtered = remove_outliers_iqr(dataset_denormalized_outlier_filtered, excluded_columns)\n",
    "# Merge excluded columns back to the filtered DataFrame using the corresponding indices\n",
    "dataset_denormalized_zscore_and_whisker_filtered = pd.merge(dataset_denormalized_zscore_and_whisker_filtered, dataset_denormalized_outlier_filtered[excluded_columns], left_index=True, right_index=True)\n",
    "\n",
    "# Plot the scatter plot after outlier filtered\n",
    "df_normalized_outlier_filtered = dataset_normalized_outlier_filtered.copy()\n",
    "df_normalized_outlier_filtered = pd.merge(df_normalized_outlier_filtered, dataset_copied[['ID', 'date']], on='ID', how='left')\n",
    "# Call the custom built scatter plot for multiple features\n",
    "scatter_plot(features, df_normalized_outlier_filtered, 'date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8eb369-60e9-43fd-a8ba-9783e3298694",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_denormalized_zscore_and_whisker_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72dadf-8b54-4ac4-acb8-34c55eb8ee29",
   "metadata": {},
   "source": [
    "### Time series plot of soil temperatures after z-score normalization and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06694b85-2e26-4ded-b3db-8ac9e6b9b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the list of columns you want to plot\n",
    "columns = [\"ST2\", \"ST5\", \"ST10\", \"ST20\", \"ST50\", \"ST100\"]\n",
    "\n",
    "# Create a figure and a set of subplots with 2 rows and 3 columns\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(17, 10), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten the 2D array of axes for easy iteration\n",
    "axs = axs.flatten()\n",
    "merged_df = dataset_denormalized_outlier_filtered.copy()\n",
    "merged_df = pd.merge(merged_df, dataset_copied[['ID', 'date']], on='ID', how='left')\n",
    "# Plot each column in a separate subplot\n",
    "for ax, column in zip(axs, columns):\n",
    "    ax.plot(merged_df[\"date\"], merged_df[column])\n",
    "    ax.set_title(f\"({column})\", fontsize=16)\n",
    "    ax.set_xlabel(\"Date\", fontsize=14)\n",
    "    ax.set_ylabel(f\"{column} Soil Temperature (°C)\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=14)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig('data/results/soil_temperature_time_trend_grid_denormalized_filtered.png', bbox_inches='tight')  # Save as PNG format\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "print('Max ST2: ',merged_df['ST2'].max())\n",
    "print('Min ST2: ',merged_df['ST2'].min())\n",
    "print('Max ST5: ',merged_df['ST5'].max())\n",
    "print('Min ST5: ',merged_df['ST5'].min())\n",
    "print('Max ST10: ',merged_df['ST10'].max())\n",
    "print('Min ST10: ',merged_df['ST10'].min())\n",
    "print('Max ST20: ',merged_df['ST20'].max())\n",
    "print('Min ST20: ',merged_df['ST20'].min())\n",
    "print('Max ST50: ',merged_df['ST20'].max())\n",
    "print('Min ST50: ',merged_df['ST50'].min())\n",
    "print('Max ST100: ',merged_df['ST100'].max())\n",
    "print('Min ST100: ',merged_df['ST100'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59dd6f-fc95-4c9d-a36b-28f3b8b7a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c2a51-e8d9-473a-a143-6f49fccd2d3a",
   "metadata": {},
   "source": [
    "## Multicollinearity analysis using correlation and covariance matrices using Pearson Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d86786-1d63-43c8-b985-77018a7a54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the covariance matrix for target ST100\n",
    "dataset_correlation = dataset_denormalized_outlier_filtered.drop(['ID', 'ST100'], axis=1)\n",
    "covariance_matrix = dataset_correlation.cov()\n",
    "print(\"Covariance Matrix:\")\n",
    "print(covariance_matrix)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = dataset_correlation.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/denormalized_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Set the threshold\n",
    "threshold = 0.9\n",
    "# Find pairs of features with correlation above the threshold\n",
    "highly_correlated = np.where(np.abs(correlation_matrix) > threshold)\n",
    "highly_correlated_pairs = [(correlation_matrix.index[x], correlation_matrix.columns[y]) \n",
    "                           for x, y in zip(*highly_correlated) if x != y and x < y]\n",
    "\n",
    "print(\"Highly correlated pairs (above threshold):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)\n",
    "# Example: Removing one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for pair in highly_correlated_pairs:\n",
    "    features_to_remove.add(pair[1])  # You can choose to remove pair[0] or pair[1]\n",
    "\n",
    "# Drop the features from the dataframe\n",
    "dataset_denormalized_outlier_filtered_reduced = dataset_denormalized_outlier_filtered.drop(columns=features_to_remove)\n",
    "\n",
    "print(f\"Removed features: {features_to_remove}\")\n",
    "print(\"Shape of the reduced dataset:\", dataset_denormalized_outlier_filtered_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49257db0-ea63-47a6-8a6a-86c42f13ac53",
   "metadata": {},
   "source": [
    "## Multicollinearity analysis using variance inflation factor (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46607e96-1e2a-4e93-8018-d25db25b3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Assuming dataset_denormalized_outlier_filtered is your DataFrame\n",
    "X_collinear = dataset_denormalized_outlier_filtered.copy()\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X_collinear = sm.add_constant(X_collinear)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_collinear.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_collinear.values, i) for i in range(X_collinear.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c4ec7-1d68-488e-895f-b23d03c07b84",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA) to reduce the dimensionality and multi-collinearity effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe37c1-7cf5-4bb4-bbd6-d61e022d6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume X is your feature dataframe\n",
    "X_pca = dataset_denormalized_outlier_filtered.drop(['ST2'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)  # Choose the number of components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "principal_df = pd.DataFrame(data=principal_components)\n",
    "\n",
    "# Add the ID, month, and day columns back to the principal component dataframe\n",
    "principal_df['ST2'] = dataset_denormalized_outlier_filtered['ST2']\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.show()\n",
    "print(principal_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59ec62-5d83-485b-8e8a-59398fad439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_denormalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7befde16-aab2-4ebe-9276-e7af3a946f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d33a20-7bf2-4610-8f27-cd5eaa107dc4",
   "metadata": {},
   "source": [
    "### Use IsolationForest to identify outliers and inliers\n",
    "#### If there are no outliers the outlier prediction should return all 1 and if there are outliers it will return -1.\n",
    "#### Identify the outliers using the Isolation Forest from the normalized dataset for ST2, ST5, ST10, ST20, ST50 and ST100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f76f7f-f8ca-4680-b724-67cf53f293b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('============================================= ST2 Outliers ========================================')\n",
    "# Reset the index of the original outlier filtered DataFrame\n",
    "dataset_normalized_outlier_filtered = dataset_normalized_outlier_filtered.reset_index(drop=True)\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features for target ST5\n",
    "ST2_X_train_no_target_normalized = dataset_normalized_outlier_filtered.drop(columns=['ST2','ST5','ST10','ST20','ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST2_isolation_forest_normalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST2_isolation_forest_normalized.fit(ST2_X_train_no_target_normalized)\n",
    "# Predict outliers in your dataset\n",
    "ST2_outlier_preds_normalized = ST2_isolation_forest_normalized.predict(ST2_X_train_no_target_normalized)\n",
    "if -1 in ST2_outlier_preds_normalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST2_outlier_indices_normalized = np.where(ST2_outlier_preds_normalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST2_outliers_normalized = ST2_X_train_no_target_normalized.iloc[ST2_outlier_indices_normalized]\n",
    "    ST2_clean_dataset_normalized = dataset_normalized_outlier_filtered.drop(ST2_outlier_indices_normalized)\n",
    "    print('ST2 Outliers indices: ', len(ST2_outliers_normalized), ST2_outliers_normalized)\n",
    "else:\n",
    "    ST2_clean_dataset_normalized = dataset_normalized_outlier_filtered.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "ST2_IF_scores = ST2_isolation_forest_normalized.decision_function(ST2_X_train_no_target_normalized)\n",
    "print(\"ST2 Scores: \", ST2_IF_scores)\n",
    "\n",
    "\n",
    "\n",
    "print('============================================= ST5 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features for target ST5\n",
    "ST5_X_train_no_target_normalized = dataset_normalized_outlier_filtered.drop(columns=['ST5','ST10','ST20','ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST5_isolation_forest_normalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST5_isolation_forest_normalized.fit(ST5_X_train_no_target_normalized)\n",
    "# Predict outliers in your dataset\n",
    "ST5_outlier_preds_normalized = ST5_isolation_forest_normalized.predict(ST5_X_train_no_target_normalized)\n",
    "if -1 in ST5_outlier_preds_normalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST5_outlier_indices_normalized = np.where(ST5_outlier_preds_normalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST5_outliers_normalized = ST5_X_train_no_target_normalized.iloc[ST5_outlier_indices_normalized]\n",
    "    ST5_clean_dataset_normalized = dataset_normalized_outlier_filtered.drop(ST5_outlier_indices_normalized)\n",
    "    print('ST5 Outliers indices: ', len(ST5_outliers_normalized), ST5_outliers_normalized)\n",
    "else:\n",
    "    ST5_clean_dataset_normalized = dataset_normalized_outlier_filtered.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "ST5_IF_scores = ST5_isolation_forest_normalized.decision_function(ST5_X_train_no_target_normalized)\n",
    "print(\"ST5 Scores: \", ST5_IF_scores)\n",
    "\n",
    "print('============================================= ST10 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features\n",
    "ST10_X_train_no_target_normalized = dataset_normalized_outlier_filtered.drop(columns=['ST10','ST20','ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST10_isolation_forest_normalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST10_isolation_forest_normalized.fit(ST10_X_train_no_target_normalized)\n",
    "# Predict outliers in your dataset\n",
    "ST10_outlier_preds_normalized = ST10_isolation_forest_normalized.predict(ST10_X_train_no_target_normalized)\n",
    "if -1 in ST10_outlier_preds_normalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST10_outlier_indices_normalized = np.where(ST10_outlier_preds_normalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST10_outliers_normalized = ST10_X_train_no_target_normalized.iloc[ST10_outlier_indices_normalized]\n",
    "    ST10_clean_dataset_normalized = dataset_normalized_outlier_filtered.drop(ST10_outlier_indices_normalized)\n",
    "    print('ST10 Outliers indices: ', len(ST10_outliers_normalized), ST10_outliers_normalized)\n",
    "else:\n",
    "    ST10_clean_dataset_normalized = dataset_normalized_outlier_filtered.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "    \n",
    "ST10_IF_scores = ST10_isolation_forest_normalized.decision_function(ST10_X_train_no_target_normalized)\n",
    "print(\"SST10 Scores: \", ST10_IF_scores)\n",
    "\n",
    "print('============================================= ST20 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features\n",
    "ST20_X_train_no_target_normalized = dataset_normalized_outlier_filtered.drop(columns=['ST20','ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST20_isolation_forest_normalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST20_isolation_forest_normalized.fit(ST20_X_train_no_target_normalized)\n",
    "# Predict outliers in your dataset\n",
    "ST20_outlier_preds_normalized = ST20_isolation_forest_normalized.predict(ST20_X_train_no_target_normalized)\n",
    "if -1 in ST20_outlier_preds_normalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST20_outlier_indices_normalized = np.where(ST20_outlier_preds_normalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST20_outliers_normalized = ST20_X_train_no_target_normalized.iloc[ST20_outlier_indices_normalized]\n",
    "    ST20_clean_dataset_normalized = dataset_normalized_outlier_filtered.drop(ST20_outlier_indices_normalized)\n",
    "    print('ST20 Outliers indices: ', len(ST20_outliers_normalized), ST20_outliers_normalized)\n",
    "else:\n",
    "    ST20_clean_dataset_normalized = dataset_normalized_outlier_filtered.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "\n",
    "ST20_IF_scores = ST20_isolation_forest_normalized.decision_function(ST20_X_train_no_target_normalized)\n",
    "print(\"ST20 Scores: \", ST20_IF_scores)\n",
    "\n",
    "print('============================================= ST50 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features\n",
    "ST50_X_train_no_target_normalized = dataset_normalized_outlier_filtered.drop(columns=['ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST50_isolation_forest_normalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST50_isolation_forest_normalized.fit(ST50_X_train_no_target_normalized)\n",
    "# Predict outliers in your dataset\n",
    "ST50_outlier_preds_normalized = ST50_isolation_forest_normalized.predict(ST50_X_train_no_target_normalized)\n",
    "if -1 in ST50_outlier_preds_normalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST50_outlier_indices_normalized = np.where(ST50_outlier_preds_normalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST50_outliers_normalized = ST50_X_train_no_target_normalized.iloc[ST50_outlier_indices_normalized]\n",
    "    ST50_clean_dataset_normalized = dataset_normalized_outlier_filtered.drop(ST50_outlier_indices_normalized)\n",
    "    print('ST50 Outliers indices: ', len(ST50_outliers_normalized), ST50_outliers_normalized)\n",
    "else:\n",
    "    ST50_clean_dataset_normalized = dataset_normalized_outlier_filtered.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "\n",
    "ST50_IF_scores = ST50_isolation_forest_normalized.decision_function(ST50_X_train_no_target_normalized)\n",
    "print(\"ST50 Scores: \", ST50_IF_scores)\n",
    "\n",
    "print('============================================= ST100 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features\n",
    "ST100_X_train_no_target_normalized = dataset_normalized_outlier_filtered.drop(columns=['ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST100_isolation_forest_normalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST100_isolation_forest_normalized.fit(ST100_X_train_no_target_normalized)\n",
    "# Predict outliers in your dataset\n",
    "ST100_outlier_preds_normalized = ST100_isolation_forest_normalized.predict(ST100_X_train_no_target_normalized)\n",
    "if -1 in ST100_outlier_preds_normalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST100_outlier_indices_normalized = np.where(ST100_outlier_preds_normalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST100_outliers_normalized = ST100_X_train_no_target_normalized.iloc[ST100_outlier_indices_normalized]\n",
    "    ST100_clean_dataset_normalized = dataset_normalized_outlier_filtered.drop(ST100_outlier_indices_normalized)\n",
    "    print('ST100 Outliers indices: ', len(ST100_outliers_normalized), ST100_outliers_normalized)\n",
    "else:\n",
    "    ST100_clean_dataset_normalized = dataset_normalized_outlier_filtered.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "\n",
    "ST100_IF_scores = ST100_isolation_forest_normalized.decision_function(ST100_X_train_no_target_normalized)\n",
    "print(\"ST100 Scores: \", ST100_IF_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41da04-cb87-480a-8430-5f03c4fbd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(ST2_X_train_no_target_normalized.iloc[:, 0], ST2_X_train_no_target_normalized.iloc[:, 1], c=ST2_IF_scores, cmap='coolwarm')\n",
    "plt.colorbar(label='Anomaly Score')\n",
    "plt.title('Anomaly Scores Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce34e2-0482-49a1-aa7e-94d219f39703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features list from the dataset columns\n",
    "ST2_features = ST2_clean_dataset_normalized.columns.tolist()\n",
    "# Temporary reomve the ID from the features list\n",
    "ST2_features.remove('ID')\n",
    "# remove the month feature from the features list\n",
    "ST2_features.remove('month')\n",
    "# reomve the day feature from the features list\n",
    "ST2_features.remove('day')\n",
    "# Call the custom built scatter plot for multiple features\n",
    "scatter_plot(ST2_features, ST2_clean_dataset_normalized, 'ST2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875dc4c0-8907-4d5f-bc94-7e8a025d5f14",
   "metadata": {},
   "source": [
    "### Isolate outliers using Isolation Forest from the original denormalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c574d-952e-4a2d-b202-516ed598174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('============================================= ST2 Outliers ========================================')\n",
    "# Reset the index of the original outlier filtered DataFrame\n",
    "dataset_denormalized_outlier_filtered = dataset_denormalized_outlier_filtered.reset_index(drop=True)\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features for target ST5\n",
    "ST2_X_train_no_target_denormalized = dataset_denormalized_outlier_filtered.drop(columns=['ST2','ST5','ST10','ST20','ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST2_isolation_forest_denormalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST2_isolation_forest_denormalized.fit(ST2_X_train_no_target_denormalized)\n",
    "# Predict outliers in your dataset\n",
    "ST2_outlier_preds_denormalized = ST2_isolation_forest_denormalized.predict(ST2_X_train_no_target_denormalized)\n",
    "if -1 in ST2_outlier_preds_denormalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST2_outlier_indices_denormalized = np.where(ST2_outlier_preds_denormalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST2_outliers_denormalized = ST2_X_train_no_target_denormalized.iloc[ST2_outlier_indices_denormalized]\n",
    "    ST2_clean_dataset_denormalized = dataset_denormalized_outlier_filtered.drop(ST2_outlier_indices_denormalized)\n",
    "    print('ST2 Outliers indices: ', len(ST2_outliers_denormalized), ST2_outliers_denormalized)\n",
    "else:\n",
    "    ST2_clean_dataset_denormalized = dataset_denormalized_outlier_filtered.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "ST2_IF_scores = ST2_isolation_forest_denormalized.decision_function(ST2_X_train_no_target_denormalized)\n",
    "print(\"ST2 Scores: \", ST2_IF_scores)\n",
    "\n",
    "ST2_clean_dataset_denormalized = ST2_clean_dataset_denormalized.reset_index(drop=True)\n",
    "print('============================================= ST5 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features for target ST5\n",
    "ST5_X_train_no_target_denormalized = ST2_clean_dataset_denormalized.drop(columns=['ST5','ST10','ST20','ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST5_isolation_forest_denormalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST5_isolation_forest_denormalized.fit(ST5_X_train_no_target_denormalized)\n",
    "# Predict outliers in your dataset\n",
    "ST5_outlier_preds_denormalized = ST5_isolation_forest_denormalized.predict(ST5_X_train_no_target_denormalized)\n",
    "if -1 in ST5_outlier_preds_denormalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST5_outlier_indices_denormalized = np.where(ST5_outlier_preds_denormalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST5_outliers_denormalized = ST5_X_train_no_target_denormalized.iloc[ST5_outlier_indices_denormalized]\n",
    "    ST5_clean_dataset_denormalized = ST2_clean_dataset_denormalized.drop(ST5_outlier_indices_denormalized)\n",
    "    print('ST5 Outliers indices: ', len(ST5_outliers_denormalized), ST5_outliers_denormalized)\n",
    "else:\n",
    "    ST5_clean_dataset_denormalized = ST2_clean_dataset_denormalized.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "ST5_IF_scores = ST5_isolation_forest_denormalized.decision_function(ST5_X_train_no_target_denormalized)\n",
    "print(\"ST5 Scores: \", ST5_IF_scores)\n",
    "\n",
    "ST5_clean_dataset_denormalized = ST5_clean_dataset_denormalized.reset_index(drop=True)\n",
    "\n",
    "print('============================================= ST10 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features\n",
    "ST10_X_train_no_target_denormalized = ST5_clean_dataset_denormalized.drop(columns=['ST10','ST20','ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST10_isolation_forest_denormalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST10_isolation_forest_denormalized.fit(ST10_X_train_no_target_denormalized)\n",
    "# Predict outliers in your dataset\n",
    "ST10_outlier_preds_denormalized = ST10_isolation_forest_denormalized.predict(ST10_X_train_no_target_denormalized)\n",
    "if -1 in ST10_outlier_preds_denormalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST10_outlier_indices_denormalized = np.where(ST10_outlier_preds_denormalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST10_outliers_denormalized = ST10_X_train_no_target_denormalized.iloc[ST10_outlier_indices_denormalized]\n",
    "    ST10_clean_dataset_denormalized = ST5_clean_dataset_denormalized.drop(ST10_outlier_indices_denormalized)\n",
    "    print('ST10 Outliers indices: ', len(ST10_outliers_denormalized), ST10_outliers_denormalized)\n",
    "else:\n",
    "    ST10_clean_dataset_denormalized = ST5_clean_dataset_denormalized.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "    \n",
    "ST10_IF_scores = ST10_isolation_forest_denormalized.decision_function(ST10_X_train_no_target_denormalized)\n",
    "print(\"SST10 Scores: \", ST10_IF_scores)\n",
    "\n",
    "ST10_clean_dataset_denormalized = ST10_clean_dataset_denormalized.reset_index(drop=True)\n",
    "print('============================================= ST20 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features\n",
    "ST20_X_train_no_target_denormalized = ST10_clean_dataset_denormalized.drop(columns=['ST20','ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST20_isolation_forest_denormalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST20_isolation_forest_denormalized.fit(ST20_X_train_no_target_denormalized)\n",
    "# Predict outliers in your dataset\n",
    "ST20_outlier_preds_denormalized = ST20_isolation_forest_denormalized.predict(ST20_X_train_no_target_denormalized)\n",
    "if -1 in ST20_outlier_preds_denormalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST20_outlier_indices_denormalized = np.where(ST20_outlier_preds_denormalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST20_outliers_denormalized = ST20_X_train_no_target_denormalized.iloc[ST20_outlier_indices_denormalized]\n",
    "    ST20_clean_dataset_denormalized = ST10_clean_dataset_denormalized.drop(ST20_outlier_indices_denormalized)\n",
    "    print('ST20 Outliers indices: ', len(ST20_outliers_denormalized), ST20_outliers_denormalized)\n",
    "else:\n",
    "    ST20_clean_dataset_denormalized = ST10_clean_dataset_denormalized.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "\n",
    "ST20_IF_scores = ST20_isolation_forest_denormalized.decision_function(ST20_X_train_no_target_denormalized)\n",
    "print(\"ST20 Scores: \", ST20_IF_scores)\n",
    "\n",
    "ST20_clean_dataset_denormalized = ST20_clean_dataset_denormalized.reset_index(drop=True)\n",
    "print('============================================= ST50 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features\n",
    "ST50_X_train_no_target_denormalized = ST20_clean_dataset_denormalized.drop(columns=['ST50','ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST50_isolation_forest_denormalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST50_isolation_forest_denormalized.fit(ST50_X_train_no_target_denormalized)\n",
    "# Predict outliers in your dataset\n",
    "ST50_outlier_preds_denormalized = ST50_isolation_forest_denormalized.predict(ST50_X_train_no_target_denormalized)\n",
    "if -1 in ST50_outlier_preds_denormalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST50_outlier_indices_denormalized = np.where(ST50_outlier_preds_denormalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST50_outliers_denormalized = ST50_X_train_no_target_denormalized.iloc[ST50_outlier_indices_denormalized]\n",
    "    ST50_clean_dataset_denormalized = ST20_clean_dataset_denormalized.drop(ST50_outlier_indices_denormalized)\n",
    "    print('ST50 Outliers indices: ', len(ST50_outliers_denormalized), ST50_outliers_denormalized)\n",
    "else:\n",
    "    ST50_clean_dataset_denormalized = ST20_clean_dataset_denormalized.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "\n",
    "ST50_IF_scores = ST50_isolation_forest_denormalized.decision_function(ST50_X_train_no_target_denormalized)\n",
    "print(\"ST50 Scores: \", ST50_IF_scores)\n",
    "\n",
    "ST50_clean_dataset_denormalized = ST50_clean_dataset_denormalized.reset_index(drop=True)\n",
    "print('============================================= ST100 Outliers ========================================')\n",
    "# Drop the target variable and unnecessary columns from the dataset that are not considered as features\n",
    "ST100_X_train_no_target_denormalized = ST50_clean_dataset_denormalized.drop(columns=['ST100', 'month', 'day','ID'])\n",
    "# Instantiate the IsolationForest model\n",
    "ST100_isolation_forest_denormalized = IsolationForest(random_state=42)\n",
    "# Fit the IsolationForest model to your training dataset\n",
    "ST100_isolation_forest_denormalized.fit(ST100_X_train_no_target_denormalized)\n",
    "# Predict outliers in your dataset\n",
    "ST100_outlier_preds_denormalized = ST100_isolation_forest_denormalized.predict(ST100_X_train_no_target_denormalized)\n",
    "if -1 in ST100_outlier_preds_denormalized:\n",
    "    print(\"Outliers detected.\")\n",
    "    # Get indices of outliers\n",
    "    ST100_outlier_indices_denormalized = np.where(ST100_outlier_preds_denormalized == -1)[0]\n",
    "    # Get the Observed data points corresponding to outliers\n",
    "    ST100_outliers_denormalized = ST100_X_train_no_target_denormalized.iloc[ST100_outlier_indices_denormalized]\n",
    "    ST100_clean_dataset_denormalized = ST50_clean_dataset_denormalized.drop(ST100_outlier_indices_denormalized)\n",
    "    print('ST100 Outliers indices: ', len(ST100_outliers_denormalized), ST100_outliers_denormalized)\n",
    "else:\n",
    "    ST100_clean_dataset_denormalized = ST50_clean_dataset_denormalized.copy()\n",
    "    print(\"No outliers detected.\")\n",
    "\n",
    "ST100_IF_scores = ST100_isolation_forest_denormalized.decision_function(ST100_X_train_no_target_denormalized)\n",
    "print(\"ST100 Scores: \", ST100_IF_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ac021d-a0c5-4723-95b5-c4c4754dc6f9",
   "metadata": {},
   "source": [
    "### Evaluate the Cronbach's alpha and Augmented Dickey-Fuller (ADF) unit root test parameters for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108380b-87e9-496b-a322-16e91e91f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "# Check the Cronbach's alpha\n",
    "def cronbach_alpha(df):\n",
    "    # Number of items\n",
    "    k = df.shape[1]\n",
    "    \n",
    "    # Variance for each item\n",
    "    variances = df.var(axis=0, ddof=1)\n",
    "    \n",
    "    # Total variance\n",
    "    total_variance = df.sum(axis=1).var(ddof=1)\n",
    "    \n",
    "    # Cronbach's alpha calculation\n",
    "    alpha = (k / (k - 1)) * (1 - (variances.sum() / total_variance))\n",
    "    return alpha\n",
    "\n",
    "# Assuming dataset_denormalized_outlier_filtered is your dataframe\n",
    "df_test = dataset_denormalized_outlier_filtered.drop(columns=['month', 'day', 'ID'])\n",
    "alpha = cronbach_alpha(df_test)\n",
    "print(f\"Cronbach's alpha: {alpha}\")\n",
    "\n",
    "# Check the Unit root test\n",
    "def check_stationarity(series):\n",
    "    result = adfuller(series, autolag='AIC')\n",
    "    adf_stat = result[0]\n",
    "    p_value = result[1]\n",
    "    critical_values = result[4]\n",
    "    return adf_stat, p_value, critical_values\n",
    "\n",
    "# Iterate through each column to perform the ADF test\n",
    "for column in df_test.columns:\n",
    "    adf_stat, p_value, critical_values = check_stationarity(df_test[column])\n",
    "    print(f'ADF Test for {column}:')\n",
    "    print(f'  ADF Statistic: {adf_stat}')\n",
    "    print(f'  p-value: {p_value}')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'  Critical Value ({key}): {value}')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed94bfe-6741-48da-af7f-d71eada68b5a",
   "metadata": {},
   "source": [
    "#### Note: The features have a good Cronbach's Alpha of 0.822 which is greater than 0.7 and stationary time-series data for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9723c0-ab89-4dbe-ab54-3213c844fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST2_clean_dataset_denormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15602f0a-4c4c-43e7-bb3b-ee7c51f86f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST5_clean_dataset_denormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c98d33-9ec6-4938-9673-81123872edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST10_clean_dataset_denormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a8782-2f2e-4625-a62d-f90c017593c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST20_clean_dataset_denormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a110529-f43f-49c2-952b-20273de8472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST50_clean_dataset_denormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4d19e-f22c-4335-83d3-8ef106a36573",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST100_clean_dataset_denormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39236f1-544b-4e98-b47a-d1686a49ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features list from the dataset columns\n",
    "ST2_features = ST2_clean_dataset_denormalized.columns.tolist()\n",
    "# Temporary reomve the ID from the features list\n",
    "ST2_features.remove('ID')\n",
    "# remove the month feature from the features list\n",
    "ST2_features.remove('month')\n",
    "# reomve the day feature from the features list\n",
    "ST2_features.remove('day')\n",
    "# Call the custom built scatter plot for multiple features\n",
    "scatter_plot(ST2_features, ST2_clean_dataset_denormalized, 'ST2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4b8b2-3eee-43d3-b937-06ca7b8f1ff4",
   "metadata": {},
   "source": [
    "### Outlier filtered denormalized dataset by Isolation Forest histogram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47523892-6934-4d5c-a61b-508bc44b2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of features from our dataset columns\n",
    "ST2_features = ST2_clean_dataset_denormalized.columns.tolist()\n",
    "# remove the month feature from the features list\n",
    "ST2_features.remove('month')\n",
    "# remove the day feature from the features list\n",
    "ST2_features.remove('day')\n",
    "# remove the ID from the features list\n",
    "ST2_features.remove('ID')\n",
    "\n",
    "# Plot histograms of Z-scores for all features\n",
    "num_features = len(ST2_features)\n",
    "num_cols = 2\n",
    "num_rows = (num_features + num_cols - 1) // num_cols\n",
    "plt.figure(figsize=(15, 5 * num_rows))\n",
    "for i, feature in enumerate(ST2_features, start=1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.histplot(ST2_clean_dataset_denormalized[feature], kde=True)\n",
    "    plt.title('Denormalized histogram distribution')\n",
    "    plt.xlabel(f'Denormalized {feature}')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/isolation_forest_smoothed_histogram_of_parameters.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3f915-db03-444f-a37c-8be909f81d51",
   "metadata": {},
   "source": [
    "### Outlier filtered by Isolation Forest normalized dataset histogram distribution ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053679dc-8841-4e4d-b4f3-e3bf03620646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of features from our dataset columns\n",
    "ST2_features = ST2_clean_dataset_normalized.columns.tolist()\n",
    "# remove the month feature from the features list\n",
    "ST2_features.remove('month')\n",
    "# remove the day feature from the features list\n",
    "ST2_features.remove('day')\n",
    "# remove the ID from the features list\n",
    "ST2_features.remove('ID')\n",
    "\n",
    "# Plot histograms of Z-scores for all features\n",
    "num_features = len(ST2_features)\n",
    "num_cols = 2\n",
    "num_rows = (num_features + num_cols - 1) // num_cols\n",
    "plt.figure(figsize=(15, 5 * num_rows))\n",
    "for i, feature in enumerate(ST2_features, start=1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.histplot(ST2_clean_dataset_normalized[feature], kde=True)\n",
    "    plt.title(f'Normalized ({feature})', fontsize=14)\n",
    "    plt.xlabel(f'Normalized {feature}', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/isolation_forest_and_z_score_filtered_histogram_all_parameters.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bed24-4186-4806-aa0e-56e617df44ac",
   "metadata": {},
   "source": [
    "### Normalized Outlier filtered by Z-score thresholds histogram for ST100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c4e20-fdb5-4e43-b87d-5589fdb6e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of features from our dataset columns\n",
    "features = ST100_clean_dataset_normalized.columns.tolist()\n",
    "# remove the month feature from the features list\n",
    "features.remove('month')\n",
    "# remove the day feature from the features list\n",
    "features.remove('day')\n",
    "# remove the ID from the features list\n",
    "features.remove('ID')\n",
    "\n",
    "# Plot histograms of Z-scores for all features\n",
    "num_features = len(features)\n",
    "num_cols = 2\n",
    "num_rows = (num_features + num_cols - 1) // num_cols\n",
    "plt.figure(figsize=(15, 5 * num_rows))\n",
    "for i, feature in enumerate(features, start=1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.histplot(ST100_clean_dataset_normalized[feature], kde=True)\n",
    "    plt.title(f'Histogram({feature})', fontsize=14)\n",
    "    plt.xlabel(f'{feature}(Normalized)', fontsize=14)\n",
    "    plt.ylabel('Frequency(after Isolation Forest)', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST100_normalized_z_score_filtered_histogram_all_parameters.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dbd4cb-adfe-4b22-a1e8-0040e1ebc08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST100_clean_dataset_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945b1df-bf78-4e97-828e-b0ceb9ffcb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c743cc-cde6-40e4-8f1d-397614f461a2",
   "metadata": {},
   "source": [
    "### Normalized Outlier filtered by Z-score thresholds histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23ce16-1d8b-47ed-8c29-ac740a903676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of features from our dataset columns\n",
    "features = dataset_normalized_outlier_filtered.columns.tolist()\n",
    "# remove the month feature from the features list\n",
    "features.remove('month')\n",
    "# remove the day feature from the features list\n",
    "features.remove('day')\n",
    "# remove the ID from the features list\n",
    "features.remove('ID')\n",
    "\n",
    "# Plot histograms of Z-scores for all features\n",
    "num_features = len(features)\n",
    "num_cols = 2\n",
    "num_rows = (num_features + num_cols - 1) // num_cols\n",
    "plt.figure(figsize=(15, 5 * num_rows))\n",
    "for i, feature in enumerate(features, start=1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.histplot(dataset_normalized_outlier_filtered[feature], kde=True)\n",
    "    plt.title(f'Histogram({feature})', fontsize=14)\n",
    "    plt.xlabel(f'{feature}(Normalized)', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.tick_params(axis='both', which='minor', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/Normalized_z_score_filtered_histogram_all_parameters.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a69af6-191a-4355-a1e0-d1cdf9296635",
   "metadata": {},
   "source": [
    "#### Redraw the scatter plot of the dataset filtered from outliers to see if the outliers are properly removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52359c8-b483-4ed6-9109-2b86410f6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features list\n",
    "features = dataset_normalized_outlier_filtered.columns.tolist()\n",
    "# Remove the unncessary features day, month and ST100 from the plot\n",
    "features.remove('day')\n",
    "features.remove('month')\n",
    "features.remove('ST100')\n",
    "features.remove('ID')\n",
    "# Call the scatter custom function created previously\n",
    "scatter_plot(features, dataset_normalized_outlier_filtered, 'ST100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19439d8e-5afc-4a9d-b126-64b48b6d70b5",
   "metadata": {},
   "source": [
    "## 2. Prediction\n",
    "### Now the data is filled for missing values and we have all numeric values, so we can start to design my model. We don't need any further data conversion as all my datatypes are numric but we don't need to have the time information in my final dataset so we should remove the date, year, month, day columns from my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98de64-d709-40aa-9592-9e4c46c6744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28718f-81a3-449a-ba6f-83853c27be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized_outlier_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c533a85b-8eb0-4eb9-839e-d0eb964316a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let us check if our dataset is all numeric in addition to the info() method\n",
    "for label, content in dataset_normalized_outlier_filtered.items():\n",
    "    if not pd.api.types.is_numeric_dtype(content):\n",
    "        print('Non-numeric column: ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35086f53-eaec-4430-9886-c521eb7b2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST100_clean_dataset_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ba3fe-a8ab-4b78-a41d-123332b61277",
   "metadata": {},
   "source": [
    "### 2.1. Dataset Splitting For target ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d75eac-ca06-43d8-b39c-bf46af4ad19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us shuffle the entire dataset so that it is randomly arranged\n",
    "np.random.seed(42)\n",
    "dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1)\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_100cm ). Drop the ID as it is not a feature\n",
    "X = dataset_shuffled.drop(['ID','ST2','ST5','ST10','ST20','ST50','ST100'], axis=1)\n",
    "Y = dataset_shuffled[\"ST2\"]\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(dataset_shuffled))\n",
    "ST100_X_train, ST100_Y_train = X[:train_split], Y[:train_split]\n",
    "ST100_X_valid, ST100_Y_valid =X[train_split:valid_split], Y[train_split:valid_split]\n",
    "ST100_X_test, ST100_Y_test = X[valid_split:], Y[valid_split:]\n",
    "# Save the ST100_X_test data to csv for future use\n",
    "ST100_X_test.to_csv(\"data/ST2_X_test_data.csv\", index=False)\n",
    "# # Split the feature and label datasets in to 80/20 training and test datasets respectively\n",
    "# ST100_X_train, ST100_X_test, ST100_Y_train, ST100_Y_test = train_test_split(X,Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c1ecb-c44d-43fb-93b4-e000f4b2c2c9",
   "metadata": {},
   "source": [
    "#### Let's build the evaluation metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205bfb7-411c-464a-91ab-5d4c68a1a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation metrics function that shows the metrics result of different metrics for a model\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, mean_absolute_percentage_error, max_error, explained_variance_score\n",
    "def rmsle(Y_test, Y_preds):\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared log error between predictions and true labels\n",
    "        \n",
    "    Parameters:\n",
    "        Y_test: A test dataset of the target label.\n",
    "        Y_preds: The predicted values of the target label.\n",
    "            \n",
    "    Returns:\n",
    "        float: the root mean squared log error of the model.\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_log_error(Y_test, Y_preds))\n",
    "\n",
    "# Create function to evaluate model on few different levels\n",
    "def show_scores(model, X_train, X_valid, Y_train, Y_valid, target='ST', model_name='RF'):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    scores = {\n",
    "              # \"Training Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_valid, val_preds),\n",
    "              # \"Training Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds),             \n",
    "              # \"Training Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),\n",
    "              # \"Training Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Training Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              # \"Training Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              # \"Training Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)}\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['Metric', 'Value'])    \n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_excel(f'data/{target}_{model_name}_scores.xlsx', index=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba63789-805b-4f6e-b91f-ae89d235f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_denormalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16feb956-b5ab-450e-ae6a-8df39ce0f4e7",
   "metadata": {},
   "source": [
    "## 2.2. Fitting the models for initial testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c59cd-b0b6-4d99-a112-c5a1568da3bb",
   "metadata": {},
   "source": [
    "### i. RandomForestRegressor Model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e83fdd-f8ec-4694-b09a-b7121d6c6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's try another model which is ensemble RandomForestRegressor\n",
    "# Import the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Create rando forest model\n",
    "rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_rf_model = rf_model\n",
    "ST50_rf_model = rf_model\n",
    "ST20_rf_model = rf_model\n",
    "ST10_rf_model = rf_model\n",
    "ST5_rf_model = rf_model\n",
    "ST2_rf_model = rf_model\n",
    "# \n",
    "ST100_rf_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "# Access the OOB Score\n",
    "oob_score = ST100_rf_model.oob_score_\n",
    "print('Out of Bag Score: ', oob_score)\n",
    "show_scores(ST100_rf_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612d029-677c-4c23-a100-58d557f9c00c",
   "metadata": {},
   "source": [
    "### ii. HistGradientBoostingRegressor Model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a9c84-5e4e-428a-86c4-8bd7d2343fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "# Create rando forest model\n",
    "gbr_model = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_gbr_model = gbr_model\n",
    "ST50_gbr_model = gbr_model\n",
    "ST20_gbr_model = gbr_model\n",
    "ST10_gbr_model = gbr_model\n",
    "ST5_gbr_model = gbr_model\n",
    "ST2_gbr_model = gbr_model\n",
    "# \n",
    "ST100_gbr_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "# Show scores\n",
    "show_scores(ST100_gbr_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'HGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39adb5-440f-4e26-afa9-e5f48d56bb6f",
   "metadata": {},
   "source": [
    "### iii. Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893fa58-195d-4e75-8680-dfef2e77c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Import the model\n",
    "from sklearn.linear_model import Ridge\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Instantiate and fit the model (on training set)\n",
    "rg_model = Ridge()\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_rg_model = rg_model\n",
    "ST50_rg_model = rg_model\n",
    "ST20_rg_model = rg_model\n",
    "ST10_rg_model = rg_model\n",
    "ST5_rg_model = rg_model\n",
    "ST2_rg_model = rg_model\n",
    "# Fit the ST100 model for soil temp at 100 cm\n",
    "ST100_rg_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "show_scores(ST100_rg_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'RR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b6645-da0f-499c-8290-790391ed1543",
   "metadata": {},
   "source": [
    "### iv. Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b927eb-834b-460a-8e55-61caa4735d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's check the Lasso model\n",
    "# Import the model\n",
    "from sklearn.linear_model import Lasso\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create rando forest model\n",
    "la_model = Lasso()\n",
    "# Create Lasso model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_la_model = la_model\n",
    "ST50_la_model = la_model\n",
    "ST20_la_model = la_model\n",
    "ST10_la_model = la_model\n",
    "ST5_la_model = la_model\n",
    "ST2_la_model = la_model\n",
    "\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_la_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "show_scores(ST100_la_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'LA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972752da-5929-4930-9d1d-be5115061828",
   "metadata": {},
   "source": [
    "### v. ElasticNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53706338-7b26-4ae9-9496-d7990a876179",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's check the ElasticNet model\n",
    "# Import the model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create rando forest model\n",
    "en_model = ElasticNet()\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_en_model = en_model\n",
    "ST50_en_model = en_model\n",
    "ST20_en_model = en_model\n",
    "ST10_en_model = en_model\n",
    "ST5_en_model = en_model\n",
    "ST2_en_model = en_model\n",
    "\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_en_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "show_scores(ST100_en_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'EN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b24e74-d406-4d9e-a7ba-06cdcecdf756",
   "metadata": {},
   "source": [
    "### vi. SVR with kernel 'linear' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1897e6d-c238-4f46-8a80-8bf692b550a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's check the SVR with kernel linear model\n",
    "# Import the model\n",
    "from sklearn.svm import SVR\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create rando forest model\n",
    "svrl_model = SVR(kernel='linear')\n",
    "\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_svrl_model = svrl_model\n",
    "ST50_svrl_model = svrl_model\n",
    "ST20_svrl_model = svrl_model\n",
    "ST10_svrl_model = svrl_model\n",
    "ST5_svrl_model = svrl_model\n",
    "ST2_svrl_model = svrl_model\n",
    "\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_svrl_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "show_scores(ST100_svrl_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'SVR_L')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740973f-c274-48fb-91c3-1e4e8910ee1e",
   "metadata": {},
   "source": [
    "### vii. SVR with kernel 'rbf' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8e728-1a49-4b4a-9a45-933d70841e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's check the SVR with kernel 'rbf' model\n",
    "# Import the model\n",
    "from sklearn.svm import SVR\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create rando forest model\n",
    "svrr_model = SVR(kernel='rbf')\n",
    "\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_svrr_model = svrr_model\n",
    "ST50_svrr_model = svrr_model\n",
    "ST20_svrr_model = svrr_model\n",
    "ST10_svrr_model = svrr_model\n",
    "ST5_svrr_model = svrr_model\n",
    "ST2_svrr_model = svrr_model\n",
    "\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_svrr_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "show_scores(ST100_svrr_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'SVR_R')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c165b-a319-44fc-af69-fe8d129cb3c3",
   "metadata": {},
   "source": [
    "### viii. AdaBoostRegressor 'adaBr' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1cee5e-a284-4aec-a7d1-59bab385a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's check the SVR with kernel 'rbf' model\n",
    "# Import the model\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_adaBr_model = AdaBoostRegressor(random_state=42)\n",
    "ST50_adaBr_model = AdaBoostRegressor(random_state=42)\n",
    "ST20_adaBr_model = AdaBoostRegressor(random_state=42)\n",
    "ST10_adaBr_model = AdaBoostRegressor(random_state=42)\n",
    "ST5_adaBr_model = AdaBoostRegressor(random_state=42)\n",
    "ST2_adaBr_model = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_adaBr_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "show_scores(ST100_adaBr_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'ADB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05cc9b-7da5-44ba-83f8-46c281708b75",
   "metadata": {},
   "source": [
    "### iX. XGBoostRegressor 'XGBoost' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8edf9-c35b-4107-a129-5ac1519aad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's check the SVR with kernel 'rbf' model\n",
    "# Import the model\n",
    "from xgboost import XGBRegressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_XGB_model = XGBRegressor(objective='reg:squarederror',random_state=42)\n",
    "ST50_XGB_model = XGBRegressor(objective='reg:squarederror',random_state=42)\n",
    "ST20_XGB_model = XGBRegressor(objective='reg:squarederror',random_state=42)\n",
    "ST10_XGB_model = XGBRegressor(objective='reg:squarederror',random_state=42)\n",
    "ST5_XGB_model = XGBRegressor(objective='reg:squarederror',random_state=42)\n",
    "ST2_XGB_model = XGBRegressor(objective='reg:squarederror',random_state=42)\n",
    "\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_XGB_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Show the scoring metrics for this model\n",
    "print(\"The Evaluation Metrics Results:\")\n",
    "show_scores(ST100_XGB_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9f649-65fb-448f-b61f-6059b9bdb407",
   "metadata": {},
   "source": [
    "### X. CatBoostRegressor 'CatBoost' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628807eb-2b47-4fd0-a566-37ef6d1c4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize CatBoost Regressor\n",
    "ST100_CB_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function='RMSE',\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "ST100_CB_model.fit(ST100_X_train, ST100_Y_train, eval_set=(ST100_X_valid, ST100_Y_valid), early_stopping_rounds=100)\n",
    "\n",
    "# Make predictions\n",
    "ST100_P_pred = ST100_CB_model.predict(ST100_X_test)\n",
    "# Evaluate the model\n",
    "show_scores(ST100_CB_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid, 'ST2', 'CB')\n",
    "# cat_mse = mean_squared_error(ST100_Y_test, ST100_P_pred)\n",
    "# cat_mae = mean_absolute_error(ST100_Y_test, ST100_P_pred)\n",
    "# print(f'Mean Squared Error: {cat_mse}')\n",
    "# print(f'Mean Squared Error: {cat_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992ec34-adf8-46db-87d3-0c26a6f3e3f4",
   "metadata": {},
   "source": [
    "### X. Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c793df-607f-4e47-8c48-2a2625671cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Input\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# ST100_X_train_nn = scaler.fit_transform(ST100_X_train)\n",
    "# ST100_X_valid_nn = scaler.transform(ST100_X_valid)\n",
    "\n",
    "# # Define the neural network nn_model\n",
    "# nn_model = Sequential()\n",
    "# nn_model.add(Input(shape=(ST100_X_train_nn.shape[1],)))\n",
    "# nn_model.add(Dense(64, activation='relu'))\n",
    "# nn_model.add(Dense(32, activation='relu'))\n",
    "# nn_model.add(Dense(1, activation='linear'))  # For regression\n",
    "\n",
    "# # Compile the nn_model\n",
    "# nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# # Train the nn_model\n",
    "# history = nn_model.fit(ST100_X_train_nn, ST100_Y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# # Evaluate the nn_model\n",
    "# mse, mae = nn_model.evaluate(ST100_X_valid_nn, ST100_Y_valid)\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "# print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# # Make predictions\n",
    "# Y_preds = nn_model.predict(ST100_X_test)\n",
    "# show_scores(nn_model, ST100_X_train_nn, ST100_X_valid_nn, ST100_Y_train, ST100_Y_valid, 'ST2', 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521c9fe-5a33-47f8-a9d2-78be4652c4bb",
   "metadata": {},
   "source": [
    "### GridSearchCV For Keras Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffefa8-edd6-4652-b699-ce20b81732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Input\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.base import BaseEstimator, RegressorMixin\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "# # Function to create the Keras model\n",
    "# def create_model(learning_rate=0.001, neurons1=64, neurons2=32):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(ST100_X_train.shape[1],)))\n",
    "#     model.add(Dense(neurons1, activation='relu'))\n",
    "#     model.add(Dense(neurons2, activation='relu'))\n",
    "#     model.add(Dense(1))  # Output layer for regression\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "#     return model\n",
    "\n",
    "# # Generate dummy data for demonstration (replace with your Observed data)\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Wrap the model using the custom KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.001, 0.01, 0.1],\n",
    "#     'neurons1': [32, 64, 128],\n",
    "#     'neurons2': [16, 32, 64],\n",
    "#     'batch_size': [10, 20, 40],\n",
    "#     'epochs': [50, 100]\n",
    "# }\n",
    "\n",
    "\n",
    "# # Create the GridSearchCV object\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid_result = grid.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# # Print the best parameters and the corresponding score\n",
    "# print(\"Best parameters found: \", grid_result.best_params_)\n",
    "# print(\"Best score: \", -grid_result.best_score_)\n",
    "# best_model = grid_result.best_estimator_\n",
    "# # # Example: Get the best model and make predictions\n",
    "# # best_model = grid_result.best_estimator_\n",
    "# # ST100_X_test = np.random.rand(20, 10)  # Dummy test data (replace with your Observed test data)\n",
    "# # ST100_Y_pred = best_model.predict(ST100_X_test)\n",
    "\n",
    "# # # Example: Print the first 5 predictions\n",
    "# # print(ST100_Y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab87fa-dd2b-445d-8d62-cc2f44abfec8",
   "metadata": {},
   "source": [
    "## 2.3. Hyperparameter Tuning with RandomizedSearchCV\n",
    "### NB: This may take time. Turn it on when you need to run it.\n",
    "#### To train a random forest, we need to specify the number of decision trees to use (the n_estimators parameter) and the maximum depth of each tree (the max_depth parameter). Other hyperparameters, such as the minimum number of samples required to split a node and the minimum number of samples required at a leaf node, can also be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0f49b-3c2f-4330-922c-4e7b0972dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'max_depth': [None, 10, 20],\n",
    "#     'min_samples_split': np.arange(2,20,2),\n",
    "#     'min_samples_leaf': np.arange(1, 20, 2),\n",
    "#     'max_features': [0.5, 1, 'sqrt', 'log2', None]\n",
    "# }\n",
    "\n",
    "# # Instantiate the model with the RandomizeSearchCV\n",
    "# rsv_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1, random_state=42), \n",
    "#                                  param_distributions=param_grid, n_iter=20,\n",
    "#                                  cv=5, verbose=0)\n",
    "\n",
    "# # Fit the RandomizedSearchCV model to the train data\n",
    "# rsv_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Find the best model paramters from the fitted model\n",
    "# rsv_model.best_params_\n",
    "# Evaluate the RandomizedSearch model\n",
    "# show_scores(rsv_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b39c76-205f-48d3-87df-1572ffb68fe7",
   "metadata": {},
   "source": [
    "## 2.4. Hyperparameter tuning using GridSearchCV\n",
    "### NB: Run it when required. This may take time so be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597d03c-9e05-4236-8a9b-3a1e4b419450",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [None, 20],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [1, 'sqrt'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'max_leaf_nodes': [31, 41, 51]\n",
    "}\n",
    "\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "ada = AdaBoostRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# Initialize GridSearchCV for each model\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=param_grid_hgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Fit the models\n",
    "grid_search_rf.fit(ST100_X_train, ST100_Y_train)\n",
    "grid_search_hgb.fit(ST100_X_train, ST100_Y_train)\n",
    "grid_search_ada.fit(ST100_X_train, ST100_Y_train)\n",
    "grid_search_xgb.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for RandomForestRegressor:\", grid_search_rf.best_params_)\n",
    "print(\"Best score for RandomForestRegressor:\", -grid_search_rf.best_score_)\n",
    "\n",
    "print(\"Best parameters for HistGradientBoostingRegressor:\", grid_search_hgb.best_params_)\n",
    "print(\"Best score for HistGradientBoostingRegressor:\", -grid_search_hgb.best_score_)\n",
    "\n",
    "print(\"Best parameters for AdaBoostRegressor:\", grid_search_ada.best_params_)\n",
    "print(\"Best score for AdaBoostRegressor:\", -grid_search_ada.best_score_)\n",
    "\n",
    "\n",
    "print(\"Best parameters for XGBRegressor:\", grid_search_xgb.best_params_)\n",
    "print(\"Best score for XGBRegressor:\", -grid_search_xgb.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410c0d6-3c40-4f51-a804-007128cbec3f",
   "metadata": {},
   "source": [
    "### Let's now train our RF model with the best hyperparameters estimated with the help of GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611a5c5-1c95-4338-8db2-cd639226f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The most ideal hyperparameters are\n",
    "ST100_ideal_rf_model = RandomForestRegressor(\n",
    "                                       n_estimators=300, \n",
    "                                       min_samples_leaf=1,\n",
    "                                       min_samples_split=2,\n",
    "                                       max_features='sqrt',\n",
    "                                       max_depth=None,\n",
    "                                       bootstrap=False,\n",
    "                                       random_state=42)\n",
    "# Fit the ideal model\n",
    "ST100_ideal_rf_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scores of the trained ideal RF model\n",
    "show_scores(ST100_ideal_rf_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3a4f8-5df2-4a86-b408-6607b3a4c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the scores from the default RF model\n",
    "show_scores(ST100_rf_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f45574-4188-4dcb-a838-46d4878909e5",
   "metadata": {},
   "source": [
    "## 2.5. Evaluating the model with cross-validation\n",
    "#### The cross validation score will evaluate the model by taking K-number of folds or splits for the entire dataset. Let's take K = CV = 10 for this test as the dataset is not big enough. This helps us avoid lucky splits in case we consider only one random split pattern.\n",
    "#### NB: This may take time be patient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112a194-5dfe-4590-a9cd-b271df4090ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "cross_val_score_r2 = cross_val_score(ST100_ideal_rf_model, X, Y, cv=10)\n",
    "cross_val_score_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed6aad-4488-4362-8e77-73229e79a94f",
   "metadata": {},
   "source": [
    "## 2.6. Predictions on the test set (ST2_X_test, ST2_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8a759-4d49-47c1-b06c-d42a97aee459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data from file\n",
    "test_data = pd.read_csv(\"data/ST2_X_test_data.csv\")\n",
    "ST100_Y_preds = ST100_ideal_rf_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815662dd-b432-42bf-b6d2-345e59ca3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST100_Y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997044c-74c0-4b68-a7ee-49911bc54cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e1946-6b72-4010-9cda-55db671858b9",
   "metadata": {},
   "source": [
    "## Reverse the z-score normalized predicted values into unnormalized predicted original values\n",
    "#### Step 1: Calculate the mean and standard deviation of each feature from the original dataset\n",
    "#### Step 2: Reverse normalization for each feature in the predicted dataset\n",
    "### Let's implement a generic function that reverses the z-score normalized data to unnormalized original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d8360-f3dc-483e-8714-809af7afa35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reverse normalization function\n",
    "def reverse_normalization(original_dataset, feature, test_set_series, model_name=None):\n",
    "    \"\"\"\n",
    "    Reverses the normalized pandas series(target variable) to its corresponding unnormalized pandas series (target variable).\n",
    "    It may reverse pandas series with predictions or simple denormalization of a pandas series depending on the model_name passed.\n",
    "    \n",
    "    parameters:\n",
    "        original_dataset: the original unnormalized dataset \n",
    "        feature: the name of the column to be reversed\n",
    "        model_name (optional): the name of the model to be used for prediction\n",
    "        test_set_series: the test data to be used for predicting the target\n",
    "    returns:\n",
    "        unnormalized_predicted_series: the unnormalized pandas series of the target variable\n",
    "    \"\"\"\n",
    "    # Exract the target variable from the original dataset\n",
    "    original_series = original_dataset[feature]\n",
    "    # Calculate the mean dand std of the original target variable\n",
    "    mean = original_series.mean()\n",
    "    std = original_series.std()\n",
    "    if model_name != None:    \n",
    "        # Predict the target from the test data using the ideal model generated\n",
    "        normalized_predicted = model_name.predict(test_set_series)\n",
    "        # Convert normalized_predicted_data to a pandas series\n",
    "        normalized_predicted_series = pd.Series(normalized_predicted)\n",
    "        # Update the test_set_series if the model exists otherwise normalize the unpredicted original series\n",
    "        test_set_series = normalized_predicted_series\n",
    "    # Calculate the unnormalized predicted series from the normalized predicted series using the mean an std\n",
    "    unnormalized_predicted_series = (test_set_series ) + mean\n",
    "    return unnormalized_predicted_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed83cba-46fa-4156-b723-6fe002318a15",
   "metadata": {},
   "source": [
    "### Generate the unnormalized target values from the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064198b-f9b6-45f8-a048-0c30f48b6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intantiate the reverse normalization function\n",
    "ST100_predicted_data = reverse_normalization(dataset_copied, 'ST100', ST100_X_test, ST100_ideal_rf_model)\n",
    "ST100_predicted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb489c64-5d25-46c1-b587-d9215c5a00cf",
   "metadata": {},
   "source": [
    "### Sklearn Evaluation Functions\n",
    "#### Some of the common evaluations functions are:\n",
    "##### 1. r2_score\n",
    "##### 2. mean_absolute_error\n",
    "##### 3. mean_squared_error\n",
    "##### 4. mean_absolute_percentage_error\n",
    "##### 5. median_absolute_error\n",
    "##### 6. max_error\n",
    "##### 7. explained_variance_score\n",
    "##### 8. OOB (out-of-bag) score is a performance metric for a machine learning model, specifically for ensemble models such as random forests. It is calculated using the samples that are not used in the training of the model, which is called out-of-bag samples. These samples are used to provide an unbiased estimate of the model’s performance, which is known as the OOB score.\n",
    "##### 9. The validation score, on the other hand, is the performance of the model on a validation dataset. This dataset is different from the training dataset and is used to evaluate the model’s performance after it has been trained on the training dataset.\n",
    "### Let's calculate the different metrics using the sklearn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4fa8d-1233-4bf3-8362-187db19c8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation functions sklearn metrics module\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, median_absolute_error, max_error, explained_variance_score\n",
    "\n",
    "np.random.seed(42)\n",
    "# The soil temperature at 100cm (ST100) normalized predicted data \n",
    "ST100_normalized_predicted = ST100_ideal_rf_model.predict(ST100_X_test)\n",
    "\n",
    "# To calculate the mean and std of the original dataset but filtered for outliers, we need to filter out the rows without outliers using their IDs\n",
    "# # Merge the two dataframes on the 'ID' column\n",
    "# merged_data = pd.merge(dataset_copied, dataset_normalized_outlier_filtered['ID'], on='ID', how='inner')\n",
    "# # Create a new dataframe containing only the rows from dataset_copied with 'ID' values found in dataset_normalized_outlier_filtered\n",
    "# data_unnormalized_outlier_filtered = dataset_copied[dataset_copied['ID'].isin(merged_data['ID'])]\n",
    "\n",
    "# The soil temperature at 1000 cm (ST100) unnormalized predicted data\n",
    "ST100_unnormalized_predicted= reverse_normalization(dataset_copied, 'ST100', ST100_X_test, ST100_ideal_rf_model)\n",
    "# The soil temperature at 1000 cm (ST100) unnormalized original data\n",
    "ST100_unnormalized_original = reverse_normalization(dataset_copied, 'ST100', ST100_Y_test)\n",
    "\n",
    "# Evaluation of the normalized target values \n",
    "r2 = r2_score(ST100_Y_test, ST100_normalized_predicted)\n",
    "# OOB_rf_score = ST100_ideal_rf_model.oob_score_\n",
    "mean_abs_err = mean_absolute_error(ST100_Y_test, ST100_normalized_predicted) # Mean absolute error\n",
    "mean_sqr_err =mean_squared_error(ST100_Y_test, ST100_normalized_predicted) # Mean Square error\n",
    "mean_abs_per_err = mean_absolute_percentage_error(ST100_Y_test, ST100_normalized_predicted) # Mean absolute percentage error\n",
    "median_abs_err = median_absolute_error(ST100_Y_test, ST100_normalized_predicted)\n",
    "max_err = max_error(ST100_Y_test, ST100_normalized_predicted)\n",
    "var_exp_err = explained_variance_score(ST100_Y_test, ST100_normalized_predicted)\n",
    "# Evaluation of the unnormalized target values \n",
    "r2_unorm = r2_score(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "mean_abs_err_unorm = mean_absolute_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "mean_sqr_err_unorm = mean_squared_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "mean_abs_per_err_unorm = mean_absolute_percentage_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "median_abs_err_unorm = median_absolute_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "max_err_unorm = max_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "var_exp_err_unorm = explained_variance_score(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "\n",
    "print(\"R^2 Score: Normalized: \", r2, \"Denormalized: \", r2_unorm)\n",
    "# print(\"Out-of-Bag Score: \", OOB_rf_score)\n",
    "print(\"mean_absolute_error: Normalized: \",mean_abs_err, \"Denormalized: \", mean_abs_err_unorm)\n",
    "print(\"mean_squared_error: Normalized: \",mean_sqr_err, \"Denormalized: \",mean_sqr_err_unorm)\n",
    "print(\"mean_absolute_percentage_error: Normalized: \",mean_abs_per_err, \"Denormalized: \",mean_abs_per_err_unorm)\n",
    "print(\"median_abs_err: Normalized: \",median_abs_err, \"Denormalized: \",median_abs_err_unorm)\n",
    "print(\"max_err: Normalized: \",max_err, \"Denormalized: \",max_err_unorm)\n",
    "print(\"var_exp_err: Normalized: \",var_exp_err, \"Denormalized: \",var_exp_err_unorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a0b0a-f48c-4ab3-9f91-1dc6e7bed19d",
   "metadata": {},
   "source": [
    "#### Check the columns of the training and test datasets for matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc609b8-1341-4297-9a36-3b00a9494796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if the columns of the trained dataset and test data match. If they don't match then the prediction won't work.\n",
    "# set(ST100_X_train.columns) - set(ST100_X_test.columns)\n",
    "# # If there is a difference in the columns, you need to manually adjust the missing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704fd95-220c-4013-83a7-c6f13972af19",
   "metadata": {},
   "source": [
    "## 2.8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2513f-0a25-4bf9-98d7-d91d291e227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features were most importance when predicting the target variable ST100 ( soil temperature at 100cm)\n",
    "rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28f2a1-6cfa-4b06-b0c5-2157bab40979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function for plotting feature importance\n",
    "def plot_features(columns, importances, file, title, n=20):\n",
    "    df = (pd.DataFrame({\"features\": columns,\n",
    "                        \"feature_importances\": importances})\n",
    "          .sort_values(\"feature_importances\", ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "    # Plot the dataframe\n",
    "    fig, ax = plt.subplots()\n",
    "    bars = ax.barh(df['features'][:n], df['feature_importances'][:n])\n",
    "    ax.set_title(f\"{title}\")\n",
    "    ax.set_ylabel('Features')\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add annotations on top of the bars\n",
    "    for bar, importance in zip(bars, df['feature_importances'][:n]):\n",
    "        ax.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{importance*100:.5f}', \n",
    "                va='center', ha='left', fontsize=8, color='black')\n",
    "    # Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file, bbox_inches='tight')  # Save as PNG format\n",
    "plot_features(ST100_X_train.columns, rf_model.feature_importances_, 'data/results/ST100_feature_analysis.png', 'ST100 Feature Importances Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ab1fd-a741-4aa5-ab99-31eee3908929",
   "metadata": {},
   "source": [
    "#### NOTE:The feature importance analysis for our best model shows that the soil temperature at 100cm can be predicted from the dataset of the month (9% importance) and the soil temperature at 50cm (90% feature importance) which enables us to predict the target value almost 99% from these values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49282d-fcca-491f-a808-ff768d18df89",
   "metadata": {},
   "source": [
    "## 2.9. Backward Attribute Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a8317-561e-4225-a7cf-c57010f49b08",
   "metadata": {},
   "source": [
    "#### To check if the important features from our dataset (month, ST50 and ST100) can be used to train our model, we need to do the same process we have done before with only these two features considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28c741-b100-4ec8-8158-06eb65e938ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset of the two important columns and the target variable\n",
    "# Initialize the random seed\n",
    "np.random.seed(42)\n",
    "selected_columns = ['month', 'ST50', 'ST100']\n",
    "ST100_dataset_important_features = dataset_normalized_outlier_filtered[selected_columns]\n",
    "# Split the dataset into train and validation set\n",
    "# Let us shuffle the entire dataset so that it is randomly arranged\n",
    "ST100_dataset_shuffled = ST100_dataset_important_features.sample(frac=1)\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_100cm )\n",
    "ST100_X = ST100_dataset_shuffled.drop(\"ST100\", axis=1)\n",
    "ST100_Y = ST100_dataset_shuffled[\"ST100\"]\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(ST100_dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(ST100_dataset_shuffled))\n",
    "ST100_X_train_3, ST100_Y_train_3 = ST100_X[:train_split], ST100_Y[:train_split]\n",
    "ST100_X_valid_3, ST100_Y_valid_3 =ST100_X[train_split:valid_split], ST100_Y[train_split:valid_split]\n",
    "ST100_X_test_3, ST100_Y_test_3 = ST100_X[valid_split:], ST100_Y[valid_split:]\n",
    "# Save the ST100_X_test data to csv for future use\n",
    "ST100_X_test_3.to_csv(\"data/optimized_test_data.csv\", index=False)\n",
    "print(\"ST100 Dataset with Important Features: \\n\")\n",
    "ST100_dataset_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf48679-7ee4-49a9-8be1-c3badb2e53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the RF regressor model\n",
    "ST100_ideal_rf_model.fit(ST100_X_train_3, ST100_Y_train_3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa13c2-b65d-4604-9be3-65468d091758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the scores\n",
    "show_scores(ST100_ideal_rf_model, ST100_X_train_3, ST100_X_valid_3, ST100_Y_train_3, ST100_Y_valid_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47a263-6b0b-434c-958b-89255409235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target ST100 values from the test set\n",
    "ST100_Y_test_preds = ST100_ideal_rf_model.predict(ST100_X_test_3)\n",
    "ST100_Y_test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e59e2d-900c-4f96-8f08-e520380ba5dd",
   "metadata": {},
   "source": [
    "## 2.9. Visualization of original and predicted ST100 values (Blue + Red = Purple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0582db-bfc3-4549-b3f5-d0ada50c18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and Plot the predicted (ST100_Y_preds) and true (J100_ST100_Y_test) traget values\n",
    "ST100_r2_score = r2_score(ST100_Y_test_3, ST100_Y_test_preds)\n",
    "ST100_mae = mean_absolute_error( ST100_Y_test_3, ST100_Y_test_preds)\n",
    "ST100_mse = mean_squared_error( ST100_Y_test_3, ST100_Y_test_preds)\n",
    "# Convert numpy array to pandas Series \n",
    "ST100_Y_test_preds_series = pd.Series( ST100_Y_test_preds)\n",
    "ST100_normalized_df = pd.DataFrame({'Normalized True Values': ST100_Y_test_3, 'Normalized Predicted Values': ST100_Y_test_preds_series})\n",
    "\n",
    "# Create the denormalized ST100 dataframe by using the denormalized outlier filtered original dataset\n",
    "ST100_denormalized_df = pd.DataFrame({'Original True Values': reverse_normalization(dataset_copied, 'ST100', ST100_Y_test_3), 'Original Predicted Values': reverse_normalization(dataset_copied, 'ST100', ST100_Y_test_preds_series)})\n",
    "# Create a figure with two subplots in one row and two columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot for true values\n",
    "sns.scatterplot(data=ST100_normalized_df, x='Normalized True Values', y='Normalized Predicted Values', color='blue', label='Normalized True Values', alpha=0.5, ax=axes[0])\n",
    "\n",
    "# Scatter plot for predicted values\n",
    "sns.scatterplot(data=ST100_normalized_df, x='Normalized True Values', y='Normalized Predicted Values', color='red', label='Normalized Predicted Values', alpha=0.5, ax=axes[0])\n",
    "\n",
    "# Scatter plot for denormalized true values\n",
    "sns.scatterplot(data=ST100_denormalized_df, x='Original True Values', y='Original Predicted Values', color='blue', label='Original True Values', alpha=0.5, ax=axes[1])\n",
    "\n",
    "# Scatter plot for denormalized predicted values\n",
    "sns.scatterplot(data=ST100_denormalized_df, x='Original True Values', y='Original Predicted Values', color='red', label='Original Predicted Values', alpha=0.5, ax=axes[1])\n",
    "\n",
    "# Set titles and labels for each subplot\n",
    "axes[0].set_title('Soil Temperature(100cm) normalized predicted vs true Values')\n",
    "axes[0].set_xlabel('True Values(Normalized)')\n",
    "axes[0].set_ylabel('Predicted Values (Normalized)')\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "\n",
    "axes[1].set_title('Soil Temperature(100cm) denormalized predicted vs true Values')\n",
    "axes[1].set_xlabel('True Values (°C)')\n",
    "axes[1].set_ylabel('Predicted Values (°C)')\n",
    "# Move legend to the upper axis\n",
    "axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "\n",
    "# Add annotations on hover for each subplot\n",
    "mplcursors.cursor(hover=True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "# Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig('data/results/True_values_vs_predicted_values_ST100.png', bbox_inches='tight')  # Save as PNG format\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d7407-ee58-49e7-adae-d86732d38326",
   "metadata": {},
   "source": [
    "#### Note: The above scatter plots show that the true values and predicted values almost overlap which means the error is very small and the colors are mixing up and become indistinguishable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2fefd-ec63-45c5-9424-3b9fc537f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the normalized ST50 values in the X test set\n",
    "ST50_denormalized = reverse_normalization(dataset_copied, 'ST50', ST100_X_test_3['ST50'])\n",
    "# Reverse the normalized ST100 values in the Y test set\n",
    "ST100_Y_test_denormalized = reverse_normalization(dataset_copied, 'ST100', ST100_Y_test_3)\n",
    "# Reverse the normalized predicted ST100 values in the Y test set\n",
    "ST100_Y_preds_denormalized = reverse_normalization(dataset_copied, 'ST100', ST100_Y_test_preds_series)\n",
    "# Make the original and predicted series to have the same index\n",
    "ST100_Y_preds_denormalized.index = ST100_Y_test_denormalized.index\n",
    "ST50_denormalized.index = ST100_Y_test_denormalized.index\n",
    "\n",
    "# Create true and predicted values dataframe for saving\n",
    "ST100_true_and_predicted_values = pd.DataFrame({'ST50 Value':ST50_denormalized,'ST100 True Value': ST100_Y_test_denormalized, 'ST100 Predicted Value': ST100_Y_preds_denormalized})\n",
    "# Save the True and Predicted Values to csv for further comparison\n",
    "ST100_true_and_predicted_values.to_csv('data/ ST100_Y_test_true_and_predicted_values.csv', index=False)\n",
    "plt.figure(figsize=(12, 6)) \n",
    "plt.scatter(ST50_denormalized, ST100_Y_test_denormalized, color='blue', label='True Values') #plotting real points\n",
    "plt.scatter(ST50_denormalized, ST100_Y_preds_denormalized , color='green', label='Predicted Values') #plotting for predicted points\n",
    "   \n",
    "plt.title(\"Soil Temperature (100cm) predicted and true values vs true values of soil temp (50cm)\")\n",
    "plt.xlabel('Soil Temp at 50cm (°C)', fontsize=14)\n",
    "plt.ylabel('Soil Temp at 100cm (°C)', fontsize=14)\n",
    "# Add legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "# Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "plt.savefig('data/results/ST100_true_predicted_values_vs_ST50.png', bbox_inches='tight')  # Save as PNG format\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d2f30-d704-4fcb-8967-da466fcecf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort Y_test and Y_preds in ascending order and reset indices\n",
    "ST100_Y_test_sorted = ST100_Y_test_denormalized.sort_values().reset_index(drop=True)\n",
    "ST100_Y_preds_sorted = ST100_Y_preds_denormalized[ST100_Y_test_denormalized.index].sort_values().reset_index(drop=True)\n",
    "\n",
    "# Calculate mean absolute error\n",
    "ST100_mae = mean_absolute_error(ST100_Y_test_denormalized, ST100_Y_preds_denormalized)\n",
    "# Calculate mean squared error\n",
    "ST100_mse = mean_squared_error(ST100_Y_test_denormalized, ST100_Y_preds_denormalized)\n",
    "# Calculate the R^2 score\n",
    "ST100_r2_score = r2_score(ST100_Y_test_denormalized, ST100_Y_preds_denormalized)\n",
    "# Plot the sorted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ST100_Y_test_sorted.index, ST100_Y_test_sorted, color='blue', label='Observed Values')\n",
    "plt.plot(ST100_Y_preds_sorted.index, ST100_Y_preds_sorted, color='red', label='Predicted Values')\n",
    "# Display the mean absolute error as text annotation\n",
    "plt.text(0.4, 0.95, f'MAE: {ST100_mae:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.6, 0.95, f'MSE: {ST100_mse:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.8, 0.95, f'R^2 score: {ST100_r2_score:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.xlabel('Index', fontsize=14)\n",
    "plt.ylabel('Soil Temperature at 100cm (°C)', fontsize=14)\n",
    "plt.title('Comparison of Observed vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('data/results/ST100_true_predicted_values.png', bbox_inches='tight')  # Save as PNG format\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06ab41-597b-437a-a156-388a46c58489",
   "metadata": {},
   "source": [
    "### Note: The scatter plot above shows that as the ST50 temperature becomes more negative the ST100 prediction capacity becomes weaker because the model is not well trained with higher negative soil temperatures as our training dataset doesn't have enough higher negative temeperatures values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f766d-509e-44be-942a-4e5b8c92fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST100_Y_preds_denormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d3cb7-1e39-4bab-933c-7b975571deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    " ST100_Y_test_denormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee179a4-6b0c-4ccc-92fd-f91280d9b9e3",
   "metadata": {},
   "source": [
    "### Visualizing a Single Decision Tree from the Random Forest Model\n",
    "##### NB: Be patient this may take time. Uncomment and run it to see how the decision tree works but it may be difficult to see the trees due to size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde44db9-2405-4775-8fe3-e40e7cbca054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.tree import plot_tree\n",
    " \n",
    "# # Let's take the ideal rf model trained\n",
    "# # Let us pick on decision tree from the many estimators, e.g., the first tree (index 0)\n",
    "# tree_to_plot = ST100_ideal_rf_model.estimators_[0]\n",
    " \n",
    "# # Plot the decision tree\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# plot_tree(tree_to_plot, feature_names=ST100_dataset_shuffled.columns.tolist(), filled=True, rounded=True, fontsize=10)\n",
    "# plt.title(\"Decision Tree from Random Forest\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b76d4c-0d4e-4c5b-8e10-ba369d5178fb",
   "metadata": {},
   "source": [
    "### Define a function to calculate the minimum and maximum resolution of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acb129-c381-4703-a2d9-0b901db37369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the minimum resolution and maximum difference of a dataframe column\n",
    "def calculate_resolution_for_column(df, column_name):\n",
    "    # Extract the specified column\n",
    "    column_values = df[column_name]\n",
    "\n",
    "    # Find the minimum and maximum values\n",
    "    min_value = column_values.min()\n",
    "    max_value = column_values.max()\n",
    "\n",
    "    # Sort the values\n",
    "    sorted_values = column_values.sort_values().values\n",
    "\n",
    "    # Calculate the differences between adjacent values\n",
    "    differences = sorted_values[1:] - sorted_values[:-1]\n",
    "\n",
    "    # Find the smallest non-zero difference\n",
    "    min_resolution = min(differences[differences > 0], default=None)\n",
    "\n",
    "    # Find the largest difference\n",
    "    max_resolution = max(differences)\n",
    "\n",
    "    return min_resolution, max_resolution\n",
    "min_resolution, max_resolution = calculate_resolution_for_column(dataset_copied, 'ST100')\n",
    "print('Minimum ST100 Resolution:', min_resolution, '\\n Maximum Difference: ', max_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc0d1e-ae18-4864-9525-49331556f88f",
   "metadata": {},
   "source": [
    "# 3. Prediction of the soil temperatures at different depths\n",
    "## 3.1. Define a custom prediction function \n",
    "### The custom function can be adapted for all predicitons of soil temperatures at different soil depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e2723-9a87-43b9-8450-44d23a9c3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first predict the soil temperature at 50cm from all other independent variables except the ST100 (soil temp at 100cm)\n",
    "# Define X_train_2 outside of the function\n",
    "X_train_2 = None  \n",
    "X_test_2 = None \n",
    "Y_train_2 = None\n",
    "Y_test_2 = None\n",
    "def predict_feature(dataset_df, features_dropped, feature, model_type):\n",
    "    new_dataset_normalized = dataset_df.drop(features_dropped, axis=1)\n",
    "    X_f = new_dataset_normalized.drop(feature, axis=1)\n",
    "    Y_f = new_dataset_normalized[feature]\n",
    "    # Split the X and Y data in to train and test data\n",
    "    X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(X_f, Y_f, test_size=0.15, train_size=0.7)\n",
    "    model_type.fit(X_train_2, Y_train_2)\n",
    "    scores = show_scores(model_type, X_train_2, X_test_2, Y_train_2, Y_test_2)\n",
    "    print(scores)\n",
    "    return X_train_2, X_test_2, Y_train_2, Y_test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309821c-c748-4cf5-923a-e04ea77e1ed4",
   "metadata": {},
   "source": [
    "## 3.2. Prediction of Soil temperature at 50cm \n",
    "#### Consider all other independent variables in the original dataset with the ST100 (soil temp at 100cm) dropped out\n",
    "#### Use the original dataset with outliers to see the effect of the outliers on the modelling\n",
    "#### NB: This may take time. Be Patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d86f9a-57cf-4f25-83f7-5f63cef3996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instantiate the custom predict_feature function to predict the ST50 values\n",
    "# Random Forest Regressor\n",
    "print(\"Random Forest Metrics:\")\n",
    "ST50_X_train_2, ST50_X_test_2, ST50_Y_train_2, ST50_Y_test_2 = predict_feature(dataset_normalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_rf_model)\n",
    "print('\\n')\n",
    "# Ridge Regressor\n",
    "print(\"Ridge Regressor Metrics:\")\n",
    "predict_feature(dataset_normalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_rg_model)\n",
    "print('\\n')\n",
    "# Lasso Regressor\n",
    "print(\"Lasso Regressor Metrics:\")\n",
    "predict_feature(dataset_normalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_la_model)\n",
    "print('\\n')\n",
    "# ElasticNet Regressor\n",
    "print(\"ElasticNet Regressor Metrics:\")\n",
    "predict_feature(dataset_normalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_en_model)\n",
    "print('\\n')\n",
    "# SVR linear Regressor\n",
    "print(\"SVR linear Regressor Metrics:\")\n",
    "predict_feature(dataset_normalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_svrl_model)\n",
    "print('\\n')\n",
    "# SVR rbf Regressor\n",
    "print(\"SVR rbf Regressor Metrics:\")\n",
    "predict_feature(dataset_normalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_svrr_model)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4acea-189d-4110-97a0-0619b1565f6d",
   "metadata": {},
   "source": [
    "## 3.3. Predictions of other soil temperatures (100cm, 50cm, 20cm, 10cm, 5cm, 2cm)\n",
    "#### As the RandomForestRegressor is working good for the prediction let us consider RF to predict the soil temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb8a60-5e8e-4d02-a4e4-831303ec92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom predict_feature function to predict the ST2, ST5, ST10, ST20 values\n",
    "# Random Forest Regressor ST100\n",
    "print(\"Random Forest Metrics ST100:\")\n",
    "ST100_X_train_2, ST100_X_test_2, ST100_Y_train_2, ST100_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ID'], 'ST50', ST100_rf_model)\n",
    "print('\\n')\n",
    "# Random Forest Regressor ST50\n",
    "print(\"Random Forest Metrics ST50:\")\n",
    "ST50_X_train_2, ST50_X_test_2, ST50_Y_train_2, ST50_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_rf_model)\n",
    "print('\\n')\n",
    "# Random Forest Regressor ST20\n",
    "print(\"Random Forest Metrics ST20:\")\n",
    "ST20_X_train_2, ST20_X_test_2, ST20_Y_train_2, ST20_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ID'], 'ST20', ST20_rf_model)\n",
    "print('\\n')\n",
    "# Random Forest Regressor ST10\n",
    "print(\"Random Forest Metrics ST10:\")\n",
    "ST10_X_train_2, ST10_X_test_2, ST10_Y_train_2, ST10_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20','ID'], 'ST10', ST10_rf_model)\n",
    "print('\\n')\n",
    "# Random Forest Regressor ST5\n",
    "print(\"Random Forest Metrics ST5:\")\n",
    "ST5_X_train_2, ST5_X_test_2, ST5_Y_train_2, ST5_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20','ST10','ID'], 'ST5', ST5_rf_model)\n",
    "print('\\n')\n",
    "# Random Forest Regressor ST2\n",
    "print(\"Random Forest Metrics ST2:\")\n",
    "ST2_X_train_2, ST2_X_test_2, ST2_Y_train_2, ST2_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20','ST10','ST5','ID'], 'ST2', ST2_rf_model)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc92e82-b50f-4f22-955a-0d478e8896e8",
   "metadata": {},
   "source": [
    "#### ST2 Feature Importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db39cd1-647f-41a3-b006-b9356a4b1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature parameter analysis for ST2\n",
    "ST2_X_train_2, ST2_X_test_2, ST2_Y_train_2, ST2_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20','ST10','ST5','ID'], 'ST2', ST2_rf_model)\n",
    "plot_features(ST2_X_train_2.columns, ST2_rf_model.feature_importances_, 'data/results/ST2_feature_analysis.png', 'ST2 Feature Importance Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4c167-c635-49d6-b229-562a95d40c24",
   "metadata": {},
   "source": [
    "#### ST5 Feature Importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30c377-db95-4d96-b3b1-6200a7d6fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature parameter analysis for ST5\n",
    "ST5_X_train_2, ST5_X_test_2, ST5_Y_train_2, ST5_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20','ST10','ID'], 'ST5', ST5_rf_model)\n",
    "plot_features(ST5_X_train_2.columns, ST5_rf_model.feature_importances_, 'data/results/ST5_feature_analysis.png', 'ST5 Feature Importances Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62725279-f5d0-4dea-838d-5926eb985513",
   "metadata": {},
   "source": [
    "#### ST10 Feature Importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f4b66-2e45-46aa-a700-ff51f179debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature parameter analysis for ST10\n",
    "ST10_X_train_2, ST10_X_test_2, ST10_Y_train_2, ST10_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20', 'ID'], 'ST10', ST10_rf_model)\n",
    "plot_features(ST10_X_train_2.columns, ST10_rf_model.feature_importances_, 'data/results/ST10_feature_analysis.png', 'ST10 Feature Importances Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c8642-ac6a-47fe-8489-cb296085cd2a",
   "metadata": {},
   "source": [
    "#### ST20 Feature Importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be745361-9b47-4f76-8227-51395613e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature parameter analysis for ST20\n",
    "ST20_X_train_2, ST20_X_test_2, ST20_Y_train_2, ST20_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ID'], 'ST20', ST20_rf_model)\n",
    "plot_features(ST20_X_train_2.columns, ST20_rf_model.feature_importances_, 'data/results/ST20_feature_analysis.png', 'ST20 Feature Importances Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce5503-0c4d-4e54-bdd0-ca5070aab317",
   "metadata": {},
   "source": [
    "#### ST50 Feature Importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f4b3e-e2ba-48e4-88d9-f3113868b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature parameter analysis for ST50\n",
    "ST50_X_train_2, ST50_X_test_2, ST50_Y_train_2, ST50_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_rf_model)\n",
    "plot_features(ST50_X_train_2.columns, ST50_rf_model.feature_importances_, 'data/results/ST50_feature_analysis.png', 'ST50 Feature Importances Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a5d2d-fc8c-449a-bd30-d21fa85f4444",
   "metadata": {},
   "source": [
    "#### Feature Importance Conclusion: From the Feature Importance Analysis, the prediction of the soil temperature at a certain depth depeneds majorly on the depth above it and the soil temperature at 2cm, depends on the mean air temperature (92%), evaporation (3%), month(2%), day, heat flux and snow depth (three of them combined less than 3%)\n",
    "#### If we predict the soil temperature at 2cm from mean air temperature, evaporation, month, day, heat flux and snow depth, we can predict the other soil temperatures at different depths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046c514-e1cd-4272-83f6-45d9521fab95",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "#### To check if the model is not overfitting, we can evaluate the learning curve for an increasing training data set size. The following chart shows how the training set and validation set MAE and MSE change with respect to change of training dataset size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05c56b-117a-44fc-b688-f3f05bcadaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def plot_learning_curves(model, X_train, Y_train, X_valid, Y_valid, feature):\n",
    "    train_errors_mae, val_errors_mae = [], []\n",
    "    train_errors_mse, val_errors_mse = [], []\n",
    "    \n",
    "    # Use different sizes of training subsets\n",
    "    subset_sizes = np.linspace(1, len(X_train), 5, dtype=int)\n",
    "    \n",
    "    for m in subset_sizes:\n",
    "        model.fit(X_train[:m], Y_train[:m])\n",
    "        Y_train_predict = model.predict(X_train[:m])\n",
    "        Y_valid_predict = model.predict(X_valid)\n",
    "        \n",
    "        train_errors_mae.append(mean_absolute_error(Y_train[:m], Y_train_predict))\n",
    "        val_errors_mae.append(mean_absolute_error(Y_valid, Y_valid_predict))\n",
    "        \n",
    "        train_errors_mse.append(mean_squared_error(Y_train[:m], Y_train_predict))\n",
    "        val_errors_mse.append(mean_squared_error(Y_valid, Y_valid_predict))\n",
    "        \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot MAE learning curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(subset_sizes, train_errors_mae, \"r-\", label=\"Training MAE\")\n",
    "    plt.plot(subset_sizes, val_errors_mae, \"b-\", label=\"Validation MAE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"MAE Learning Curves\")\n",
    "    \n",
    "    # Plot MSE learning curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(subset_sizes, train_errors_mse, \"r-\", label=\"Training MSE\")\n",
    "    plt.plot(subset_sizes, val_errors_mse, \"b-\", label=\"Validation MSE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"MSE Learning Curves\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves(ST2_rf_model, ST2_X_train, ST2_Y_train, ST2_X_valid, ST2_Y_valid, \"ST2\")\n",
    "plot_learning_curves(ST5_rf_model, ST5_X_train_2, ST5_Y_train_2, ST5_X_test_2, ST5_Y_test_2, \"ST5\")\n",
    "plot_learning_curves(ST10_rf_model, ST10_X_train_2, ST10_Y_train_2, ST10_X_test_2, ST10_Y_test_2, \"ST10\")\n",
    "plot_learning_curves(ST20_rf_model, ST20_X_train_2, ST20_Y_train_2, ST20_X_test_2, ST20_Y_test_2, \"ST20\")\n",
    "plot_learning_curves(ST50_rf_model, ST50_X_train_2, ST50_Y_train_2, ST50_X_test_2, ST50_Y_test_2, \"ST50\")\n",
    "plot_learning_curves(ST100_rf_model, ST100_X_train_2, ST100_Y_train_2, ST100_X_test_2, ST100_Y_test_2, \"ST100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99bb44-a88a-4114-bb9a-6204bbd7b743",
   "metadata": {},
   "source": [
    "### Learning Curve Results analysis\n",
    "#### 1. MAE and MSE decreasing for validation Set: The decreasing trend of MAE and MSE for the validation set indicates that the model's performance improves as more data is provided for training. This suggests that the model is learning from the additional data and making more accurate predictions on unseen data.\n",
    "#### 2. MAE and MSE Plateauing: Once a certain size of the training set is reached, both MAE and MSE plateau, indicating that providing more training data beyond this point does not significantly improve the model's performance on the validation set. This suggests that the model has learned as much as it can from the available data, and adding more data does not lead to substantial improvements\n",
    "#### 3. Slow Slope of MSE for Training Set: The slow slope of MSE for the training set suggests that the model's performance on the training data is relatively stable even as more data is added. This indicates that the model is not suffering from high variance (overfitting) with respect to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cee30-2c15-4aac-b72e-217e76deae90",
   "metadata": {},
   "source": [
    "### Advantages of Random Forest Regression\n",
    "1. It is easy to use and less sensitive to the training data compared to the decision tree.\n",
    "2. It is more accurate than the decision tree algorithm.\n",
    "3. It is effective in handling large datasets that have many attributes.\n",
    "4. It can handle missing data, outliers, and noisy features.\n",
    "### Disadvantages of Random Forest Regression\n",
    "1. The model can also be difficult to interpret.\n",
    "2. This algorithm may require some domain expertise to choose the appropriate parameters like the number of decision trees, the maximum depth of each tree, and the number of features to consider at each split.\n",
    "4. It is computationally expensive, especially for large datasets.\n",
    "5. It may suffer from overfitting if the model is too complex or the number of decision trees is too high.\n",
    "### The major issue of overfitting probability can be checked by\n",
    "1. Splitting the dataset in to training, validation and test sets and the model performed well in all sets\n",
    "2. Cross-validation performance of the model for different k-fold cross-validation helps us to see if the model is not overfitting and was checked that it is performing good.\n",
    "3. Learning curves that show the model's performance (e.g., mean squared error or mean absolute error) on the training and validation sets as a function of the training set size. If the model's performance on the training set continues to improve while the performance on the validation set plateaus or worsens, it may be overfitting.\n",
    "4. Feature Importance: Check the feature importances provided by the random forest model. If certain features have very high importance values while others have low or zero importance, it may indicate that the model is overfitting to those important feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37389ba-7053-4144-8002-23f03f54860b",
   "metadata": {},
   "source": [
    "# 4. Saving and loading a trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b321cd4-061c-4575-94c1-e283619cf83d",
   "metadata": {},
   "source": [
    "#### a. Using Pickle module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a69fb-912d-488f-879c-36bbe008fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model you created to file\n",
    "# pickle.dump(ST100_ideal_rf_model, open(\"models/model_Temperature_100cm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af04c5c-0654-4716-a27e-9f8f84e1c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved model\n",
    "# loaded_pickle_model = pickle.load(open(\"models/model_Temperature_100cm.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689ee7f-81a1-467e-8d6f-2f556b63f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the score\n",
    "# loaded_pickle_model.score(ST100_X_test_3, ST100_Y_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c94fe9-3a8e-411b-801a-a03b3e4e93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if the model loaded works\n",
    "# pickle_Y_preds = loaded_pickle_model.predict(ST100_X_test_3)\n",
    "# pickle_Y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70c895-9a40-4309-9907-ab7d726e967f",
   "metadata": {},
   "source": [
    "#### b. Using Joblib module\n",
    "##### NB: Go for Joblib if the data used for modelling is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633bc46-8dd3-45e0-853b-34352da766e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to file\n",
    "dump(ST100_ideal_rf_model, filename=\"models/model_Temperature_100cm_joblib.joblib\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3aba9a-51a4-45b4-94c1-a3d1d9a3a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import saved joblib model\n",
    "loaded_joblib_model = load(filename=\"models/model_Temperature_100cm_joblib.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472f457-15d5-406b-bd39-3af49a0d30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the score and prediction\n",
    "loaded_joblib_model.score(ST100_X_test_3, ST100_Y_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be142997-3b4e-45c9-bb16-dd661f4d1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_joblib_model.predict(ST100_X_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39375f99-9558-4524-9227-a45a7d5213f2",
   "metadata": {},
   "source": [
    "# 5. Forecast of Soil Temperature for all depths at once\n",
    "### Let's check if it possible to use the RF model trained above to forecast the soil temperatures at different depths from the forecasted evaporation, data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb77cd-81e1-412a-ab1f-3fb2ba543260",
   "metadata": {},
   "source": [
    "### Let's bring all Code in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e91a8d-41b2-4202-936b-370ce607dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Import Important modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import mplcursors\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, median_absolute_error, max_error, explained_variance_score\n",
    "\n",
    "# Read CSV file\n",
    "dataset = pd.read_csv(\"data/NMBUBigDatasetFinal.csv\", low_memory=False)\n",
    "\n",
    "# 1. Data Preprocessing stage =========================================\n",
    "\n",
    "# Add a year, month and day columns by feature engineering to make manipulation of these easy\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "dataset_copied = dataset.copy()\n",
    "dataset_copied['year'] = dataset_copied['date'].dt.year\n",
    "dataset_copied['month'] = dataset_copied['date'].dt.month\n",
    "dataset_copied['day'] = dataset_copied['date'].dt.day\n",
    "\n",
    "# Filter rows where 'evaporation_mm' is less than or equal to 10 \n",
    "dataset_copied = dataset_copied[(dataset_copied['evaporation_mm'] <= 10) | (pd.isna(dataset_copied['evaporation_mm']))]\n",
    "\n",
    "# Calculate the mean for evaporation_mm considering only non-NaN values for each day and month\n",
    "mean_values = dataset_copied.groupby(['day', 'month'])['evaporation_mm'].apply(lambda x: x.dropna().mean()).reset_index(name='evaporation_mm_mean')\n",
    "\n",
    "# Merge mean_values with the original DataFrame to fill missing values\n",
    "dataset_copied = pd.merge(dataset_copied, mean_values, on=['day', 'month'], how='left')\n",
    "\n",
    "# Fill missing values for evaporation_mm with the mean values calculated for the specific range of years\n",
    "def fill_missing_evaporation(row):\n",
    "    if pd.isna(row['evaporation_mm']):\n",
    "        # Calculate the mean value for the specific range of years\n",
    "        filtered_means = mean_values[(mean_values['day'] == row['day']) & (mean_values['month'] == row['month'])]\n",
    "        mean_value = filtered_means['evaporation_mm_mean'].mean() if not filtered_means.empty else None\n",
    "        return mean_value\n",
    "    else:\n",
    "        return row['evaporation_mm']\n",
    "\n",
    "# Fill missing values for evaporation_mm with the mean values calculated for the specific range of years\n",
    "dataset_copied['evaporation_mm'] = dataset_copied.apply(fill_missing_evaporation, axis=1)\n",
    "# Drop auxiliary columns\n",
    "dataset_copied.drop('evaporation_mm_mean', axis=1, inplace=True)\n",
    "# Drop duplicates in case there are overlapping values from the merge\n",
    "dataset_copied.drop_duplicates(inplace=True)\n",
    "\n",
    "# Filter the dataframe to select rows where evaporation_mm is NaN\n",
    "nan_evaporation = dataset_copied[dataset_copied['evaporation_mm'].isna()]\n",
    "# Group by month and count NaN occurrenc\n",
    "nan_evaporation_counts = nan_evaporation.groupby('month').size()\n",
    "\n",
    "# Fill the missing evaporation_mm values by zero\n",
    "dataset_copied['evaporation_mm'] = dataset_copied['evaporation_mm'].fillna(0.0)\n",
    "\n",
    "# Merge the two dataframes one from MET Norway and the dataset_copied based on the 'date' column\n",
    "snow_thickness = pd.read_csv('data/surface_snow_daily_2016_2024.csv')\n",
    "snow_thickness['date'] = pd.to_datetime(snow_thickness['date'])\n",
    "# Replace all values of snow_thickness['snow_depth_cm'] equal to -1 with 0 because -1 in MET Frost API indicates it is a very low or zero snow depth\n",
    "snow_thickness.loc[snow_thickness['snow_depth_cm'] == -1, 'snow_depth_cm'] = 0\n",
    "# Merge the two dataframes based on the 'date' column\n",
    "merged_df = pd.merge(dataset_copied, snow_thickness, on='date', suffixes=('_copied', '_thickness'), how='left')\n",
    "# Copy values from 'snow_depth_cm_thickness' to 'snow_depth_cm_copied' where 'snow_depth_cm_copied' is NaN\n",
    "merged_df['snow_depth_cm_copied'] = merged_df['snow_depth_cm_copied'].fillna(merged_df['snow_depth_cm_thickness'])\n",
    "# Drop the 'snow_depth_cm_thickness' column\n",
    "merged_df.drop(['snow_depth_cm_thickness','index','elementId','sourceId'], axis=1, inplace=True)\n",
    "# If needed, you can rename the 'snow_depth_cm_copied' column back to 'snow_depth_cm'\n",
    "merged_df.rename(columns={'snow_depth_cm_copied': 'snow_depth_cm'}, inplace=True)\n",
    "# Now, merged_df contains the updated snow depth values in the dataset_copied dataframe\n",
    "# To overwrite the original dataset_copied with the updated values:\n",
    "dataset_copied = merged_df.copy()\n",
    "\n",
    "# Define a generic function that is used for filling missing values different columns\n",
    "def fill_missing_values(row_data, dataset_passed, column_name):\n",
    "    \"\"\"\n",
    "    Fill missing values for a specified column based on conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        row_data (pandas.Series): A single row_data of the DataFrame.\n",
    "        dataset (pandas.DataFrame): The DataFrame containing the dataset.\n",
    "        column_name (str): The name of the column to fill missing values for.\n",
    "    \n",
    "    Returns:\n",
    "        float: The filled value for the specified column.\n",
    "    \"\"\"\n",
    "    if pd.isna(row_data[column_name]):\n",
    "        # Extract year and month from the current row_data\n",
    "        year = row_data['year']\n",
    "        month = row_data['month']\n",
    "        \n",
    "        # Check if there are any non-NaN values for the same month and year\n",
    "        same_month_year = dataset_passed[(dataset_passed['year'] == year) & (dataset_passed['month'] == month)]\n",
    "        valid_values = same_month_year.dropna(subset=[column_name])\n",
    "        \n",
    "        if not valid_values.empty:\n",
    "            # Calculate the mean of non-NaN values for the same month and year\n",
    "            mean_value = valid_values[column_name].mean()\n",
    "        else:\n",
    "            # Calculate the mean of non-NaN values for the same month in other years\n",
    "            other_years = dataset_passed[(dataset_passed['month'] == month) & (dataset_passed['year'] != year)]\n",
    "            other_years_valid_value = other_years.dropna(subset=[column_name])\n",
    "            mean_value = other_years_valid_value[column_name].mean()\n",
    "        \n",
    "        return mean_value\n",
    "    else:\n",
    "        # If the value is not NaN, return the original value\n",
    "        return row_data[column_name]\n",
    "\n",
    "def fill_missing_by_monthly_mean(column_name, dataset_passed):\n",
    "    dataset_passed[column_name] = dataset_passed.apply(lambda row_data: fill_missing_values(row_data, dataset_passed, column_name), axis=1)\n",
    "\n",
    "# Fill the missing values for all features by the monthly mean of that specific year or mean value of that month across all other years if the month of that specific year is NaN\n",
    "fill_missing_by_monthly_mean(\"evaporation_mm\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"ST2\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"ST5\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"ST10\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"ST20\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"ST50\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"ST100\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"relative_humidity\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"air_pressure_2m_mbar\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"radiation_balance_w_m2\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"albedo_RR_GR\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"earth_heat_flux_MJ_m2\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"precipitation_mm\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"snow_depth_cm\", dataset_copied)\n",
    "fill_missing_by_monthly_mean(\"phosynthetic_active_radiation_mE_m2\", dataset_copied)\n",
    "\n",
    "# Extract the features list from the dataset columns\n",
    "features = dataset_copied.columns.tolist()\n",
    "# Temporary reomve the ID from the features list\n",
    "features.remove('ID')\n",
    "# reomve the date feature from the features list\n",
    "features.remove('date')\n",
    "# reomve the year feature from the features list\n",
    "features.remove('year')\n",
    "\n",
    "# Create a local copy of the dataset and drop the date and year features\n",
    "local_dataset = dataset_copied.drop(['ID','date', 'year'], axis=1)\n",
    "# Keep the original dataset's means of each feature for later use in denormalization\n",
    "mean_original = local_dataset[features].mean()\n",
    "# Keep the original dataset's standard deviationa of each feature for later use in denormalization\n",
    "std_original = local_dataset[features].std()\n",
    "# Calculate Z-score for all features\n",
    "zscore_df = (local_dataset[features] - mean_original) / std_original\n",
    "# Create a new DataFrame to store the normalized values\n",
    "dataset_normalized = zscore_df.copy()\n",
    "# copy the ID column from the original dataset to the dataset_normalized\n",
    "dataset_normalized['ID'] = dataset_copied['ID']\n",
    "# Save the normalized dataset to file\n",
    "dataset_normalized.to_csv('data/dataset_normalized.csv', index=False)\n",
    "dataset_copied.to_csv('data/dataset_unnormalized.csv', index=False)\n",
    "\n",
    "# Get the list of features from our dataset columns\n",
    "features = dataset_normalized.columns.tolist()\n",
    "# remove the month feature from the features list\n",
    "features.remove('month')\n",
    "# remove the day feature from the features list\n",
    "features.remove('day')\n",
    "# remove the ID from the features list\n",
    "features.remove('ID')\n",
    "# The thresholds of the DataFrame normalized 'dataset_normalized' (already z-score normalized)\n",
    "zscore_thresholds = {\n",
    "    'mean_air_temperature_2m': (-4, 3),\n",
    "    'min_air_temperature_2m': (-4, 3),\n",
    "    'max_air_temperature_2m': (-4, 3),\n",
    "    'relative_humidity': (-4, 2),\n",
    "    'air_pressure_2m_mbar': (-5, 5),\n",
    "    'precipitation_mm': (-1, 8),\n",
    "    'evaporation_mm': (-1, 4),\n",
    "    'earth_heat_flux_MJ_m2': (-5, 5),\n",
    "    'ST2': (-2, 2.5),\n",
    "    'ST5': (-2, 2.5),\n",
    "    'ST10': (-2, 2.5),\n",
    "    'ST20': (-2, 2.5),\n",
    "    'ST50': (-2, 2.5),\n",
    "    'radiation_balance_w_m2': (-2, 3),\n",
    "    'phosynthetic_active_radiation_mE_m2': (-1.5, 3),\n",
    "    'albedo_RR_GR': (-15, 5),\n",
    "    'snow_depth_cm': (-1, 12),\n",
    "    'ST100': (-2, 2.5)\n",
    "}\n",
    "# Identify outliers based on Z-score for all features using different thresholds\n",
    "outliers_zscore_dict = {}\n",
    "for feature in features:\n",
    "    lower_threshold, upper_threshold = zscore_thresholds.get(feature)  \n",
    "    if upper_threshold is None or lower_threshold is None:\n",
    "        continue\n",
    "    outliers_zscore = dataset_normalized[(dataset_normalized[feature]> upper_threshold) | (dataset_normalized[feature] < lower_threshold)]\n",
    "    if not outliers_zscore.empty:\n",
    "        outliers_zscore_dict[feature] = outliers_zscore[feature].tolist()\n",
    "\n",
    "# Calculate IQR for all features\n",
    "Q1 = dataset_normalized[features].quantile(0.25)\n",
    "Q3 = dataset_normalized[features].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identify outliers based on IQR for all features\n",
    "outliers_iqr_dict = {}\n",
    "for feature in features:\n",
    "    outliers_iqr = dataset_normalized[(dataset_normalized[feature] < Q1[feature] - 1.5 * IQR[feature]) | \n",
    "                                               (dataset_normalized[feature] > Q3[feature] + 1.5 * IQR[feature])]\n",
    "    if not outliers_iqr.empty:\n",
    "        outliers_iqr_dict[feature] = outliers_iqr[feature].tolist()\n",
    "\n",
    "# Deifne the function to remove the outliers from the dataset\n",
    "def remove_outliers(df, thresholds):\n",
    "    \"\"\"\n",
    "    Removes outliers from a DataFrame based on z-score thresholds.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame (already z-score normalized).\n",
    "        thresholds (dict): A dictionary containing feature names as keys and (lower, upper) z-score thresholds as values.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    for feature, (lower, upper) in thresholds.items():\n",
    "        df = df[(df[feature] >= lower) & (df[feature] <= upper)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Remove the outliers and generate a new normalized filtered dataset without outliers\n",
    "dataset_normalized_outlier_filtered = remove_outliers(dataset_normalized, zscore_thresholds)\n",
    "# Outliers removed denormalized dataset\n",
    "# Extract the features list from the dataset columns\n",
    "features = dataset_normalized_outlier_filtered.columns.tolist()\n",
    "# Temporary reomve the ID from the features list\n",
    "features.remove('ID')\n",
    "# Using the original mean and std to covert back to the original scale\n",
    "dataset_denormalized_outlier_filtered = (dataset_normalized_outlier_filtered[features] *std_original) + mean_original\n",
    "dataset_denormalized_outlier_filtered['ID'] = dataset_normalized_outlier_filtered['ID']\n",
    "\n",
    "# Round the dataframe values to 4 decimal points\n",
    "dataset_normalized_outlier_filtered = dataset_normalized_outlier_filtered.round(6)\n",
    "dataset_denormalized_outlier_filtered = dataset_denormalized_outlier_filtered.round(4)\n",
    "\n",
    "# Save the outliers filtered normalized dataset in file\n",
    "dataset_normalized_outlier_filtered.to_csv('data/dataset_normalized_outlier_filtered.csv', index=False)\n",
    "# Save the outliers filtered denormalized dataset in file\n",
    "dataset_denormalized_outlier_filtered.to_csv('data/dataset_denormalized_outlier_filtered.csv', index=False)\n",
    "\n",
    "#### Let us check if our dataset is all numeric in addition to the info() method\n",
    "for label, content in dataset_normalized_outlier_filtered.items():\n",
    "    if not pd.api.types.is_numeric_dtype(content):\n",
    "        print('Non-numeric column: ', label)\n",
    "\n",
    "# 2. Modeling stage ============================================================\n",
    "\n",
    "# Let us shuffle the entire dataset so that it is randomly arranged\n",
    "np.random.seed(42)\n",
    "dataset_shuffled = dataset_normalized_outlier_filtered.sample(frac=1)\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_100cm ). Drop the ID as it is not a feature\n",
    "X = dataset_shuffled.drop(['ID','ST100'], axis=1)\n",
    "Y = dataset_shuffled[\"ST100\"]\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(dataset_shuffled))\n",
    "ST100_X_train, ST100_Y_train = X[:train_split], Y[:train_split]\n",
    "ST100_X_valid, ST100_Y_valid =X[train_split:valid_split], Y[train_split:valid_split]\n",
    "ST100_X_test, ST100_Y_test = X[valid_split:], Y[valid_split:]\n",
    "# Save the ST100_X_test data to csv for future use\n",
    "ST100_X_test.to_csv(\"data/ST100_X_test_data.csv\", index=False)\n",
    "\n",
    "# Create evaluation metrics function that shows the metrics result of different metrics for a model\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, mean_absolute_percentage_error, max_error, explained_variance_score\n",
    "def rmsle(Y_test, Y_preds):\n",
    "    \"\"\"\n",
    "    Calculates the root mean squared log error between predictions and true labels\n",
    "        \n",
    "    Parameters:\n",
    "        Y_test: A test dataset of the target label.\n",
    "        Y_preds: The predicted values of the target label.\n",
    "            \n",
    "    Returns:\n",
    "        float: the root mean squared log error of the model.\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_log_error(Y_test, Y_preds))\n",
    "\n",
    "# Create function to evaluate model on few different levels\n",
    "def show_scores(model, X_train, X_valid, Y_train, Y_valid):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    scores = {\"Training Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_valid, val_preds),\n",
    "              \"Training Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds),             \n",
    "              \"Training Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),\n",
    "              \"Training Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Training Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              \"Training Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              \"Training Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)}\n",
    "    return scores\n",
    "\n",
    "# Fitting the models\n",
    "\n",
    "# A. RandomForestRegressor\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST50_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST20_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST10_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST5_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST2_rf_model = RandomForestRegressor(n_estimators=300, \n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_samples_split=2,\n",
    "                                     max_features='sqrt',\n",
    "                                     max_depth=None,\n",
    "                                     bootstrap=False,\n",
    "                                     random_state=42)\n",
    "ST100_rf_all_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST50_rf_all_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST20_rf_all_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST10_rf_all_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST5_rf_all_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "ST2_rf_all_model = RandomForestRegressor(n_estimators=300, \n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_samples_split=2,\n",
    "                                     max_features='sqrt',\n",
    "                                     max_depth=None,\n",
    "                                     bootstrap=False,\n",
    "                                     random_state=42)\n",
    "# Fit the model for ST100 to start with\n",
    "ST100_rf_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================Random Forest The Evaluation Metrics Results For ST100 Normalized =======================\\n\")\n",
    "# Access the OOB Score\n",
    "oob_score = ST100_rf_model.oob_score_\n",
    "print('Out of Bag Score: ', oob_score)\n",
    "print(show_scores(ST100_rf_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# B. Ridge Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_rg_model = Ridge()\n",
    "ST50_rg_model = Ridge()\n",
    "ST20_rg_model = Ridge()\n",
    "ST10_rg_model = Ridge()\n",
    "ST5_rg_model = Ridge()\n",
    "ST2_rg_model = Ridge()\n",
    "# Fit the ST100 model for soil temp at 100 cm\n",
    "ST100_rg_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Ridge Regressor Evaluation Metrics Results For ST100 Normalized =======================\\n\")\n",
    "print(show_scores(ST100_rg_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# B. Lasso Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create Lasso model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_la_model = Lasso()\n",
    "ST50_la_model = Lasso()\n",
    "ST20_la_model = Lasso()\n",
    "ST10_la_model = Lasso()\n",
    "ST5_la_model = Lasso()\n",
    "ST2_la_model = Lasso()\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_la_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Lasso Regressor Evaluation Metrics Results For ST100 Normalized =======================\\n\")\n",
    "print(show_scores(ST100_la_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_en_model = ElasticNet()\n",
    "ST50_en_model = ElasticNet()\n",
    "ST20_en_model = ElasticNet()\n",
    "ST10_en_model = ElasticNet()\n",
    "ST5_en_model = ElasticNet()\n",
    "ST2_en_model = ElasticNet()\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_en_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The ElasticNet Regressor Evaluation Metrics Results For ST100 Normalized =======================\\n\")\n",
    "print(show_scores(ST100_en_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid))\n",
    "print(\"=========================================================================================================\\n\")\n",
    "\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_svrl_model = SVR(kernel='linear')\n",
    "ST50_svrl_model = SVR(kernel='linear')\n",
    "ST20_svrl_model = SVR(kernel='linear')\n",
    "ST10_svrl_model = SVR(kernel='linear')\n",
    "ST5_svrl_model = SVR(kernel='linear')\n",
    "ST2_svrl_model = SVR(kernel='linear')\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_svrl_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with linear model Evaluation Metrics Results For ST100 Normalized =======================\\n\")\n",
    "print(show_scores(ST100_svrl_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid))\n",
    "print(\"==========================================================================================================\\n\")\n",
    "\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_svrr_model = SVR(kernel='rbf')\n",
    "ST50_svrr_model = SVR(kernel='rbf')\n",
    "ST20_svrr_model = SVR(kernel='rbf')\n",
    "ST10_svrr_model = SVR(kernel='rbf')\n",
    "ST5_svrr_model = SVR(kernel='rbf')\n",
    "ST2_svrr_model = SVR(kernel='rbf')\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_svrr_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with rfb model Evaluation Metrics Results For ST100 Normalized =======================\\n\")\n",
    "print(show_scores(ST100_svrr_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "# NB: Uncomment and run it when it is only required. It is resource intensive and time taking\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'max_depth': [None, 10, 20],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': [1, 'sqrt', 'log2'],\n",
    "#     'bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "# # Perform Grid Search with cross-validation\n",
    "# gsc_model = GridSearchCV(RandomForestRegressor(n_jobs=-1, random_state=42), param_grid=param_grid, \n",
    "#                            cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# # Fit the Grid Search to the data\n",
    "# gsc_model.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = gsc_model.best_params_\n",
    "# best_score = gsc_model.best_score_\n",
    "\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Score:\", best_score)\n",
    "\n",
    "# Fit for the most ideal hyperparameters tuned by the previous GridSearchCV\n",
    "ST100_ideal_rf_model = RandomForestRegressor(n_estimators=300, \n",
    "                                       min_samples_leaf=1,\n",
    "                                       min_samples_split=2,\n",
    "                                       max_features='sqrt',\n",
    "                                       max_depth=20,\n",
    "                                       bootstrap=False,\n",
    "                                       random_state=42)\n",
    "# Fit the ideal model\n",
    "ST100_ideal_rf_model.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scores of the trained ideal RF model\n",
    "print('==============================The Hyperparameter tuned Random Forest Evaluation metrics results for ST100')\n",
    "print(show_scores(ST100_ideal_rf_model, ST100_X_train, ST100_X_valid, ST100_Y_train, ST100_Y_valid))\n",
    "\n",
    "# Evaluating the model with cross-validation\n",
    "np.random.seed(42)\n",
    "cross_val_score_r2 = cross_val_score(ST100_ideal_rf_model, X, Y, cv=10)\n",
    "print(\"=================The cross-val-scores are=================\\n\")\n",
    "print(cross_val_score_r2)\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "# Predict on the ST100 Test Set\n",
    "# Read the test data from file\n",
    "test_data = pd.read_csv(\"data/ST100_X_test_data.csv\")\n",
    "ST100_Y_preds = ST100_ideal_rf_model.predict(test_data)\n",
    "\n",
    "# Define the reverse normalization function ( denormalizing function)\n",
    "def reverse_normalization(original_dataset, feature, test_set_series, model_name=None):\n",
    "    \"\"\"\n",
    "    Reverses the normalized pandas series(target variable) to its corresponding unnormalized pandas series (target variable).\n",
    "    It may reverse pandas series with predictions or simple denormalization of a pandas series depending on the model_name passed.\n",
    "    \n",
    "    parameters:\n",
    "        original_dataset: the original unnormalized dataset \n",
    "        feature: the name of the column to be reversed\n",
    "        model_name (optional): the name of the model to be used for prediction\n",
    "        test_set_series: the test data to be used for predicting the target\n",
    "    returns:\n",
    "        unnormalized_predicted_series: the unnormalized pandas series of the target variable\n",
    "    \"\"\"\n",
    "    # Exract the target variable from the original dataset\n",
    "    original_series = original_dataset[feature]\n",
    "    # Calculate the mean dand std of the original target variable\n",
    "    mean = original_series.mean()\n",
    "    std = original_series.std()\n",
    "    if model_name != None:    \n",
    "        # Predict the target from the test data using the ideal model generated\n",
    "        normalized_predicted = model_name.predict(test_set_series)\n",
    "        # Convert normalized_predicted_data to a pandas series\n",
    "        normalized_predicted_series = pd.Series(normalized_predicted)\n",
    "        # Update the test_set_series if the model exists otherwise normalize the unpredicted original series\n",
    "        test_set_series = normalized_predicted_series\n",
    "    # Calculate the unnormalized predicted series from the normalized predicted series using the mean an std\n",
    "    unnormalized_predicted_series = (test_set_series ) + mean\n",
    "    return unnormalized_predicted_series\n",
    "\n",
    "# Generate the ST100 denormalized predicted values dataset by using the reverse normalization function\n",
    "ST100_predicted_data = reverse_normalization(dataset_copied, 'ST100', ST100_X_test, ST100_ideal_rf_model)\n",
    "print(\"============================ ST100 Denormalized Predicted Values====================\\n\")\n",
    "print(ST100_predicted_data)\n",
    "print(\"====================================================================================\\n\")\n",
    "\n",
    "# Sklearn Evaluation Functions\n",
    "\n",
    "# The soil temperature at 100cm (ST100) normalized predicted data \n",
    "ST100_normalized_predicted = ST100_ideal_rf_model.predict(ST100_X_test)\n",
    "# The soil temperature at 1000 cm (ST100) unnormalized predicted data\n",
    "ST100_unnormalized_predicted= reverse_normalization(dataset_copied, 'ST100', ST100_X_test, ST100_ideal_rf_model)\n",
    "# The soil temperature at 1000 cm (ST100) unnormalized original data\n",
    "ST100_unnormalized_original = reverse_normalization(dataset_copied, 'ST100', ST100_Y_test)\n",
    "\n",
    "# Evaluation of the normalized target values \n",
    "r2 = r2_score(ST100_Y_test, ST100_normalized_predicted)\n",
    "# OOB_rf_score = ST100_ideal_rf_model.oob_score_\n",
    "mean_abs_err = mean_absolute_error(ST100_Y_test, ST100_normalized_predicted) # Mean absolute error\n",
    "mean_sqr_err =mean_squared_error(ST100_Y_test, ST100_normalized_predicted) # Mean Square error\n",
    "mean_abs_per_err = mean_absolute_percentage_error(ST100_Y_test, ST100_normalized_predicted) # Mean absolute percentage error\n",
    "median_abs_err = median_absolute_error(ST100_Y_test, ST100_normalized_predicted)\n",
    "max_err = max_error(ST100_Y_test, ST100_normalized_predicted)\n",
    "var_exp_err = explained_variance_score(ST100_Y_test, ST100_normalized_predicted)\n",
    "# Evaluation of the unnormalized target values \n",
    "r2_unorm = r2_score(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "mean_abs_err_unorm = mean_absolute_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "mean_sqr_err_unorm = mean_squared_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "mean_abs_per_err_unorm = mean_absolute_percentage_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "median_abs_err_unorm = median_absolute_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "max_err_unorm = max_error(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "var_exp_err_unorm = explained_variance_score(ST100_unnormalized_original, ST100_unnormalized_predicted)\n",
    "\n",
    "print(\"============================ ST100 Normalized and Denormalized Evaluation Metrics Scores=================\\n\")\n",
    "print(\"R^2 Score: Normalized: \", r2, \"Denormalized: \", r2_unorm)\n",
    "# print(\"Out-of-Bag Score: \", OOB_rf_score)\n",
    "print(\"mean_absolute_error: Normalized: \",mean_abs_err, \"Denormalized: \", mean_abs_err_unorm)\n",
    "print(\"mean_squared_error: Normalized: \",mean_sqr_err, \"Denormalized: \",mean_sqr_err_unorm)\n",
    "print(\"mean_absolute_percentage_error: Normalized: \",mean_abs_per_err, \"Denormalized: \",mean_abs_per_err_unorm)\n",
    "print(\"median_abs_err: Normalized: \",median_abs_err, \"Denormalized: \",median_abs_err_unorm)\n",
    "print(\"max_err: Normalized: \",max_err, \"Denormalized: \",max_err_unorm)\n",
    "print(\"var_exp_err: Normalized: \",var_exp_err, \"Denormalized: \",var_exp_err_unorm)\n",
    "print(\"=========================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# Feature Importance Analysis\n",
    "print(\"=============ST100 RF Models Feature Importances=======================================\\n\")\n",
    "print(ST100_rf_model.feature_importances_)\n",
    "print(\"=======================================================================================\\n\")\n",
    "\n",
    "# Let's make a function for plotting feature importance\n",
    "def plot_features(columns, importances, file, title, n=20):\n",
    "    df = (pd.DataFrame({\"features\": columns,\n",
    "                        \"feature_importances\": importances})\n",
    "          .sort_values(\"feature_importances\", ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "    # Plot the dataframe\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    bars = ax.barh(df['features'][:n], df['feature_importances'][:n])\n",
    "    ax.set_title(f\"{title}\")\n",
    "    ax.set_ylabel('Features')\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add annotations on top of the bars\n",
    "    for bar, importance in zip(bars, df['feature_importances'][:n]):\n",
    "        ax.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{importance:.5f}', \n",
    "                va='center', ha='left', fontsize=8, color='black')\n",
    "    # Save the figure to a file (e.g., PNG, PDF, etc.)\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file)  # Save as PNG format\n",
    "# Plot the feature importances of ST100. The same can be done to the other target variables     \n",
    "plot_features(ST100_X_train.columns, ST100_rf_model.feature_importances_, 'data/results/ST100_feature_analysis.png', 'ST100 Feature Importances')\n",
    "\n",
    "# Backward Attribute Selection\n",
    "# Generate the dataset of the two important columns and the target variable\n",
    "# Initialize the random seed\n",
    "np.random.seed(42)\n",
    "selected_columns = ['month', 'ST50', 'ST100']\n",
    "ST100_dataset_important_features = dataset_normalized_outlier_filtered[selected_columns]\n",
    "# Split the dataset into train and validation set\n",
    "# Let us shuffle the entire dataset so that it is randomly arranged\n",
    "ST100_dataset_shuffled = ST100_dataset_important_features.sample(frac=1)\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_100cm )\n",
    "ST100_X = ST100_dataset_shuffled.drop(\"ST100\", axis=1)\n",
    "ST100_Y = ST100_dataset_shuffled[\"ST100\"]\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(ST100_dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(ST100_dataset_shuffled))\n",
    "ST100_X_train_3, ST100_Y_train_3 = ST100_X[:train_split], ST100_Y[:train_split]\n",
    "ST100_X_valid_3, ST100_Y_valid_3 =ST100_X[train_split:valid_split], ST100_Y[train_split:valid_split]\n",
    "ST100_X_test_3, ST100_Y_test_3 = ST100_X[valid_split:], ST100_Y[valid_split:]\n",
    "# Save the ST100_X_test data to csv for future use\n",
    "ST100_X_test_3.to_csv(\"data/ST100_optimized_X_test_data.csv\", index=False)\n",
    "# Fit the RF regressor model\n",
    "ST100_rf_model.fit(ST100_X_train_3, ST100_Y_train_3);\n",
    "# Show scores for ST100\n",
    "print(\"==============ST100 evalutation metrics scores for train and validation sets with two normalized features ==============\\n\")\n",
    "print(show_scores(ST100_rf_model, ST100_X_train_3, ST100_X_valid_3, ST100_Y_train_3, ST100_Y_valid_3))\n",
    "print(\"===========================================================================================\\n\")\n",
    "# Predict the target ST100 values from the test set\n",
    "ST100_Y_test_preds = ST100_rf_model.predict(ST100_X_test_3)\n",
    "print(\"============== ST100 Predicted values for test set with two features ==============\\n\")\n",
    "print(ST100_Y_test_preds)\n",
    "print(\"============== ST100 Observed values for test set with two features =================\\n\")\n",
    "print(ST100_Y_test_3)\n",
    "print(\"===========================================================================================\\n\")\n",
    "\n",
    "# Visualization of original and predicted ST100 values\n",
    "\n",
    "# Reverse the normalized ST50 values in the X test set\n",
    "ST50_denormalized = reverse_normalization(dataset_copied, 'ST50', ST100_X_test_3['ST50'])\n",
    "# Reverse the normalized ST100 values in the Y test set\n",
    "ST100_Y_test_denormalized = reverse_normalization(dataset_copied, 'ST100', ST100_Y_test_3)\n",
    "# Convert numpy array to pandas series\n",
    "ST100_Y_test_preds_series = pd.Series( ST100_Y_test_preds)\n",
    "# Reverse the normalized predicted ST100 values in the Y test set\n",
    "ST100_Y_preds_denormalized = reverse_normalization(dataset_copied, 'ST100', ST100_Y_test_preds_series)\n",
    "# Make the original and predicted series to have the same index\n",
    "ST100_Y_preds_denormalized.index = ST100_Y_test_denormalized.index\n",
    "ST50_denormalized.index = ST100_Y_test_denormalized.index\n",
    "# Create true and predicted values dataframe for saving\n",
    "ST100_true_and_predicted_values = pd.DataFrame({'ST50 Value':ST50_denormalized,'ST100 True Value': ST100_Y_test_denormalized, 'ST100 Predicted Value': ST100_Y_preds_denormalized})\n",
    "# Save the True and Predicted Values to csv for further comparison\n",
    "ST100_true_and_predicted_values.to_csv('data/ ST100_Y_test_true_and_predicted_values.csv', index=False)\n",
    "\n",
    "# Sort Y_test and Y_preds in ascending order and reset indices\n",
    "ST100_Y_test_sorted = ST100_Y_test_denormalized.sort_values().reset_index(drop=True)\n",
    "ST100_Y_preds_sorted = ST100_Y_preds_denormalized[ST100_Y_test_denormalized.index].sort_values().reset_index(drop=True)\n",
    "\n",
    "# Calculate mean absolute error\n",
    "ST100_mae = mean_absolute_error(ST100_Y_test_denormalized, ST100_Y_preds_denormalized)\n",
    "# Calculate mean squared error\n",
    "ST100_mse = mean_squared_error(ST100_Y_test_denormalized, ST100_Y_preds_denormalized)\n",
    "# Calculate the R^2 score\n",
    "ST100_r2_score = r2_score(ST100_Y_test_denormalized, ST100_Y_preds_denormalized)\n",
    "# Plot the sorted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ST100_Y_test_sorted.index, ST100_Y_test_sorted, color='blue', label='Observed Values')\n",
    "plt.plot(ST100_Y_preds_sorted.index, ST100_Y_preds_sorted, color='red', label='Predicted Values')\n",
    "# Display the mean absolute error as text annotation\n",
    "plt.text(0.4, 0.95, f'MAE: {ST100_mae:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.6, 0.95, f'MSE: {ST100_mse:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.8, 0.95, f'R^2 score: {ST100_r2_score:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.xlabel('Index', fontsize=14)\n",
    "plt.ylabel('Soil Temperature at 100cm (°C)', fontsize=14)\n",
    "plt.title('Comparison of Observed vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Prediction of the soil temperatures at different depths\n",
    "\n",
    "# Let us first predict the soil temperature at 50cm from all other independent variables except the ST100 (soil temp at 100cm)\n",
    "# Define X_train_2 outside of the function to mean two datasets (train and test)\n",
    "X_train_2 = None  \n",
    "X_test_2 = None \n",
    "Y_train_2 = None\n",
    "Y_test_2 = None\n",
    "def predict_feature(dataset_df, features_dropped, target, model_type):\n",
    "    new_dataset_normalized = dataset_df.drop(features_dropped, axis=1)\n",
    "    X_f = new_dataset_normalized.drop(target, axis=1)\n",
    "    Y_f = new_dataset_normalized[target]\n",
    "    # Split the X and Y data in to train and test data\n",
    "    X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(X_f, Y_f, test_size=0.15, train_size=0.7)\n",
    "    model_type.fit(X_train_2, Y_train_2)\n",
    "    scores = show_scores(model_type, X_train_2, X_valid_2, Y_train_2, Y_valid_2)\n",
    "    print(scores)\n",
    "    return X_train_2, X_valid_2, Y_train_2, Y_valid_2\n",
    "\n",
    "# First let's check with all features considered for each target variable\n",
    "# Instantiate the custom predict_feature function to predict the ST2, ST5, ST10, ST20 values\n",
    "# Random Forest Regressor ST100 with all features considered\n",
    "print(\"================================= Random Forest Metrics ST100 with all denormalized features considered ==============================\\n\")\n",
    "ST100_X_train_2_all, ST100_X_test_2_all, ST100_Y_train_2_all, ST100_Y_test_2_all = predict_feature(dataset_denormalized_outlier_filtered, ['ID'], 'ST100', ST100_rf_all_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST50 with all features considered\n",
    "print(\"================================= Random Forest Metrics ST50 with all denormalized features considered ==============================\\n\")\n",
    "ST50_X_train_2_all, ST50_X_test_2_all, ST50_Y_train_2_all, ST50_Y_test_2_all = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ID'], 'ST50', ST50_rf_all_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST20 with all features considered\n",
    "print(\"================================= Random Forest Metrics ST20 with all denormalized features considered ==============================\\n\")\n",
    "ST20_X_train_2_all, ST20_X_test_2_all, ST20_Y_train_2_all, ST20_Y_test_2_all = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ID'], 'ST20', ST20_rf_all_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST10 with all features considered\n",
    "print(\"================================= Random Forest Metrics ST10 with all denormalized features considered ==============================\\n\")\n",
    "ST10_X_train_2_all, ST10_X_test_2_all, ST10_Y_train_2_all, ST10_Y_test_2_all = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20','ID'], 'ST10', ST10_rf_all_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST5 with all features considered\n",
    "print(\"================================= Random Forest Metrics ST5 with all denormalized features considered ==============================\\n\")\n",
    "ST5_X_train_2_all, ST5_X_test_2_all, ST5_Y_train_2_all, ST5_Y_test_2_all = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20','ST10','ID'], 'ST5', ST5_rf_all_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST2 with all features considered\n",
    "print(\"================================= Random Forest Metrics ST2 with all denormalized features considered ==============================\\n\")\n",
    "ST2_X_train_2_all, ST2_X_test_2_all, ST2_Y_train_2_all, ST2_Y_test_2_all = predict_feature(dataset_denormalized_outlier_filtered, ['ST100','ST50','ST20','ST10','ST5','ID'], 'ST2', ST2_rf_all_model)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "# Analysis are prediction error with respect to each feature using Partial Dependence Plots (PDPs), Individual Conditional Expectation (ICE) Plots, Residual Analysis\n",
    "# ST2 Partial Dependence Plot (PDP)\n",
    "print('====================================================== ST2 Partial Dependence Plot For Prediction with all denormalized features considered')\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "# Get feature names\n",
    "ST2_feature_names = ST2_X_train_2_all.columns.tolist()\n",
    "# Create PartialDependenceDisplay object for all features\n",
    "display = PartialDependenceDisplay.from_estimator(ST2_rf_all_model, ST2_X_train_2_all, features=ST2_feature_names, ax=ax)\n",
    "# Plot partial dependence for all features\n",
    "display.plot()\n",
    "# Adjust vertical spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.5)  # Adjust the value as needed\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ST2 Individual Conditional Expectation (ICE) Plot\n",
    "print('======================== ST2 Individual Conditional Expectation Plot For Prediction with all denormalized features considered ===============================')\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "# Create PartialDependenceDisplay object for all features\n",
    "display = PartialDependenceDisplay.from_estimator(ST2_rf_all_model, ST2_X_train_2_all, features=ST2_feature_names, ax=ax,kind='individual')\n",
    "# Plot partial dependence for all features\n",
    "display.plot()\n",
    "# Adjust vertical spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.5)  # Adjust the value as needed\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Residual Analysis\n",
    "print('========================= ST2 Residual Analysis Plot For Prediction with all denormalized features considered ==============================')\n",
    "ST2_residuals = ST2_Y_test_2_all - ST2_rf_all_model.predict(ST2_X_test_2_all)\n",
    "plt.scatter(ST2_Y_test_2_all, ST2_residuals)\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('ST2 Residuals Analysis Plot', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# # Decision Tree Analysis\n",
    "# print('========================= ST2 Decison Tree Visualition Plot For Prediction with all denormalized features considered ==============================')\n",
    "# plt.figure(figsize=(20,10))\n",
    "# ST2_feature_names = ST2_X_train_2_all.columns\n",
    "# plot_tree(ST2_rf_all_model.estimators_[0], feature_names=ST2_feature_names, filled=True)\n",
    "# plt.title('ST2 Decision Tree Visualization Plot')\n",
    "# plt.show()\n",
    "\n",
    "# Feature Importance analysis for ST2, ST5, ST10, ST50, ST50 and ST100 with only important features\n",
    "# Feature parameter analysis for ST2\n",
    "plot_features(ST2_X_train_2_all.columns, ST2_rf_all_model.feature_importances_, 'data/results/ST2_feature_analysis.png', 'ST2 Feature Importances')\n",
    "\n",
    "# Feature parameter analysis for ST5\n",
    "plot_features(ST5_X_train_2_all.columns, ST5_rf_all_model.feature_importances_, 'data/results/ST5_feature_analysis.png', 'ST5 Feature Importances')\n",
    "\n",
    "# Feature parameter analysis for ST10\n",
    "plot_features(ST10_X_train_2_all.columns, ST10_rf_all_model.feature_importances_, 'data/results/ST10_feature_analysis.png', 'ST10 Feature Importances')\n",
    "\n",
    "# Feature parameter analysis for ST20\n",
    "plot_features(ST20_X_train_2_all.columns, ST20_rf_all_model.feature_importances_, 'data/results/ST20_feature_analysis.png', 'ST20 Feature Importances')\n",
    "\n",
    "# Feature parameter analysis for ST50\n",
    "plot_features(ST50_X_train_2_all.columns, ST50_rf_all_model.feature_importances_, 'data/results/ST50_feature_analysis.png', 'ST50 Feature Importances')\n",
    "\n",
    "# Feature parameter analysis for ST100\n",
    "plot_features(ST100_X_train_2_all.columns, ST100_rf_all_model.feature_importances_, 'data/results/ST100_feature_analysis.png', 'ST100 Feature Importances')\n",
    "\n",
    "# Second let's check with only important features considered for each target variable\n",
    "# Instantiate the custom predict_feature function to predict the ST2, ST5, ST10, ST20 values\n",
    "# Random Forest Regressor ST100 with only important features considered\n",
    "print(\"================================= Random Forest Metrics ST100 with only important denormalized features considered ==============================\\n\")\n",
    "ST100_X_train_2, ST100_X_test_2, ST100_Y_train_2, ST100_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "                                                                                   ['ID', 'mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','evaporation_mm','earth_heat_flux_MJ_m2','ST2','ST5','ST10','ST20', 'radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','snow_depth_cm','day'], \n",
    "                                                                                   'ST100', \n",
    "                                                                                   ST100_rf_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST50 with only important features considered\n",
    "print(\"================================= Random Forest Metrics ST50 with only important denormalized features considered ==============================\\n\")\n",
    "ST50_X_train_2, ST50_X_test_2, ST50_Y_train_2, ST50_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "                                                                               ['ID', 'mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','evaporation_mm','earth_heat_flux_MJ_m2','ST2','ST5','ST10','ST100','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','snow_depth_cm','day'],  \n",
    "                                                                               'ST50', \n",
    "                                                                               ST50_rf_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST20 with only important features considered\n",
    "print(\"================================= Random Forest Metrics ST20 with only important denormalized features considered ==============================\\n\")\n",
    "ST20_X_train_2, ST20_X_test_2, ST20_Y_train_2, ST20_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "                                                                               ['ID', 'mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','evaporation_mm','earth_heat_flux_MJ_m2','ST2','ST5','ST50','ST100','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','snow_depth_cm','day', 'month'],   \n",
    "                                                                               'ST20', \n",
    "                                                                               ST20_rf_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST10 with only important features considered\n",
    "print(\"================================= Random Forest Metrics ST10 with only important denormalized features considered ==============================\\n\")\n",
    "ST10_X_train_2, ST10_X_test_2, ST10_Y_train_2, ST10_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "                                                                               ['ID', 'mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','evaporation_mm','earth_heat_flux_MJ_m2','ST2','ST20','ST50','ST100','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','snow_depth_cm','day', 'month'],   \n",
    "                                                                               'ST10', \n",
    "                                                                               ST10_rf_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST5with only important features considered\n",
    "print(\"================================= Random Forest Metrics ST5 with only important denormalized features considered ==============================\\n\")\n",
    "ST5_X_train_2, ST5_X_test_2, ST5_Y_train_2, ST5_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "                                                                           ['ID', 'mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','evaporation_mm','earth_heat_flux_MJ_m2','ST10','ST20','ST50','ST100','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','snow_depth_cm','day', 'month'],    \n",
    "                                                                           'ST5', \n",
    "                                                                           ST5_rf_model)\n",
    "print('===========================================================================================\\n')\n",
    "# Random Forest Regressor ST2 with only important features considered\n",
    "print(\"================================= Random Forest Metrics ST2 with only important denormalized features considered ==============================\\n\")\n",
    "ST2_X_train_2, ST2_X_test_2, ST2_Y_train_2, ST2_Y_test_2 = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "                                                                           ['ID', 'min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','ST5','ST10','ST20','ST50','ST100','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR'],     \n",
    "                                                                           'ST2', \n",
    "                                                                           ST2_rf_model)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Using Joblib module save the trained models to file for future use \n",
    "dump(ST100_rf_model, filename=\"models/trained_model_temperature_100cm_joblib.joblib\")\n",
    "dump(ST50_rf_model, filename=\"models/trained_model_temperature_50cm_joblib.joblib\")\n",
    "dump(ST20_rf_model, filename=\"models/trained_model_temperature_20cm_joblib.joblib\")\n",
    "dump(ST10_rf_model, filename=\"models/trained_model_temperature_10cm_joblib.joblib\")\n",
    "dump(ST5_rf_model, filename=\"models/trained_model_temperature_5cm_joblib.joblib\")\n",
    "dump(ST2_rf_model, filename=\"models/trained_model_temperature_2cm_joblib.joblib\")\n",
    "\n",
    "# We can load the save models using load function\n",
    "loaded_joblib_model = load(filename=\"models/trained_model_temperature_100cm_joblib.joblib\")\n",
    "loaded_joblib_model.score(ST100_X_test_2, ST100_Y_test_2)\n",
    "loaded_joblib_model.predict(ST100_X_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a31f3-07fc-4e7f-894e-9ee24704d0ea",
   "metadata": {},
   "source": [
    "### RF GridSearchCV results\n",
    "### Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n",
    "### Best Parameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "### Best Score: 0.9947150605219992\n",
    "### CPU times: user 54.7 s, sys: 7.19 s, total: 1min 1s\n",
    "### Wall time: 1h 17min 25s\n",
    "### GridSearchCV results for other models\n",
    "Best parameters for HistGradientBoostingRegressor: {'learning_rate': 0.1, 'max_iter': 300, 'max_leaf_nodes': 41}\r\n",
    "Best score for HistGradientBoostingRegressor: 0.7037423868138698\r\n",
    "Best parameters for AdaBoostRegressor: {'learning_rate': 0.1, 'n_estimators': 100}\r\n",
    "Best score for AdaBoostRegressor: 1.1447703421723503\r\n",
    "Best parameters for XGBRegressor: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200, 'subsample': 0.8}\r\n",
    "Best score for XGBRegressor: 0.7048773147810861\r\n",
    "Best parameters for CatBoostRegressor: {'depth': 6, 'iterations': 500, 'l2_leaf_reg': 3, 'learning_rate': 0.1}\r\n",
    "Best score for CatBoostRegressor: 0.7017612120888386\r\n",
    "CPU times: user 41 s, sys: 5.28 s, total: 46.3 s\r\n",
    "Wall time: 42min 44smin 8s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a118ba8-f327-4ac6-998c-1f9aff5413a0",
   "metadata": {},
   "source": [
    "### Predictions of Evaporation, Radiation Balance, Albedo and Goethermal Heat Flux as targets with respect to other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0bd189-2111-42b4-9952-261201595037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_denormalized_outlier_filtered[dataset_denormalized_outlier_filtered['albedo_RR_GR'] < 0]['albedo_RR_GR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587b1d9-3111-4290-b000-54b643606790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Regressor Evaporation with only important features considered\n",
    "# evaporation_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "# radiation_balance_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "# albedo_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "# heat_flux_rf_model = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "\n",
    "# # Metrics and Feature importance analysis for evaporation\n",
    "# print(\"================================= Random Forest Metrics For Evaporation with all features except soil temperatures considered ==============================\\n\")\n",
    "# Evaporation_X_train, Evaporation_X_test, Evaporation_Y_train, Evaporation_Y_test = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "#                                                                            ['ID', 'ST2', 'ST5', 'ST10', 'ST20', 'ST50', 'ST100'],    \n",
    "#                                                                            'evaporation_mm', \n",
    "#                                                                            evaporation_rf_model)\n",
    "# print('===========================================================================================\\n')\n",
    "# plot_features(Evaporation_X_train.columns, evaporation_rf_model.feature_importances_, 'data/results/Evaporation_feature_analysis.png', 'Evaporation Feature Importances')\n",
    "\n",
    "# # Metrics and Feature importance analysis for radiation balance\n",
    "# print(\"================================= Random Forest Metrics For radiation balance with all features except soil temperatures considered ==============================\\n\")\n",
    "# Radiation_balance_X_train, Radiation_balance_X_test, Radiation_balance_Y_train, Radiation_balance_Y_test = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "#                                                                            ['ID', 'ST2', 'ST5', 'ST10', 'ST20', 'ST50', 'ST100'],    \n",
    "#                                                                            'radiation_balance_w_m2', \n",
    "#                                                                            radiation_balance_rf_model)\n",
    "# print('===========================================================================================\\n')\n",
    "# plot_features(Radiation_balance_X_train.columns, radiation_balance_rf_model.feature_importances_, 'data/results/Radiation_balance_feature_analysis.png', 'Radiation Balance Feature Importances')\n",
    "\n",
    "# # Metrics and Feature importance analysis for albedo\n",
    "# print(\"================================= Random Forest Metrics For Albedo with all features except soil temperatures considered ==============================\\n\")\n",
    "# albedo_X_train, albedo_X_test, albedo_Y_train, albedo_Y_test = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "#                                                                            ['ID', 'ST2', 'ST5', 'ST10', 'ST20', 'ST50', 'ST100'],    \n",
    "#                                                                            'albedo_RR_GR', \n",
    "#                                                                            albedo_rf_model)\n",
    "# print('===========================================================================================\\n')\n",
    "# plot_features(albedo_X_train.columns, albedo_rf_model.feature_importances_, 'data/results/Albedo_feature_analysis.png', 'Albedo Feature Importances')\n",
    "\n",
    "# # Metrics and Feature importance analysis for albedo\n",
    "# print(\"================================= Random Forest Metrics For Heat Flux with all features except soil temperatures considered ==============================\\n\")\n",
    "# Heat_flux_X_train, Heat_flux_X_test, Heat_flux_Y_train, Heat_flux_Y_test = predict_feature(dataset_denormalized_outlier_filtered, \n",
    "#                                                                            ['ID', 'ST2', 'ST5', 'ST10', 'ST20', 'ST50', 'ST100'],    \n",
    "#                                                                            'earth_heat_flux_MJ_m2', \n",
    "#                                                                            heat_flux_rf_model)\n",
    "# print('===========================================================================================\\n')\n",
    "# # Feature importance analysis for Evaporation\n",
    "# plot_features(Heat_flux_X_train.columns, heat_flux_rf_model.feature_importances_, 'data/results/Heat_flux_feature_analysis.png', 'Heat Flux Feature Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740ae9a-d98d-4286-b59f-5b53621fcdca",
   "metadata": {},
   "source": [
    "dataset_denormalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b502e-ac7f-4fea-8b5d-8d57992111bb",
   "metadata": {},
   "source": [
    "# 6. 10 days Soil Temperature Forecast From 10 days MET Norway Weather Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21651200-57ec-4b4b-9dad-59933de5f5a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests as requests\n",
    "import json as json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Initialize the expires and last_modified variables \n",
    "expire_time = None\n",
    "last_modified = None\n",
    "latitude = 59.6603 # in degrees\n",
    "longitude = 10.7817 # in degrees\n",
    "altitude = 93 # in meters\n",
    "# Now let us fetch the meteorological forecast data from MET Norway\n",
    "request_url = f'https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={latitude}&lon={longitude}&altitude={altitude}'\n",
    "# Email address to include as User-Agent ID\n",
    "user_agent = '259646@usn.no'\n",
    "# Define headers with User-Agent\n",
    "headers = {'User-Agent': user_agent, 'If-Modified-Since':last_modified}\n",
    "# Send HTTP GET request with headers\n",
    "response = requests.get(request_url, headers=headers)\n",
    "\n",
    "# Extract the month and day\n",
    "current_datetime = datetime.now()\n",
    "current_month = current_datetime.strftime(\"%B\")  # Full month name\n",
    "current_day = current_datetime.strftime(\"%d\") \n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "     # Save the expires and last_modified variables to avoid frequency request to the server\n",
    "    expires_header = response.headers.get('expires')\n",
    "    last_modified_header = response.headers.get('last-modified')\n",
    "\n",
    "    # Convert expires and last_modified to RFC 1123 format\n",
    "    expire_time = datetime.strptime(expires_header, '%a, %d %b %Y %H:%M:%S GMT').strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "    last_modified = datetime.strptime(last_modified_header, '%a, %d %b %Y %H:%M:%S GMT').strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "    weather_data = response.json()\n",
    "else:\n",
    "    # Request failed, print error message\n",
    "    weather_data = {}\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "\n",
    "# Extract relevant information\n",
    "rows = []\n",
    "for entry in weather_data['properties']['timeseries']:\n",
    "    instant_details = entry['data']['instant']['details']\n",
    "    next_6_hours_details = entry['data'].get('next_6_hours', {}).get('details', None)\n",
    "    row = {\n",
    "        'time': entry['time'],\n",
    "        'instant_air_temperature': instant_details['air_temperature'],\n",
    "        'instant_air_pressure_at_sea_level': instant_details['air_pressure_at_sea_level'],\n",
    "        'instant_relative_humidity': instant_details['relative_humidity'],\n",
    "        'next_6_hours_air_temperature_max': next_6_hours_details['air_temperature_max'] if next_6_hours_details else None,\n",
    "        'next_6_hours_air_temperature_min': next_6_hours_details['air_temperature_min'] if next_6_hours_details else None,\n",
    "        'next_6_hours_precipitation_amount': next_6_hours_details['precipitation_amount'] if next_6_hours_details else None\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "MET_weather_df = pd.DataFrame(rows)\n",
    "# Save to file\n",
    "MET_weather_df.to_csv(f'data/{current_month}_{current_day}_MET_weather_forecast_14_days.csv')\n",
    "\n",
    "MET_weather_df['time'] = pd.to_datetime(MET_weather_df['time'])\n",
    "# Add snow depth with zero values for testing during the summer season as there is no snow but should be automated for all seasons not part of the weather forecast\n",
    "MET_weather_df['snow_depth_cm'] = 0.0\n",
    "# # Convert \"time\" column to datetime object\n",
    "# MET_weather_df['MET_time'] = pd.to_datetime(MET_weather_df['time'])\n",
    "# # Extract date from datetime object\n",
    "# MET_weather_df['date'] = MET_weather_df['time'].dt.date\n",
    "# # Check if each date has 24 hours\n",
    "# complete_dates = MET_weather_df['date'].value_counts()[MET_weather_df['date'].value_counts() == 24].index.tolist()\n",
    "# # Filter rows with complete dates\n",
    "# MET_weather_df = MET_weather_df[MET_weather_df['date'].isin(complete_dates)]\n",
    "# # Drop the 'date' column\n",
    "# MET_weather_df = MET_weather_df.drop(columns=['date'])\n",
    "\n",
    "# Group DataFrame by date\n",
    "grouped_df = MET_weather_df.groupby(MET_weather_df['time'].dt.date)\n",
    "\n",
    "# Calculate daily mean, min, max\n",
    "MET_daily_stats = grouped_df.agg(\n",
    "    mean_air_temperature_2m=('instant_air_temperature', 'mean'),\n",
    "    min_air_temperature_2m=('next_6_hours_air_temperature_min', 'min'),\n",
    "    max_air_temperature_2m=('next_6_hours_air_temperature_max', 'max'),\n",
    "    relative_humidity=('instant_relative_humidity', 'mean'),\n",
    "    air_pressure_2m_mbar=('instant_air_pressure_at_sea_level', 'mean'),\n",
    "    precipitation_mm=('next_6_hours_precipitation_amount', 'mean'),    \n",
    "    snow_depth_cm=('snow_depth_cm', 'mean')\n",
    ")\n",
    "\n",
    "# Option 2: Fetch weather data from Open Meteo API\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://api.open-meteo.com/v1/metno\"\n",
    "params = {\n",
    "\t\"latitude\": 59.6602,\n",
    "\t\"longitude\": 10.7817,\n",
    "\t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\", \"snowfall\"],\n",
    "\t\"timezone\": \"auto\",\n",
    "\t\"past_hours\": 24,\n",
    "\t\"forecast_hours\": 24\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_rain = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_snowfall = hourly.Variables(4).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "hourly_data[\"rain\"] = hourly_rain\n",
    "hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "\n",
    "open_meteo_hourly_df = pd.DataFrame(data = hourly_data)\n",
    "open_meteo_hourly_df .to_csv(f'data/{current_month}_{current_day}_Meteo_weather_forecast_2_days.csv')\n",
    "\n",
    "# Create function to evaluate model on few different levels\n",
    "def predict_and_score(model, X_train, X_valid, Y_train, Y_valid):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    scores = {\"Training Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_valid, val_preds),\n",
    "              \"Training Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds),             \n",
    "              \"Training Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),\n",
    "              \"Training Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Training Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              \"Training Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              \"Training Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)}\n",
    "    return scores\n",
    "\n",
    "def fit_model(dataset_df, features_dropped, target, model_type):\n",
    "    X_f = dataset_df.drop(features_dropped, axis=1)\n",
    "    Y_f = dataset_df[target]\n",
    "    # Split the X and Y data in to train and test data\n",
    "    X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(X_f, Y_f, test_size=0.2)\n",
    "    model_type.fit(X_train_2, Y_train_2)\n",
    "    scores = predict_and_score(model_type, X_train_2, X_valid_2, Y_train_2, Y_valid_2)\n",
    "    print(scores)\n",
    "    return X_train_2, X_valid_2, Y_train_2, Y_valid_2\n",
    "    \n",
    "def calculate_pressure(P0, T, altitude):\n",
    "    # Constants\n",
    "    g = 9.80665  # Acceleration due to gravity (m/s^2)\n",
    "    M = 0.0289644  # Molar mass of Earth's air (kg/mol)\n",
    "    R = 8.31432  # Universal gas constant (J/(mol*K))    \n",
    "    # Calculate pressure\n",
    "    pressure = P0 * math.exp((-g * M * altitude) / (R * (T + 273.15)))\n",
    "    return pressure\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_and_plot(model, ST_X_test, ST_X_validation, ST_Y_validation, name):\n",
    "   # Predict the test set which is forecast data\n",
    "   ST_Y_test_preds = model.predict(ST_X_test)\n",
    "   # Changes the predicted array values to pandas series\n",
    "   ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "   # Convert the Series to a DataFrame\n",
    "   ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "   ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "   # Predict the validation set\n",
    "   ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "   # Change validation predictions to pandas series\n",
    "   ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "   # Make the original and predicted series to have the same index\n",
    "   ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "   # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "   ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "   ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "   # Calculate mean absolute error\n",
    "   ST_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Calculate mean squared error\n",
    "   ST_mse = mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Calculate the R^2 score\n",
    "   ST_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Plot the sorted values\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Observed Values')\n",
    "   plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Predicted Values')\n",
    "   # Display the mean absolute error as text annotation\n",
    "   plt.text(0.4, 0.95, f'MAE: {ST_mae:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.text(0.6, 0.95, f'MSE: {ST_mse:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.text(0.8, 0.95, f'R^2 score: {ST_r2_score:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.xlabel('Index', fontsize=14)\n",
    "   plt.ylabel(f'Soil Temperature at {name}(°C)')\n",
    "   plt.title(f'Comparison of {name} Observed vs Predicted Values')\n",
    "   plt.legend()\n",
    "   plt.grid(True)\n",
    "   plt.show()\n",
    "   return ST_Y_test_preds_df\n",
    "\n",
    "# ============================ Fit the soil temperature at 2cm from the forecast weather data ===================================== \n",
    "# calculate the air_pressure at certain altitude from the sea level air pressure forecast fetched from MET Norway\n",
    "MET_daily_stats['air_pressure_2m_mbar'] = MET_daily_stats.apply(lambda row: calculate_pressure(row['air_pressure_2m_mbar'], row['mean_air_temperature_2m'], altitude), axis=1)\n",
    "# Convert index to datetime\n",
    "MET_daily_stats.index = pd.to_datetime(MET_daily_stats.index)\n",
    "# Extract month and day from the index\n",
    "MET_daily_stats['month'] = MET_daily_stats.index.month\n",
    "MET_daily_stats['day'] = MET_daily_stats.index.day\n",
    "# Create test set for soil temperature at 2cm as ST_X_test\n",
    "ST2_X_test = MET_daily_stats\n",
    "# # Normalize\n",
    "# ST2_X_test_norm = (ST2_X_test - mean_original['ST2'])/std_original['ST2']\n",
    "\n",
    "# Initialize ST2 RF regressor\n",
    "ST2_rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                       min_samples_leaf=1,\n",
    "                                       min_samples_split=2,\n",
    "                                       max_features='sqrt',\n",
    "                                       max_depth=20,\n",
    "                                       bootstrap=False,\n",
    "                                       random_state=42)\n",
    "\n",
    "print(\"================================= Random Forest Metrics ST2 with weather data selected features for forecast testing ==============================\\n\")\n",
    "ST2_X_training, ST2_X_validation, ST2_Y_training, ST2_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','ST100','ST50','ST20','ST10','ST5','ST2','ID'], \n",
    "                                                                                           'ST2', \n",
    "                                                                                           ST2_rf_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST2_Y_test_preds_df = predict_and_plot(ST2_rf_regressor, ST2_X_test, ST2_X_validation, ST2_Y_validation, 'ST2')\n",
    "# Merge the Original DataFrame with the new predicted dataframe\n",
    "ST2_forecast_df = pd.concat([ST2_X_test, ST2_Y_test_preds_df], axis=1)\n",
    "# ======================= Fit the soil temperature at 5cm from the predictions at 2cm ================================\n",
    "# Copy the ST2 predicted values as test data input for ST5\n",
    "ST5_features = ['ST2', 'month']\n",
    "ST5_X_test = ST2_forecast_df[ST5_features]\n",
    "ST5_X_test.index = ST2_forecast_df.index\n",
    "# ST5_X_test_reshaped = ST5_X_test.values.reshape(-1, 1)\n",
    "# Initialize the RF regressor\n",
    "ST5_rf_regressor = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Random Forest Metrics ST5 with ST2 as a feature for forecast testing ==============================\\n\")\n",
    "ST5_X_training, ST5_X_validation, ST5_Y_training, ST5_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST50','ST20','ST10','ST5','ID'], \n",
    "                                                                                           'ST5', \n",
    "                                                                                           ST5_rf_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST5_Y_test_preds_df = predict_and_plot(ST5_rf_regressor, ST5_X_test, ST5_X_validation, ST5_Y_validation, 'ST5')\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST5_forecast_df = pd.concat([ST2_forecast_df, ST5_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 10cm from the predictions at 5cm ================================\n",
    "# Copy the ST5 predicted values as test data input for ST10\n",
    "ST10_features = ['ST5', 'month']\n",
    "ST10_X_test = ST5_forecast_df[ST10_features]\n",
    "ST10_X_test.index = ST5_forecast_df.index\n",
    "# ST10_X_test_reshaped = ST10_X_test.values.reshape(-1, 1)\n",
    "# Initialize the RF regressor\n",
    "ST10_rf_regressor = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Random Forest Metrics ST10 with ST2 as a feature for forecast testing ==============================\\n\")\n",
    "ST10_X_training, ST10_X_validation, ST10_Y_training, ST10_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST50','ST20','ST10','ST2','ID'], \n",
    "                                                                                           'ST10', \n",
    "                                                                                           ST10_rf_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST10_Y_test_preds_df = predict_and_plot(ST10_rf_regressor, ST10_X_test, ST10_X_validation, ST10_Y_validation, 'ST10')\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST10_forecast_df = pd.concat([ST5_forecast_df, ST10_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 20cm from the predictions at 10cm and month ================================\n",
    "# Copy the ST10 predicted values as test data input for ST20\n",
    "ST20_features = ['ST10', 'month']\n",
    "ST20_X_test = ST10_forecast_df[ST20_features]\n",
    "ST20_X_test.index = ST10_forecast_df.index\n",
    "# Initialize the RF regressor\n",
    "ST20_rf_regressor = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Random Forest Metrics ST20 with ST2 as a feature for forecast testing ==============================\\n\")\n",
    "ST20_X_training, ST20_X_validation, ST20_Y_training, ST20_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST50','ST20','ST5','ST2','ID'], \n",
    "                                                                                           'ST20', \n",
    "                                                                                           ST20_rf_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST20_Y_test_preds_df = predict_and_plot(ST20_rf_regressor, ST20_X_test, ST20_X_validation, ST20_Y_validation, 'ST20')\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST20_forecast_df = pd.concat([ST10_forecast_df, ST20_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 50cm from the predictions at 20cm and month ================================\n",
    "# Copy the ST20 predicted values as test data input for ST50\n",
    "ST50_features = ['ST20', 'month']\n",
    "ST50_X_test = ST20_forecast_df[ST50_features]\n",
    "ST50_X_test.index = ST20_forecast_df.index\n",
    "# Initialize the RF regressor\n",
    "ST50_rf_regressor = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Random Forest Metrics ST50 with ST2 as a feature for forecast testing ==============================\\n\")\n",
    "ST50_X_training, ST50_X_validation, ST50_Y_training, ST50_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST50','ST10','ST5','ST2','ID'], \n",
    "                                                                                           'ST50', \n",
    "                                                                                           ST50_rf_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST50_Y_test_preds_df = predict_and_plot(ST50_rf_regressor, ST50_X_test, ST50_X_validation, ST50_Y_validation, 'ST50')\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST50_forecast_df = pd.concat([ST20_forecast_df, ST50_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 100cm from the predictions at 50cm and month ================================\n",
    "# Copy the ST50 predicted values as test data input for ST100\n",
    "ST100_features = ['ST50', 'month']\n",
    "ST100_X_test = ST50_forecast_df[ST100_features]\n",
    "ST100_X_test.index = ST50_forecast_df.index\n",
    "# Initialize the RF regressor\n",
    "ST100_rf_regressor = RandomForestRegressor(n_jobs=-1, random_state=42, oob_score=True)\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Random Forest Metrics ST100 with ST2 as a feature for forecast testing ==============================\\n\")\n",
    "ST100_X_training, ST100_X_validation, ST100_Y_training, ST100_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST20','ST10','ST5','ST2','ID'], \n",
    "                                                                                           'ST100', \n",
    "                                                                                           ST100_rf_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST100_Y_test_preds_df = predict_and_plot(ST100_rf_regressor, ST100_X_test, ST100_X_validation, ST100_Y_validation, 'ST100')\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST100_forecast_df = pd.concat([ST50_forecast_df, ST100_Y_test_preds_df], axis=1)\n",
    "\n",
    "# Merge the Original DataFrame with the predicted Series DataFrames\n",
    "ST_forecast_df = ST100_forecast_df.copy()\n",
    "\n",
    "# Save the merged DataFrame to an Excel file\n",
    "ST_forecast_df.to_excel(f'data/{current_month}_{current_day}_Soil_temperature_forecast_10_days.xlsx', index=True)\n",
    "ST_forecast_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4993f-e8e4-4c6c-90c7-9b5d48bee1d1",
   "metadata": {},
   "source": [
    "## Observations\n",
    "#### The soil temperature forecast was compared with the real-time soil temperature on NMBU BIOKLIM website and the forecast is a good approximation.\n",
    "#### The following reasons may be the cause for the deviation:\n",
    "##### 1. The MET Norway weather forecast has little error\n",
    "##### 2. Weather forecast by its nature is prone to unpredictable factors and the initial weather forecast may not be accurate.\n",
    "##### 3. The NMBU BIOKLIM real-time data is not verified and there might be some error on the display\n",
    "##### 4. The ML prediction algorithm has a mean absolute error of 0.72 for the ST2 prediction which requires an improvement to make it close to the resolution of temperature measurements (0.1 °C) by NMBU soil temeprature sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73300c7a-514f-48ba-ad13-44a2807ae450",
   "metadata": {},
   "source": [
    "# 7. Performance check of the 10 days soil temperature forecast using different features\n",
    "### a. First option is to check for performance on the prediction algorithm is using stacking regressors with only few features considered for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84442904-e0f9-4645-8a15-a8334e92a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests as requests\n",
    "import json as json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "import time\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Initialize the expires and last_modified variables \n",
    "expire_time = None\n",
    "last_modified = None\n",
    "latitude = 59.6603 # in degrees\n",
    "longitude = 10.7817 # in degrees\n",
    "altitude = 93 # in meters\n",
    "\n",
    "# Extract the month and day\n",
    "current_datetime = datetime.now()\n",
    "# Extract date and time information at once\n",
    "formatted_date = current_datetime.strftime(\"%Y-%m-%d\")\n",
    "formatted_time = current_datetime.strftime(\"%H-%M-%S\")\n",
    "\n",
    "# Now let us fetch the meteorological forecast data from MET Norway\n",
    "request_url = f'https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={latitude}&lon={longitude}&altitude={altitude}'\n",
    "# Email address to include as User-Agent ID\n",
    "user_agent = '259646@usn.no'\n",
    "# Define headers with User-Agent\n",
    "headers = {'User-Agent': user_agent, 'If-Modified-Since':last_modified}\n",
    "# Send HTTP GET request with headers\n",
    "response = requests.get(request_url, headers=headers)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "     # Save the expires and last_modified variables to avoid frequency request to the server\n",
    "    expires_header = response.headers.get('expires')\n",
    "    last_modified_header = response.headers.get('last-modified')\n",
    "\n",
    "    # Convert expires and last_modified to RFC 1123 format\n",
    "    expire_time = datetime.strptime(expires_header, '%a, %d %b %Y %H:%M:%S GMT').strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "    last_modified = datetime.strptime(last_modified_header, '%a, %d %b %Y %H:%M:%S GMT').strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "    weather_data = response.json()\n",
    "else:\n",
    "    # Request failed, print error message\n",
    "    weather_data = {}\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "\n",
    "# Extract relevant information\n",
    "rows = []\n",
    "for entry in weather_data['properties']['timeseries']:\n",
    "    instant_details = entry['data']['instant']['details']\n",
    "    next_6_hours_details = entry['data'].get('next_6_hours', {}).get('details', None)\n",
    "    row = {\n",
    "        'time': entry['time'],\n",
    "        'instant_air_temperature': instant_details['air_temperature'],\n",
    "        'instant_air_pressure_at_sea_level': instant_details['air_pressure_at_sea_level'],\n",
    "        'instant_relative_humidity': instant_details['relative_humidity'],\n",
    "        'next_6_hours_air_temperature_max': next_6_hours_details['air_temperature_max'] if next_6_hours_details else None,\n",
    "        'next_6_hours_air_temperature_min': next_6_hours_details['air_temperature_min'] if next_6_hours_details else None,\n",
    "        'next_6_hours_precipitation_amount': next_6_hours_details['precipitation_amount'] if next_6_hours_details else None\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "MET_weather_df = pd.DataFrame(rows)\n",
    "# Save to file\n",
    "MET_weather_df.to_csv(f'data/{formatted_date}-{formatted_time}_MET_weather_forecast_10_days.csv')\n",
    "\n",
    "MET_weather_df['time'] = pd.to_datetime(MET_weather_df['time'])\n",
    "# Add snow depth with zero values for testing during the summer season as there is no snow but should be automated for all seasons not part of the weather forecast\n",
    "MET_weather_df['snow_depth_cm'] = 0.0\n",
    "\n",
    "# Group DataFrame by date\n",
    "grouped_df = MET_weather_df.groupby(MET_weather_df['time'].dt.date)\n",
    "\n",
    "# Calculate daily mean, min, max\n",
    "MET_daily_stats = grouped_df.agg(\n",
    "    mean_air_temperature_2m=('instant_air_temperature', 'mean'),\n",
    "    min_air_temperature_2m=('next_6_hours_air_temperature_min', 'min'),\n",
    "    max_air_temperature_2m=('next_6_hours_air_temperature_max', 'max'),\n",
    "    relative_humidity=('instant_relative_humidity', 'mean'),\n",
    "    air_pressure_2m_mbar=('instant_air_pressure_at_sea_level', 'mean'),\n",
    "    precipitation_mm=('next_6_hours_precipitation_amount', 'mean'),    \n",
    "    snow_depth_cm=('snow_depth_cm', 'mean')\n",
    ")\n",
    "\n",
    "# Option 2: Fetch weather data from Open Meteo API\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://api.open-meteo.com/v1/metno\"\n",
    "params = {\n",
    "\t\"latitude\": 59.6602,\n",
    "\t\"longitude\": 10.7817,\n",
    "\t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\", \"snowfall\"],\n",
    "\t\"timezone\": \"auto\",\n",
    "\t\"past_hours\": 24,\n",
    "\t\"forecast_hours\": 24\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_rain = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_snowfall = hourly.Variables(4).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "hourly_data[\"rain\"] = hourly_rain\n",
    "hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "\n",
    "open_meteo_hourly_df = pd.DataFrame(data = hourly_data)\n",
    "open_meteo_hourly_df .to_csv(f'data/{formatted_date}_{formatted_time}_Meteo_weather_forecast_2_days.csv')\n",
    "\n",
    "# Create function to evaluate model on few different levels\n",
    "def predict_and_score(model, X_train, X_valid, Y_train, Y_valid):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = cross_val_predict(model, X_train, Y_train, cv=5)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    scores = {\"Training Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_valid, val_preds),\n",
    "              \"Training Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds),             \n",
    "              \"Training Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),\n",
    "              \"Training Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Training Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              \"Training Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              \"Training Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)}\n",
    "    return scores\n",
    "\n",
    "def fit_model(dataset_df, features_dropped, target, model_type):\n",
    "    X_f = dataset_df.drop(features_dropped, axis=1)\n",
    "    Y_f = dataset_df[target]\n",
    "    # Split the X and Y data in to train and test data\n",
    "    X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(X_f, Y_f, test_size=0.15)\n",
    "    model_type.fit(X_train_2, Y_train_2)\n",
    "    scores = predict_and_score(model_type, X_train_2, X_valid_2, Y_train_2, Y_valid_2)\n",
    "    print(scores)\n",
    "    return X_train_2, X_valid_2, Y_train_2, Y_valid_2\n",
    "    \n",
    "def calculate_pressure(P0, T, altitude):\n",
    "    # Constants\n",
    "    g = 9.80665  # Acceleration due to gravity (m/s^2)\n",
    "    M = 0.0289644  # Molar mass of Earth's air (kg/mol)\n",
    "    R = 8.31432  # Universal gas constant (J/(mol*K))    \n",
    "    # Calculate pressure\n",
    "    pressure = P0 * math.exp((-g * M * altitude) / (R * (T + 273.15)))\n",
    "    return pressure\n",
    "# calculate the air_pressure at certain altitude from the sea level air pressure forecast fetched from MET Norway\n",
    "MET_daily_stats['air_pressure_2m_mbar'] = MET_daily_stats.apply(lambda row: calculate_pressure(row['air_pressure_2m_mbar'], row['mean_air_temperature_2m'], altitude), axis=1)\n",
    "# Convert index to datetime\n",
    "MET_daily_stats.index = pd.to_datetime(MET_daily_stats.index)\n",
    "# Extract month and day from the index\n",
    "MET_daily_stats['month'] = MET_daily_stats.index.month\n",
    "MET_daily_stats['day'] = MET_daily_stats.index.day\n",
    "# Create test set for soil temperature at 2cm as ST_X_test\n",
    "ST2_X_test = MET_daily_stats\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_plot(model, ST_X_test, ST_X_validation, ST_Y_validation, name):\n",
    "   # Predict the test set which is forecast data\n",
    "   ST_Y_test_preds = model.predict(ST_X_test)\n",
    "   # Changes the predicted array values to pandas series\n",
    "   ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "   # Convert the Series to a DataFrame\n",
    "   ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "   ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "   # Predict the validation set\n",
    "   ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "   # Change validation predictions to pandas series\n",
    "   ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "   # Make the original and predicted series to have the same index\n",
    "   ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "   # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "   ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "   ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "   # Calculate mean absolute error\n",
    "   ST_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Calculate mean squared error\n",
    "   ST_mse = mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Calculate the R^2 score\n",
    "   ST_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Plot the sorted values\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Observed Values')\n",
    "   plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Predicted Values')\n",
    "   # Display the mean absolute error as text annotation\n",
    "   plt.text(0.4, 0.95, f'MAE: {ST_mae:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.text(0.6, 0.95, f'MSE: {ST_mse:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.text(0.8, 0.95, f'R^2 score: {ST_r2_score:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.xlabel('Index', fontsize=14)\n",
    "   plt.ylabel(f'Soil Temperature at {name}(°C)')\n",
    "   plt.title(f'Comparison of {name} Observed vs Predicted Values')\n",
    "   plt.legend()\n",
    "   plt.grid(True)\n",
    "   plt.show()\n",
    "   return ST_Y_test_preds_df\n",
    "\n",
    "def plot_stacking_models(estimators_array, X_train, Y_train, target):\n",
    "    # Measure and plot the results\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
    "    axs = np.ravel(axs)\n",
    "    \n",
    "    for ax, (name, est) in zip(axs, estimators_array):\n",
    "        scorers = {\"R^2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\", \"MSE\":\"neg_mean_squared_error\"}\n",
    "    \n",
    "        start_time = time.time()\n",
    "        scores = cross_validate(est, X_train, Y_train, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "        y_pred = cross_val_predict(est, X_train, Y_train, n_jobs=-1, verbose=0)\n",
    "        scores = {\n",
    "            key: (\n",
    "                f\"{np.abs(np.mean(scores[f'test_{value}'])):.4f} ± \"\n",
    "                f\"{np.std(scores[f'test_{value}']):.4f}\"\n",
    "            )\n",
    "            for key, value in scorers.items()\n",
    "        }\n",
    "    \n",
    "        display = PredictionErrorDisplay.from_predictions(\n",
    "            y_true=Y_train,\n",
    "            y_pred=y_pred,\n",
    "            kind=\"actual_vs_predicted\",\n",
    "            ax=ax,\n",
    "            scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "            line_kwargs={\"color\": \"tab:red\"},\n",
    "        )\n",
    "        ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.4f} seconds\", fontsize=14)\n",
    "        # Set custom x-label and y-label\n",
    "        ax.set_xlabel(f\"Soil Temperature Forecast at {target} (°C)\")\n",
    "        ax.set_ylabel(f\"Observed Soil Temperature at {target} (°C)\")\n",
    "        for name, score in scores.items():\n",
    "            ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
    "        ax.legend(loc=\"upper left\")\n",
    "    \n",
    "    plt.suptitle(\"Comparison between single predictor versus stacked predictors\")\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(f'data/results/{target}_stacking_regressors_forecast_vs_Observed_values.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "\n",
    "global_estimators = []\n",
    "# ============================ Fit the soil temperature at 2cm from the forecast weather data ===================================== \n",
    "def initialize_stacking_regressor():\n",
    "    # Stack of predictors on a single data set\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                           min_samples_leaf=1,\n",
    "                                           min_samples_split=2,\n",
    "                                           max_features='sqrt',\n",
    "                                           max_depth=20,\n",
    "                                           bootstrap=False,\n",
    "                                           random_state=42)\n",
    "    lasso_regressor = LassoCV()\n",
    "    gbdt_regresssor = HistGradientBoostingRegressor(random_state=42)\n",
    "    # Define the estimators to be used in the stacking regressor\n",
    "    local_estimators = [\n",
    "        (\"Random Forest\", rf_regressor),\n",
    "        (\"Lasso\", lasso_regressor),\n",
    "        (\"Gradient Boosting\", gbdt_regresssor),\n",
    "    ]\n",
    "    \n",
    "    # Initialize the stacking regressor\n",
    "    stackingRegressorModel = StackingRegressor(estimators=local_estimators, final_estimator=RidgeCV())\n",
    "    # Append to estimators\n",
    "    global_estimators.extend(local_estimators)\n",
    "    global_estimators.append((\"Stacking Regressor\", stackingRegressorModel))\n",
    "    return stackingRegressorModel\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "ST2_stacking_regressor = initialize_stacking_regressor()\n",
    "ST2_X_test.to_excel('models/test_data/ST2_X_test.xlsx')\n",
    "\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST2 with weather data selected features for forecast testing ==============================\\n\")\n",
    "ST2_X_training, ST2_X_validation, ST2_Y_training, ST2_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','ST100','ST50','ST20','ST10','ST5','ST2','ID'], \n",
    "                                                                                           'ST2', \n",
    "                                                                                           ST2_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "# The stacked models evaluation plots\n",
    "plot_stacking_models(global_estimators, ST2_X_training, ST2_Y_training, 'ST2')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST2_Y_test_preds_df = predict_plot(ST2_stacking_regressor, ST2_X_test, ST2_X_validation, ST2_Y_validation, 'ST2')\n",
    "dump(ST2_stacking_regressor, filename=\"models/soil_temps/ST2.joblib\");\n",
    "\n",
    "# Merge the Original DataFrame with the new predicted dataframe\n",
    "ST2_forecast_df = pd.concat([ST2_X_test, ST2_Y_test_preds_df], axis=1)\n",
    "# ======================= Fit the soil temperature at 5cm from the predictions at 2cm ================================\n",
    "# Copy the ST2 predicted values as test data input for ST5\n",
    "ST5_features = ['ST2', 'month']\n",
    "ST5_X_test = ST2_forecast_df[ST5_features]\n",
    "ST5_X_test.index = ST2_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST5_stacking_regressor= initialize_stacking_regressor()\n",
    "ST5_X_test.to_excel('models/test_data/ST5_X_test.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST5 with ST2 as a feature for forecast testing ==============================\\n\")\n",
    "ST5_X_training, ST5_X_validation, ST5_Y_training, ST5_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST50','ST20','ST10','ST5','ID'], \n",
    "                                                                                           'ST5', \n",
    "                                                                                           ST5_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "# The stacked models evaluation plots\n",
    "plot_stacking_models(global_estimators, ST5_X_training, ST5_Y_training, 'ST5')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST5_Y_test_preds_df = predict_plot(ST5_stacking_regressor, ST5_X_test, ST5_X_validation, ST5_Y_validation, 'ST5')\n",
    "dump(ST5_stacking_regressor, filename=\"models/soil_temps/ST5.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST5_forecast_df = pd.concat([ST2_forecast_df, ST5_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 10cm from the predictions at 5cm ================================\n",
    "# Copy the ST5 predicted values as test data input for ST10\n",
    "ST10_features = ['ST5', 'month']\n",
    "ST10_X_test = ST5_forecast_df[ST10_features]\n",
    "ST10_X_test.index = ST5_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST10_stacking_regressor = initialize_stacking_regressor()\n",
    "ST10_X_test.to_excel('models/test_data/ST10_X_test.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST10 with ST5 as a feature for forecast testing ==============================\\n\")\n",
    "ST10_X_training, ST10_X_validation, ST10_Y_training, ST10_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST50','ST20','ST10','ST2','ID'], \n",
    "                                                                                           'ST10', \n",
    "                                                                                           ST10_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "# The stacked model evaluation plots\n",
    "plot_stacking_models(global_estimators, ST10_X_training, ST10_Y_training, 'ST10')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST10_Y_test_preds_df = predict_plot(ST10_stacking_regressor, ST10_X_test, ST10_X_validation, ST10_Y_validation, 'ST10')\n",
    "dump(ST10_stacking_regressor, filename=\"models/soil_temps/ST10.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST10_forecast_df = pd.concat([ST5_forecast_df, ST10_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 20cm from the predictions at 10cm and month ================================\n",
    "# Copy the ST10 predicted values as test data input for ST20\n",
    "ST20_features = ['ST10', 'month']\n",
    "ST20_X_test = ST10_forecast_df[ST20_features]\n",
    "ST20_X_test.index = ST10_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST20_stacking_regressor = initialize_stacking_regressor()\n",
    "ST20_X_test.to_excel('models/test_data/ST20_X_test.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST20 with ST10 as a feature for forecast testing ==============================\\n\")\n",
    "ST20_X_training, ST20_X_validation, ST20_Y_training, ST20_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST50','ST20','ST5','ST2','ID'], \n",
    "                                                                                           'ST20', \n",
    "                                                                                           ST20_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "# The stacked model evaluation plots\n",
    "plot_stacking_models(global_estimators, ST20_X_training, ST20_Y_training, 'ST20')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST20_Y_test_preds_df = predict_plot(ST20_stacking_regressor, ST20_X_test, ST20_X_validation, ST20_Y_validation, 'ST20')\n",
    "dump(ST20_stacking_regressor, filename=\"models/soil_temps/ST20.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST20_forecast_df = pd.concat([ST10_forecast_df, ST20_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 50cm from the predictions at 20cm and month ================================\n",
    "# Copy the ST20 predicted values as test data input for ST50\n",
    "ST50_features = ['ST20', 'month']\n",
    "ST50_X_test = ST20_forecast_df[ST50_features]\n",
    "ST50_X_test.index = ST20_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST50_stacking_regressor = initialize_stacking_regressor()\n",
    "ST50_X_test.to_excel('models/test_data/ST50_X_test.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST50 with ST20 as a feature for forecast testing ==============================\\n\")\n",
    "ST50_X_training, ST50_X_validation, ST50_Y_training, ST50_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST50','ST10','ST5','ST2','ID'], \n",
    "                                                                                           'ST50', \n",
    "                                                                                           ST50_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "# The stacked model evaluation plots\n",
    "plot_stacking_models(global_estimators, ST50_X_training, ST50_Y_training, 'ST50')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST50_Y_test_preds_df = predict_plot(ST50_stacking_regressor, ST50_X_test, ST50_X_validation, ST50_Y_validation, 'ST50')\n",
    "dump(ST50_stacking_regressor, filename=\"models/soil_temps/ST50.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST50_forecast_df = pd.concat([ST20_forecast_df, ST50_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 100cm from the predictions at 50cm and month ================================\n",
    "# Copy the ST50 predicted values as test data input for ST100\n",
    "ST100_features = ['ST50', 'month']\n",
    "ST100_X_test = ST50_forecast_df[ST100_features]\n",
    "ST100_X_test.index = ST50_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST100_stacking_regressor = initialize_stacking_regressor()\n",
    "ST100_X_test.to_excel('models/test_data/ST100_X_test.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST100 with ST50 as a feature for forecast testing ==============================\\n\")\n",
    "ST100_X_training, ST100_X_validation, ST100_Y_training, ST100_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['mean_air_temperature_2m','min_air_temperature_2m','max_air_temperature_2m','relative_humidity','air_pressure_2m_mbar','precipitation_mm','earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','snow_depth_cm','day','ST100','ST20','ST10','ST5','ST2','ID'], \n",
    "                                                                                           'ST100', \n",
    "                                                                                           ST100_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "# The stacked model evaluation plots\n",
    "plot_stacking_models(global_estimators, ST100_X_training, ST100_Y_training, 'ST100')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST100_Y_test_preds_df = predict_plot(ST100_stacking_regressor, ST100_X_test, ST100_X_validation, ST100_Y_validation, 'ST100')\n",
    "dump(ST100_stacking_regressor, filename=\"models/soil_temps/ST100.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST100_forecast_df = pd.concat([ST50_forecast_df, ST100_Y_test_preds_df], axis=1)\n",
    "\n",
    "# Merge the Original DataFrame with the predicted Series DataFrames\n",
    "ST_forecast_df = ST100_forecast_df.copy()\n",
    "\n",
    "# Save the merged DataFrame to an Excel file\n",
    "ST_forecast_df.to_excel(f'data/{formatted_date}-{formatted_time}_Soil_temperature_forecast_10_days.xlsx', index=True)\n",
    "ST_forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70704e44-570b-40c8-8910-b4b713cb2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_denormalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84d871-6fb6-4a43-adc9-0cba64fb455f",
   "metadata": {},
   "source": [
    "### b. Second option is to check for performance on the prediction algorithm is using stacking regressors with all features considered for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b6867-da3c-4b9b-a6af-99ef6600ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests as requests\n",
    "import json as json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "import time\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Initialize the expires and last_modified variables \n",
    "expire_time = None\n",
    "last_modified = None\n",
    "latitude = 59.6603 # in degrees\n",
    "longitude = 10.7817 # in degrees\n",
    "altitude = 93 # in meters\n",
    "\n",
    "# Extract the month and day\n",
    "current_datetime = datetime.now()\n",
    "# Extract date and time information at once\n",
    "formatted_date = current_datetime.strftime(\"%Y-%m-%d\")\n",
    "formatted_time = current_datetime.strftime(\"%H-%M-%S\")\n",
    "\n",
    "# Now let us fetch the meteorological forecast data from MET Norway\n",
    "request_url = f'https://api.met.no/weatherapi/locationforecast/2.0/complete?lat={latitude}&lon={longitude}&altitude={altitude}'\n",
    "# Email address to include as User-Agent ID\n",
    "user_agent = '259646@usn.no'\n",
    "# Define headers with User-Agent\n",
    "headers = {'User-Agent': user_agent, 'If-Modified-Since':last_modified}\n",
    "# Send HTTP GET request with headers\n",
    "response = requests.get(request_url, headers=headers)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "     # Save the expires and last_modified variables to avoid frequency request to the server\n",
    "    expires_header = response.headers.get('expires')\n",
    "    last_modified_header = response.headers.get('last-modified')\n",
    "\n",
    "    # Convert expires and last_modified to RFC 1123 format\n",
    "    expire_time = datetime.strptime(expires_header, '%a, %d %b %Y %H:%M:%S GMT').strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "    last_modified = datetime.strptime(last_modified_header, '%a, %d %b %Y %H:%M:%S GMT').strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "    weather_data = response.json()\n",
    "else:\n",
    "    # Request failed, print error message\n",
    "    weather_data = {}\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "\n",
    "# Extract relevant information\n",
    "rows = []\n",
    "for entry in weather_data['properties']['timeseries']:\n",
    "    instant_details = entry['data']['instant']['details']\n",
    "    next_6_hours_details = entry['data'].get('next_6_hours', {}).get('details', None)\n",
    "    row = {\n",
    "        'time': entry['time'],\n",
    "        'instant_air_temperature': instant_details['air_temperature'],\n",
    "        'instant_air_pressure_at_sea_level': instant_details['air_pressure_at_sea_level'],\n",
    "        'instant_relative_humidity': instant_details['relative_humidity'],\n",
    "        'next_6_hours_air_temperature_max': next_6_hours_details['air_temperature_max'] if next_6_hours_details else None,\n",
    "        'next_6_hours_air_temperature_min': next_6_hours_details['air_temperature_min'] if next_6_hours_details else None,\n",
    "        'next_6_hours_precipitation_amount': next_6_hours_details['precipitation_amount'] if next_6_hours_details else None\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "MET_weather_df = pd.DataFrame(rows)\n",
    "# Save to file\n",
    "MET_weather_df.to_csv(f'data/{formatted_date}-{formatted_time}_MET_weather_forecast_10_days.csv')\n",
    "\n",
    "MET_weather_df['time'] = pd.to_datetime(MET_weather_df['time'])\n",
    "# Add snow depth with zero values for testing during the summer season as there is no snow but should be automated for all seasons not part of the weather forecast\n",
    "MET_weather_df['snow_depth_cm'] = 0.0\n",
    "# # Convert \"time\" column to datetime object\n",
    "# MET_weather_df['time'] = pd.to_datetime(MET_weather_df['time'])\n",
    "# # Extract date from datetime object\n",
    "# MET_weather_df['date'] = MET_weather_df['time'].dt.date\n",
    "# # Check if each date has 24 hours\n",
    "# complete_dates = MET_weather_df['date'].value_counts()[MET_weather_df['date'].value_counts() == 24].index.tolist()\n",
    "# # Filter rows with complete dates\n",
    "# MET_weather_df = MET_weather_df[MET_weather_df['date'].isin(complete_dates)]\n",
    "# # Drop the 'date' column\n",
    "# MET_weather_df = MET_weather_df.drop(columns=['date'])\n",
    "\n",
    "# Group DataFrame by date\n",
    "grouped_df = MET_weather_df.groupby(MET_weather_df['time'].dt.date)\n",
    "\n",
    "# Calculate daily mean, min, max\n",
    "MET_daily_stats = grouped_df.agg(\n",
    "    mean_air_temperature_2m=('instant_air_temperature', 'mean'),\n",
    "    min_air_temperature_2m=('next_6_hours_air_temperature_min', 'min'),\n",
    "    max_air_temperature_2m=('next_6_hours_air_temperature_max', 'max'),\n",
    "    relative_humidity=('instant_relative_humidity', 'mean'),\n",
    "    air_pressure_2m_mbar=('instant_air_pressure_at_sea_level', 'mean'),\n",
    "    precipitation_mm=('next_6_hours_precipitation_amount', 'mean'),    \n",
    "    snow_depth_cm=('snow_depth_cm', 'mean')\n",
    ")\n",
    "\n",
    "# Option 2: Fetch weather data from Open Meteo API\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://api.open-meteo.com/v1/metno\"\n",
    "params = {\n",
    "\t\"latitude\": 59.6602,\n",
    "\t\"longitude\": 10.7817,\n",
    "\t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\", \"snowfall\"],\n",
    "\t\"timezone\": \"auto\",\n",
    "\t\"past_hours\": 24,\n",
    "\t\"forecast_hours\": 24\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_rain = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_snowfall = hourly.Variables(4).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "hourly_data[\"rain\"] = hourly_rain\n",
    "hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "\n",
    "open_meteo_hourly_df = pd.DataFrame(data = hourly_data)\n",
    "open_meteo_hourly_df .to_csv(f'data/{formatted_date}_{formatted_time}_Meteo_weather_forecast_2_days.csv')\n",
    "\n",
    "# Create function to evaluate model on few different levels\n",
    "def predict_and_score(model, X_train, X_valid, Y_train, Y_valid):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = cross_val_predict(model, X_train, Y_train, cv=5)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    scores = {\"Training Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_valid, val_preds),\n",
    "              \"Training Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds),             \n",
    "              \"Training Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),\n",
    "              \"Training Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Training Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              \"Training Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              \"Training Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)}\n",
    "    return scores\n",
    "\n",
    "def fit_model(dataset_df, features_dropped, target, model_type):\n",
    "    X_f = dataset_df.drop(features_dropped, axis=1)\n",
    "    Y_f = dataset_df[target]\n",
    "    # Split the X and Y data in to train and test data\n",
    "    X_train_2, X_valid_2, Y_train_2, Y_valid_2 = train_test_split(X_f, Y_f, test_size=0.15)\n",
    "    model_type.fit(X_train_2, Y_train_2)\n",
    "    scores = predict_and_score(model_type, X_train_2, X_valid_2, Y_train_2, Y_valid_2)\n",
    "    print(scores)\n",
    "    return X_train_2, X_valid_2, Y_train_2, Y_valid_2\n",
    "    \n",
    "def calculate_pressure(P0, T, altitude):\n",
    "    # Constants\n",
    "    g = 9.80665  # Acceleration due to gravity (m/s^2)\n",
    "    M = 0.0289644  # Molar mass of Earth's air (kg/mol)\n",
    "    R = 8.31432  # Universal gas constant (J/(mol*K))    \n",
    "    # Calculate pressure\n",
    "    pressure = P0 * math.exp((-g * M * altitude) / (R * (T + 273.15)))\n",
    "    return pressure\n",
    "# calculate the air_pressure at certain altitude from the sea level air pressure forecast fetched from MET Norway\n",
    "MET_daily_stats['air_pressure_2m_mbar'] = MET_daily_stats.apply(lambda row: calculate_pressure(row['air_pressure_2m_mbar'], row['mean_air_temperature_2m'], altitude), axis=1)\n",
    "# Convert index to datetime\n",
    "MET_daily_stats.index = pd.to_datetime(MET_daily_stats.index)\n",
    "# Extract month and day from the index\n",
    "MET_daily_stats['month'] = MET_daily_stats.index.month\n",
    "MET_daily_stats['day'] = MET_daily_stats.index.day\n",
    "# Create test set for soil temperature at 2cm as ST_X_test\n",
    "ST2_X_test = MET_daily_stats\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_plot(model, ST_X_test, ST_X_validation, ST_Y_validation, name):\n",
    "   # Predict the test set which is forecast data\n",
    "   ST_Y_test_preds = model.predict(ST_X_test)\n",
    "   # Changes the predicted array values to pandas series\n",
    "   ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "   # Convert the Series to a DataFrame\n",
    "   ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "   ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "   # Predict the validation set\n",
    "   ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "   # Change validation predictions to pandas series\n",
    "   ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "   # Make the original and predicted series to have the same index\n",
    "   ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "   # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "   ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "   ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "   # Calculate mean absolute error\n",
    "   ST_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Calculate mean squared error\n",
    "   ST_mse = mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Calculate the R^2 score\n",
    "   ST_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "   # Plot the sorted values\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Observed Values')\n",
    "   plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Predicted Values')\n",
    "   # Display the mean absolute error as text annotation\n",
    "   plt.text(0.4, 0.95, f'MAE: {ST_mae:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.text(0.6, 0.95, f'MSE: {ST_mse:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.text(0.8, 0.95, f'R^2 score: {ST_r2_score:.2f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "   plt.xlabel('Index', fontsize=14)\n",
    "   plt.ylabel(f'Soil Temperature at {name}(°C)')\n",
    "   plt.title(f'Comparison of {name} Observed vs Predicted Values')\n",
    "   plt.legend()\n",
    "   plt.grid(True)\n",
    "   plt.show()\n",
    "   return ST_Y_test_preds_df\n",
    "\n",
    "def plot_stacking_models(estimators_array, X_train, Y_train, target):\n",
    "    # Measure and plot the results\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
    "    axs = np.ravel(axs)\n",
    "    \n",
    "    for ax, (name, est) in zip(axs, estimators_array):\n",
    "        scorers = {\"R^2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\", \"MSE\":\"neg_mean_squared_error\"}\n",
    "    \n",
    "        start_time = time.time()\n",
    "        scores = cross_validate(est, X_train, Y_train, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "        y_pred = cross_val_predict(est, X_train, Y_train, n_jobs=-1, verbose=0)\n",
    "        scores = {\n",
    "            key: (\n",
    "                f\"{np.abs(np.mean(scores[f'test_{value}'])):.4f} ± \"\n",
    "                f\"{np.std(scores[f'test_{value}']):.4f}\"\n",
    "            )\n",
    "            for key, value in scorers.items()\n",
    "        }\n",
    "    \n",
    "        display = PredictionErrorDisplay.from_predictions(\n",
    "            y_true=Y_train,\n",
    "            y_pred=y_pred,\n",
    "            kind=\"actual_vs_predicted\",\n",
    "            ax=ax,\n",
    "            scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "            line_kwargs={\"color\": \"tab:red\"},\n",
    "        )\n",
    "        ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.4f} seconds\", fontsize=14)\n",
    "        # Set custom x-label and y-label\n",
    "        ax.set_xlabel(f\"Soil Temperature Forecast at {target} (°C)\")\n",
    "        ax.set_ylabel(f\"Observed Soil Temperature at {target} (°C)\")\n",
    "        for name, score in scores.items():\n",
    "            # if(name=='MSE'):\n",
    "            #     score = score/std\n",
    "            ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
    "        ax.legend(loc=\"upper left\")\n",
    "    \n",
    "    plt.suptitle(\"Comparison between single predictor versus stacked predictors\")\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(f'data/results/{target}_stacking_regressors_forecast_vs_Observed_values.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "\n",
    "global_estimators = []\n",
    "# ============================ Fit the soil temperature at 2cm from the forecast weather data ===================================== \n",
    "def initialize_stacking_regressor():\n",
    "    # Stack of predictors on a single data set\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                           min_samples_leaf=1,\n",
    "                                           min_samples_split=2,\n",
    "                                           max_features='sqrt',\n",
    "                                           max_depth=None,\n",
    "                                           bootstrap=False,\n",
    "                                           random_state=42)\n",
    "    lasso_regressor = LassoCV()\n",
    "    gbdt_regresssor = HistGradientBoostingRegressor(random_state=42)\n",
    "    # Define the estimators to be used in the stacking regressor\n",
    "    local_estimators = [\n",
    "        (\"Random Forest\", rf_regressor),\n",
    "        (\"Lasso\", lasso_regressor),\n",
    "        (\"Gradient Boosting\", gbdt_regresssor),\n",
    "    ]\n",
    "    \n",
    "    # Initialize the stacking regressor\n",
    "    stackingRegressorModel = StackingRegressor(estimators=local_estimators, final_estimator=RidgeCV())\n",
    "    # Append to estimators\n",
    "    global_estimators.extend(local_estimators)\n",
    "    global_estimators.append((\"Stacking Regressor\", stackingRegressorModel))\n",
    "    return stackingRegressorModel\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "ST2_stacking_regressor = initialize_stacking_regressor()\n",
    "ST2_X_test.to_excel('models/test_data/ST2_X_test_all.xlsx')\n",
    "\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST2 with weather data selected features for forecast testing ==============================\\n\")\n",
    "ST2_X_training, ST2_X_validation, ST2_Y_training, ST2_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','ST100','ST50','ST20','ST10','ST5','ST2','ID'], \n",
    "                                                                                           'ST2', \n",
    "                                                                                           ST2_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Make sure the columns in the X data are ordered\n",
    "ST2_X_columns = ST2_X_training.columns\n",
    "ST2_X_validation = ST2_X_validation[ST2_X_columns]\n",
    "ST2_X_test = ST2_X_test[ST2_X_columns]\n",
    "# The stacked models evaluation plots\n",
    "plot_stacking_models(global_estimators, ST2_X_training, ST2_Y_training, 'ST2')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST2_Y_test_preds_df = predict_plot(ST2_stacking_regressor, ST2_X_test, ST2_X_validation, ST2_Y_validation, 'ST2')\n",
    "dump(ST2_stacking_regressor, filename=\"models/soil_temps/ST2_all.joblib\");\n",
    "\n",
    "# Merge the Original DataFrame with the new predicted dataframe\n",
    "ST2_forecast_df = pd.concat([ST2_X_test, ST2_Y_test_preds_df], axis=1)\n",
    "# ======================= Fit the soil temperature at 5cm from the predictions at 2cm ================================\n",
    "# Copy the ST2 predicted values as test data input for ST5\n",
    "# ST5_features = ['ST2', 'month']\n",
    "ST5_X_test = ST2_forecast_df.copy()\n",
    "ST5_X_test.index = ST2_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST5_stacking_regressor= initialize_stacking_regressor()\n",
    "ST5_X_test.to_excel('models/test_data/ST5_X_test_all.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST5 with ST2 as a feature for forecast testing ==============================\\n\")\n",
    "ST5_X_training, ST5_X_validation, ST5_Y_training, ST5_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','ST100','ST50','ST20','ST10','ST5','ID'], \n",
    "                                                                                           'ST5', \n",
    "                                                                                           ST5_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Make sure the columns in the X data are ordered\n",
    "ST5_X_columns = ST5_X_training.columns\n",
    "ST5_X_validation = ST5_X_validation[ST5_X_columns]\n",
    "ST5_X_test = ST5_X_test[ST5_X_columns]\n",
    "# The stacked models evaluation plots\n",
    "plot_stacking_models(global_estimators, ST5_X_training, ST5_Y_training, 'ST5')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST5_Y_test_preds_df = predict_plot(ST5_stacking_regressor, ST5_X_test, ST5_X_validation, ST5_Y_validation, 'ST5')\n",
    "dump(ST5_stacking_regressor, filename=\"models/soil_temps/ST5_all.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST5_forecast_df = pd.concat([ST2_forecast_df, ST5_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 10cm from the predictions at 5cm ================================\n",
    "# Copy the ST5 predicted values as test data input for ST10\n",
    "# ST10_features = ['ST5', 'month']\n",
    "ST10_X_test = ST5_forecast_df.copy()\n",
    "ST10_X_test.index = ST5_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST10_stacking_regressor = initialize_stacking_regressor()\n",
    "ST10_X_test.to_excel('models/test_data/ST10_X_test_all.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST10 with ST5 as a feature for forecast testing ==============================\\n\")\n",
    "ST10_X_training, ST10_X_validation, ST10_Y_training, ST10_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','ST100','ST50','ST20','ST10','ID'], \n",
    "                                                                                           'ST10', \n",
    "                                                                                           ST10_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Make sure the columns in the X data are ordered\n",
    "ST10_X_columns = ST10_X_training.columns\n",
    "ST10_X_validation = ST10_X_validation[ST10_X_columns]\n",
    "ST10_X_test = ST10_X_test[ST10_X_columns]\n",
    "# The stacked model evaluation plots\n",
    "plot_stacking_models(global_estimators, ST10_X_training, ST10_Y_training, 'ST10')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST10_Y_test_preds_df = predict_plot(ST10_stacking_regressor, ST10_X_test, ST10_X_validation, ST10_Y_validation, 'ST10')\n",
    "dump(ST10_stacking_regressor, filename=\"models/soil_temps/ST10_all.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST10_forecast_df = pd.concat([ST5_forecast_df, ST10_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 20cm from the predictions at 10cm and month ==============================\n",
    "# Copy the ST10 predicted values as test data input for ST20\n",
    "# ST20_features = ['ST10', 'month']\n",
    "ST20_X_test = ST10_forecast_df.copy()\n",
    "ST20_X_test.index = ST10_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST20_stacking_regressor = initialize_stacking_regressor()\n",
    "ST20_X_test.to_excel('models/test_data/ST20_X_test_all.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST20 with ST10 as a feature for forecast testing ==============================\\n\")\n",
    "ST20_X_training, ST20_X_validation, ST20_Y_training, ST20_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','ST100','ST50','ST20','ID'], \n",
    "                                                                                           'ST20', \n",
    "                                                                                           ST20_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Make sure the columns in the X data are ordered\n",
    "ST20_X_columns = ST20_X_training.columns\n",
    "ST20_X_validation = ST20_X_validation[ST20_X_columns]\n",
    "ST20_X_test = ST20_X_test[ST20_X_columns]\n",
    "# The stacked model evaluation plots\n",
    "plot_stacking_models(global_estimators, ST20_X_training, ST20_Y_training, 'ST20')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST20_Y_test_preds_df = predict_plot(ST20_stacking_regressor, ST20_X_test, ST20_X_validation, ST20_Y_validation, 'ST20')\n",
    "dump(ST20_stacking_regressor, filename=\"models/soil_temps/ST20_all.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST20_forecast_df = pd.concat([ST10_forecast_df, ST20_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 50cm from the predictions at 20cm and month ================================\n",
    "# Copy the ST20 predicted values as test data input for ST50\n",
    "# ST50_features = ['ST20', 'month']\n",
    "ST50_X_test = ST20_forecast_df.copy()\n",
    "ST50_X_test.index = ST20_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST50_stacking_regressor = initialize_stacking_regressor()\n",
    "ST50_X_test.to_excel('models/test_data/ST50_X_test_all.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST50 with ST20 as a feature for forecast testing ==============================\\n\")\n",
    "ST50_X_training, ST50_X_validation, ST50_Y_training, ST50_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','ST100','ST50','ID'], \n",
    "                                                                                           'ST50', \n",
    "                                                                                           ST50_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "# Make sure the columns in the X data are ordered\n",
    "ST50_X_columns = ST50_X_training.columns\n",
    "ST50_X_validation = ST50_X_validation[ST50_X_columns]\n",
    "ST50_X_test = ST50_X_test[ST50_X_columns]\n",
    "# The stacked model evaluation plots\n",
    "plot_stacking_models(global_estimators, ST50_X_training, ST50_Y_training, 'ST50')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST50_Y_test_preds_df = predict_plot(ST50_stacking_regressor, ST50_X_test, ST50_X_validation, ST50_Y_validation, 'ST50')\n",
    "dump(ST50_stacking_regressor, filename=\"models/soil_temps/ST50_all.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST50_forecast_df = pd.concat([ST20_forecast_df, ST50_Y_test_preds_df], axis=1)\n",
    "\n",
    "# ======================= Fit the soil temperature at 100cm from the predictions at 50cm and month ================================\n",
    "# Copy the ST50 predicted values as test data input for ST100\n",
    "# ST100_features = ['ST50', 'month']\n",
    "ST100_X_test = ST50_forecast_df.copy()\n",
    "ST100_X_test.index = ST50_forecast_df.index\n",
    "\n",
    "# Initialize the Stacking regressor\n",
    "global_estimators = []\n",
    "ST100_stacking_regressor = initialize_stacking_regressor()\n",
    "ST100_X_test.to_excel('models/test_data/ST100_X_test_all.xlsx')\n",
    "# Fit and predict the validation set\n",
    "print(\"================================= Stacking Regressor Metrics ST100 with ST50 as a feature for forecast testing ==============================\\n\")\n",
    "ST100_X_training, ST100_X_validation, ST100_Y_training, ST100_Y_validation = fit_model(dataset_denormalized_outlier_filtered, \n",
    "                                                                                           ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','evaporation_mm','ST100','ID'], \n",
    "                                                                                           'ST100', \n",
    "                                                                                           ST100_stacking_regressor)\n",
    "print('===========================================================================================\\n')\n",
    "\n",
    "# Make sure the columns in the X data are ordered\n",
    "ST100_X_columns = ST100_X_training.columns\n",
    "ST100_X_validation = ST100_X_validation[ST100_X_columns]\n",
    "ST100_X_test = ST100_X_test[ST100_X_columns]\n",
    "# The stacked model evaluation plots\n",
    "plot_stacking_models(global_estimators, ST100_X_training, ST100_Y_training, 'ST100')\n",
    "# Call the validation sets prediction function to plot the Observed and predicted values and get the test predicted values\n",
    "ST100_Y_test_preds_df = predict_plot(ST100_stacking_regressor, ST100_X_test, ST100_X_validation, ST100_Y_validation, 'ST100')\n",
    "dump(ST100_stacking_regressor, filename=\"models/soil_temps/ST100_all.joblib\");\n",
    "# Merge the previous DataFrame with the new predicted dataframe\n",
    "ST100_forecast_df = pd.concat([ST50_forecast_df, ST100_Y_test_preds_df], axis=1)\n",
    "\n",
    "# Merge the Original DataFrame with the predicted Series DataFrames\n",
    "ST_forecast_df = ST100_forecast_df.copy()\n",
    "\n",
    "# Save the merged DataFrame to an Excel file\n",
    "ST_forecast_df.to_excel(f'data/{formatted_date}-{formatted_time}_Soil_temperature_forecast_10_days.xlsx', index=True)\n",
    "ST_forecast_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc909d51-7b97-462b-96c0-09cfc4d86c00",
   "metadata": {},
   "source": [
    "### Apply GridSearchCV Hyperparameter tuning when required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd54990-30eb-4d3c-8757-c015df947649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300, 400],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10, 20],\n",
    "#     'min_samples_leaf': [1, 2, 4, 6],\n",
    "#     'max_features': [0.3, 1, 'sqrt', 'log2'],\n",
    "#     'bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "# # Perform Grid Search with cross-validation\n",
    "# gsc_model = GridSearchCV(RandomForestRegressor(n_jobs=-1, random_state=42), param_grid=param_grid, \n",
    "#                            cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# # Fit the Grid Search to the data\n",
    "# gsc_model.fit(ST2_X_training, ST2_Y_training)\n",
    "\n",
    "# # Get the best parameters and best score\n",
    "# best_params = gsc_model.best_params_\n",
    "# best_score = gsc_model.best_score_\n",
    "\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05bd595-98c2-4980-9fc4-dad00bb5de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_denormalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad33d96-28b8-4286-aa10-5ae9c96acc28",
   "metadata": {},
   "source": [
    "# 8. Final Optimized Prediction Using Stacking Regressor for for the different dataset cases for all soil depths.\n",
    "## A. Stacking Regressor for Soil temperature at 2cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05565f-583c-4536-9325-f58f103f8cdb",
   "metadata": {},
   "source": [
    "### Multicollinearity analysis using correlation and covariance matrices for ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22180d18-bc0d-4d95-b4d4-189306a03009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the covariance matrix for target ST100\n",
    "ST2_dataset_correlation = dataset_denormalized_outlier_filtered.drop(['ST100', 'ST50','ST20','ST10','ST5','ST2'], axis=1)\n",
    "ST2_covariance_matrix = ST2_dataset_correlation.cov()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "ST2_correlation_matrix = ST2_dataset_correlation.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST2_correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST2/ST2_denormalized_before_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Set the threshold\n",
    "threshold = 0.95\n",
    "# Find pairs of features with correlation above the threshold\n",
    "highly_correlated = np.where(np.abs(ST2_correlation_matrix) > threshold)\n",
    "highly_correlated_pairs = [(ST2_correlation_matrix.index[x], ST2_correlation_matrix.columns[y]) \n",
    "                           for x, y in zip(*highly_correlated) if x != y and x < y]\n",
    "\n",
    "print(\"Highly correlated pairs (above threshold):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)\n",
    "# Example: Removing one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for pair in highly_correlated_pairs:\n",
    "    features_to_remove.add(pair[1])  # You can choose to remove pair[0] or pair[1]\n",
    "\n",
    "# Drop the features from the dataframe\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated = ST2_dataset_correlation.drop(columns=features_to_remove)\n",
    "\n",
    "print(f\"Removed features: {features_to_remove}\")\n",
    "print(\"Shape of the reduced dataset:\", ST2_dataset_denormalized_outlier_filtered_uncorrelated.shape)\n",
    "\n",
    "# After removing the correlated features\n",
    "# Calculate the correlation matrix\n",
    "ST2_correlation_matrix_new = ST2_dataset_denormalized_outlier_filtered_uncorrelated.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST2_correlation_matrix_new, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST2/ST2_denormalized_after_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Assuming dataset_denormalized_outlier_filtered is your DataFrame\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST2_dataset_denormalized_outlier_filtered_uncorrelated.copy()\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = sm.add_constant(ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(data):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Iteratively remove features with VIF above the threshold\n",
    "def remove_high_vif_features(data, threshold=40.0):\n",
    "    while True:\n",
    "        vif_data = calculate_vif(data)\n",
    "        max_vif = vif_data['VIF'].max()\n",
    "        if max_vif > threshold:\n",
    "            # Identify the feature with the highest VIF\n",
    "            feature_to_remove = vif_data.sort_values('VIF', ascending=False)['feature'].iloc[0]\n",
    "            print(f\"Removing feature '{feature_to_remove}' with VIF: {max_vif}\")\n",
    "            data = data.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "    return data, vif_data\n",
    "\n",
    "# Remove high VIF features\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif, ST2_final_vif_data = remove_high_vif_features(ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "\n",
    "print(\"Final VIF data:\")\n",
    "print(ST2_final_vif_data)\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID'] = dataset_denormalized_outlier_filtered['ID']\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST2'] = dataset_denormalized_outlier_filtered['ST2']\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated['ID'] = dataset_denormalized_outlier_filtered['ID']\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated['ST2'] = dataset_denormalized_outlier_filtered['ST2']\n",
    "# Remove the constant term before creating the final DataFrame\n",
    "if 'const' in ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.columns:\n",
    "    ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(columns=['const'])\n",
    "\n",
    "# Store the 'ID' and 'ST2' columns with their corresponding index before PCA\n",
    "ID_index_mapping = ST2_dataset_denormalized_outlier_filtered_uncorrelated['ID']\n",
    "ST2_index_mapping = ST2_dataset_denormalized_outlier_filtered_uncorrelated['ST2']\n",
    "\n",
    "# Assume X is your feature dataframe\n",
    "ST2_X_pca = ST2_dataset_denormalized_outlier_filtered_uncorrelated.drop(['ST2', 'ID'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ST2_X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)  # Choose the number of components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "ST2_pca_df = pd.DataFrame(data=principal_components, columns=[f\"PC{i}\" for i in range(principal_components.shape[1])])\n",
    "\n",
    "# Merge PCA DataFrame with original DataFrame to maintain original index order\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ID_index_mapping, ST2_index_mapping, left_index=True, right_index=True)\n",
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca, ST2_pca_df, left_index=True, right_index=True)\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.savefig('data/results/ST2/ST2_PCA_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot the Box Plot of all features\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Number of features in the DataFrame\n",
    "num_features = dataset_denormalized_outlier_filtered.shape[1]\n",
    "# Calculate the number of rows needed to plot all features in 3 columns\n",
    "num_cols = 3\n",
    "num_rows = math.ceil(num_features / num_cols)\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "# Define colors for each column\n",
    "colors = ['green', 'purple', 'red']\n",
    "# Create a Box Plot for each feature\n",
    "for i, column in enumerate(dataset_denormalized_outlier_filtered.columns):\n",
    "    col_index = i % num_cols  # Determine the column index (0, 1, or 2)\n",
    "    sns.boxplot(data=dataset_denormalized_outlier_filtered[column], ax=axes[i], color=colors[col_index])\n",
    "    axes[i].set_title(f'Box Plot for {column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Values', fontsize=14)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[i].tick_params(axis='both', which='minor', labelsize=12)\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST2/Box_plot_of_features.png')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39afc2-2863-486c-b9d0-ed5fd0a3cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST2_dataset_denormalized_outlier_filtered_uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233c1dd-108d-474a-84a5-b833d8604113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normalized_outlier_filtered['snow_depth_cm'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223326e-92bf-4398-84f2-2fd54f3c9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac972b-1c3d-4ee9-8759-41fe0e1232ee",
   "metadata": {},
   "source": [
    "### Option 1:  ST2 Prediction by varying the dataset cases\n",
    "#### Note: Choose the dataset case at this line of the code: dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1)\n",
    "#### Dataset Cases:\n",
    "##### Case 1. dataset_denormalized_outlier_filtered\n",
    "##### case 2. ST2_clean_dataset_denormalized\n",
    "##### case 3. ST2_dataset_denormalized_outlier_filtered_uncorrelated\n",
    "##### case 4. ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif\n",
    "##### case 5. ST2_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4f1df-6c11-4131-9ec0-da0b8b4ff058",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR  \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Choose any of the time-independent dataset cases\n",
    "# Select the dataset case here\n",
    "dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1) # Choose dataset case here\n",
    "  \n",
    "# Create function to evaluate model on few different levels\n",
    "def show_scores(model, X_train, X_valid, X_test, Y_train, Y_valid, Y_test, std, target='ST2', model_name='RF'):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    test_preds = model.predict(X_test)\n",
    "    scores = {\n",
    "              # \"Validation Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_test, test_preds),   \n",
    "              # \"Validation Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds), \n",
    "              # \"Validation Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set RMSE\": root_mean_squared_error(Y_valid, val_preds),\n",
    "              # \"Test Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Test Set R^2 Score\":r2_score(Y_valid, val_preds),  \n",
    "              # \"Test Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Test Set MAE\": mean_absolute_error(Y_test, test_preds), \n",
    "              # \"Test Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Tes Set RMSE\": root_mean_squared_error(Y_test, test_preds),\n",
    "              # \"Validation Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),             \n",
    "              # \"Validation Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['Metric', 'Value'])    \n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_excel(f'data/results/{target}/{model_name}_scores.xlsx', index=False)\n",
    "    return scores\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_plot(model, ST_X_train, ST_Y_train, ST_X_test, ST_Y_test, ST_X_validation, ST_Y_validation, name, std):\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_train_preds = model.predict(ST_X_train)\n",
    "    # Change train predictions to pandas series\n",
    "    ST_Y_train_preds_series = pd.Series(ST_Y_train_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_train_preds_series.index = ST_Y_train.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_train_sorted = ST_Y_train.sort_values().reset_index(drop=True)\n",
    "    ST_Y_train_preds_sorted = ST_Y_train_preds_series[ST_Y_train.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_train_mae = mean_absolute_error(ST_Y_train, ST_Y_train_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_train_rmse = root_mean_squared_error(ST_Y_train,ST_Y_train_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_train_r2_score = r2_score(ST_Y_train,ST_Y_train_preds)\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "    # Change validation predictions to pandas series\n",
    "    ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "    ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_valid_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_valid_rmse = root_mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_valid_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "\n",
    "    # Predict the test set which is forecast data\n",
    "    ST_Y_test_preds = model.predict(ST_X_test)\n",
    "    # Changes the predicted array values to pandas series\n",
    "    ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "    ST_Y_test_preds_series.index =ST_Y_test.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_test_sorted = ST_Y_test.sort_values().reset_index(drop=True)\n",
    "    ST_Y_test_preds_sorted = ST_Y_test_preds_series[ST_Y_test.index].sort_values().reset_index(drop=True)\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    ST_test_mae = mean_absolute_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate mean squared error\n",
    "    ST_test_rmse = root_mean_squared_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_test_r2_score = r2_score(ST_Y_test,ST_Y_test_preds)\n",
    "    \n",
    "    # Convert the Series to a DataFrame to return as dataframe\n",
    "    ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "    ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "\n",
    "\n",
    "     # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_train_sorted.index,ST_Y_train_sorted, color='blue', label=f'{name} Training Observed Values')\n",
    "    plt.plot(ST_Y_train_preds_sorted.index,ST_Y_train_preds_sorted, color='red', label=f'{name} Training Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_train_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_train_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_train_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 2 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Training Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_train_set_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Validation Set Observed Values')\n",
    "    plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Validation Set Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_valid_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_valid_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_valid_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 2 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Validation Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_valid_set_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the test sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_test_sorted.index,ST_Y_test_sorted, color='blue', label=f'{name} Test Observed Values')\n",
    "    plt.plot(ST_Y_test_preds_sorted.index,ST_Y_test_preds_sorted, color='red', label=f'{name} Test Predicted Values')\n",
    "    # Display the metrics as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_test_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_test_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_test_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 2 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Final Test Scores For {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_test_set_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()    \n",
    "    return ST_Y_test_preds_df\n",
    "    \n",
    "std_deviation = dataset_denormalized_outlier_filtered['ST2'].std()\n",
    "# Split the dataset into features and target\n",
    "many_features_dropped = ['ST2','ST5','ST10','ST20','ST50','ST100','ID']\n",
    "soil_features_dropped = ['ST2','ST5','ST10','ST20','ST50','ST100','ID']\n",
    "uncorrelated_dropped = ['ST2','ID']\n",
    "# ST2_X = dataset_shuffled.drop(many_features_dropped, axis=1)\n",
    "ST2_X = dataset_shuffled.drop(soil_features_dropped, axis=1)\n",
    "ST2_Y = dataset_shuffled['ST2']\n",
    "\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_2cm ).\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(dataset_shuffled))\n",
    "ST2_X_train, ST2_Y_train = ST2_X[:train_split], ST2_Y[:train_split]\n",
    "ST2_X_valid, ST2_Y_valid =ST2_X[train_split:valid_split], ST2_Y[train_split:valid_split]\n",
    "ST2_X_test, ST2_Y_test = ST2_X[valid_split:], ST2_Y[valid_split:]\n",
    "\n",
    "# A. CatBoostRegressor (CB)\n",
    "# Create CB model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_cb_model_stack = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "# Fit the model for ST2 to start with\n",
    "ST2_cb_model_stack.fit(ST2_X_train, ST2_Y_train, eval_set=(ST2_X_valid, ST2_Y_valid), early_stopping_rounds=100)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================CatBoost The Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST2_cb_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'CB'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# B. RandomForestRegressor\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_rf_model_stack = RandomForestRegressor(n_estimators=500, \n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         max_features=None,\n",
    "                                         max_depth=20,\n",
    "                                         bootstrap=False,\n",
    "                                         random_state=42)\n",
    "\n",
    "# Fit the model for ST2 to start with\n",
    "ST2_rf_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================Random Forest The Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST2_rf_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'RF'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# C. Histogram Based Gradient Boosting Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_gbr_model_stack = HistGradientBoostingRegressor(l2_regularization=1,  \n",
    "                                                  learning_rate=0.05, \n",
    "                                                  max_iter=500, \n",
    "                                                  max_depth=20,\n",
    "                                                  max_leaf_nodes=50,\n",
    "                                                  min_samples_leaf=20,\n",
    "                                                  random_state=42)\n",
    "# ST2_gbr_model_stack = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "#                                               max_iter=300, \n",
    "#                                               max_leaf_nodes=41,\n",
    "#                                               random_state=42)\n",
    "# Fit the ST2 model for soil temp at 100 cm\n",
    "ST2_gbr_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Histogram-Based Gradient Boosting Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_gbr_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'HGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# D. XGBoost Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create XGBoost for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_xgb_model_stack = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "# Fit the ST2 model for soil temp at 100 cm\n",
    "ST2_xgb_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The XGBoost Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_xgb_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'XGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# E. AdaBoostRegressor \n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create AdaBoost Regressor for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_adb_model_stack = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                        loss='linear',\n",
    "                                        n_estimators=100,\n",
    "                                        random_state=42)\n",
    "# Fit the ST2 model for soil temp at 100 cm\n",
    "ST2_adb_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The AdaBoost Regressor Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_adb_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'ADB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# F. Ridge Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_rg_model_stack = Ridge(random_state=42)\n",
    "# Fit the ST2 model for soil temp at 100 cm\n",
    "ST2_rg_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Ridge Regressor Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_rg_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'RR'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# G. Lasso Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create Lasso model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_la_model_stack = Lasso(random_state=42)\n",
    "# Fit the ST2 model for soil temp at 100cm\n",
    "ST2_la_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Lasso Regressor Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_la_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'LA'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# H. ElasticNet Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_en_model_stack = ElasticNet(random_state=42)\n",
    "# Fit the ST2 model for soil temp at 100cm\n",
    "ST2_en_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The ElasticNet Regressor Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_en_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'EN'))\n",
    "print(\"=========================================================================================================\\n\")\n",
    "\n",
    "# I. SVR-L Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-L model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_svrl_model_stack = SVR(kernel='linear')\n",
    "# Fit the ST2 model for soil temp at 100cm\n",
    "ST2_svrl_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with linear model Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_svrl_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'SVR-L'))\n",
    "print(\"==========================================================================================================\\n\")\n",
    "\n",
    "# J. SVR-R Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-R model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST2_svrr_model_stack = SVR(kernel='rbf')\n",
    "# Fit the ST2 model for soil temp at 100cm\n",
    "ST2_svrr_model_stack.fit(ST2_X_train, ST2_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with rfb model Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_svrr_model_stack, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'SVR-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# Stack of predictors on a single data set\n",
    "ST2_rf_regressor = RandomForestRegressor(n_estimators=500, \n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         max_features='sqrt',\n",
    "                                         max_depth=20,\n",
    "                                         bootstrap=False,\n",
    "                                         random_state=42)\n",
    "ST2_gbdt_regresssor = HistGradientBoostingRegressor(l2_regularization=1,  \n",
    "                                                  learning_rate=0.05, \n",
    "                                                  max_iter=500, \n",
    "                                                  max_depth=20,\n",
    "                                                  max_leaf_nodes=50,\n",
    "                                                  min_samples_leaf=20,\n",
    "                                                  random_state=42)\n",
    "ST2_xgb_model = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "ST2_cb_regressor = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "ST2_adb_regressor = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    (\"RandomForest\", ST2_rf_regressor),\n",
    "    (\"CatBoost\", ST2_cb_regressor),\n",
    "    (\"HistGradientBoosting\", ST2_gbdt_regresssor),\n",
    "    (\"XGBoost\", ST2_xgb_model)\n",
    "]\n",
    "ST2_stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\n",
    "\n",
    "# Measure and plot the results\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for ax, (name, est) in zip(axs, estimators + [(\"Stacking Regressor\", ST2_stacking_regressor)]):\n",
    "    scorers = {\"R^2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\", \"RMSE\": \"neg_root_mean_squared_error\"}\n",
    "\n",
    "    start_time = time.time()\n",
    "    scores = cross_validate(est, ST2_X_train, ST2_Y_train, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_pred = cross_val_predict(est, ST2_X_valid, ST2_Y_valid, n_jobs=-1, verbose=0)\n",
    "    y_test = cross_val_predict(est, ST2_X_test, ST2_Y_test, n_jobs=-1, verbose=0)\n",
    "    \n",
    "    # Calculate mean and std_deviation for each scorer\n",
    "    scores_mean_std = {\n",
    "        key: (np.abs(np.mean(scores[f'test_{value}'])), np.std(scores[f'test_{value}']))\n",
    "        for key, value in scorers.items()\n",
    "    }\n",
    "\n",
    "    # Format the scores\n",
    "    formatted_scores = {\n",
    "        key: f\"{mean:.4f} ± {std_dev:.4f}\"\n",
    "        for key, (mean, std_dev) in scores_mean_std.items()\n",
    "    }\n",
    "\n",
    "    display = PredictionErrorDisplay.from_predictions(\n",
    "        y_true=ST2_Y_valid,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        ax=ax,\n",
    "        scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "        line_kwargs={\"color\": \"tab:red\"},\n",
    "    )\n",
    "    ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.4f} seconds\", fontsize=14)\n",
    "    # Set custom x-label and y-label\n",
    "    ax.set_xlabel(\"Predicted Soil Temperature at 2 cm (°C)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Observed Soil Temperature at 2 cm (°C)\", fontsize=14)\n",
    "\n",
    "    for metric_name, (mean, std_dev) in scores_mean_std.items():\n",
    "        if metric_name == 'R^2':\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {formatted_scores[metric_name]}\")\n",
    "        else:\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {mean:.4f} ± {std_dev:.4f}\")\n",
    "    \n",
    "    ax.legend(loc=\"best\", fontsize='small')\n",
    "    # Save the mean and std scores to an Excel file\n",
    "    df_scores_summary = pd.DataFrame(scores_mean_std).T\n",
    "    df_scores_summary.columns = ['Train Mean', 'Train Std Dev']\n",
    "    df_scores_summary.to_excel(f'data/results/ST2/{name}_cv_scores.xlsx', index=True)\n",
    "# Hide any unused subplots\n",
    "for i in range(len(estimators)+1, len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "# Apply tight layout\n",
    "plt.tight_layout()\n",
    "# Save the entire figure with all subplots to a file\n",
    "fig.savefig('data/results/ST2/stacked_regressors_prediction_error_plots.png', bbox_inches='tight')\n",
    "\n",
    "# Sort actual values and get sorted indices\n",
    "ST2_Y_valid_sorted = ST2_Y_valid.sort_values()\n",
    "sorted_indices = ST2_Y_valid_sorted.index\n",
    "\n",
    "# Reorder y_pred using the sorted indices\n",
    "y_pred_sorted = pd.Series(y_pred, index=ST2_Y_valid.index).loc[sorted_indices]\n",
    "\n",
    "# Calculate metrics for the validation set predictions\n",
    "mae_valid = mean_absolute_error(ST2_Y_valid, y_pred)\n",
    "rmse_valid = np.sqrt(mean_squared_error(ST2_Y_valid, y_pred))\n",
    "r2_valid = r2_score(ST2_Y_valid, y_pred)\n",
    "\n",
    "# Calculate metrics for the test set predictions\n",
    "mae_test = mean_absolute_error(ST2_Y_test, y_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(ST2_Y_test, y_test))\n",
    "r2_test = r2_score(ST2_Y_test, y_test)\n",
    "\n",
    "# Save the validation metrics to an Excel file\n",
    "validation_test_metrics = {\n",
    "    'V_R^2': [r2_valid],\n",
    "    'V_MAE': [mae_valid],\n",
    "    'V_RMSE': [rmse_valid],\n",
    "    'T_R^2': [r2_test],\n",
    "    'T_MAE': [mae_test],\n",
    "    'T_RMSE': [rmse_test]    \n",
    "}\n",
    "# Save the validation metrics to an Excel file\n",
    "\n",
    "df_metrics = pd.DataFrame(validation_test_metrics)\n",
    "df_metrics.to_excel(f'data/results/ST2/{name}_validation_test_metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the sorted actual values and corresponding predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ST2_Y_valid_sorted.values, color='blue', label='Observed Values')\n",
    "plt.plot(y_pred_sorted.values, color='red', label='Predicted Values')\n",
    "\n",
    "# Display the metrics as text annotation\n",
    "plt.text(0.1, 0.75, f'MAE: {mae_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.3, 0.75, f'RMSE: {rmse_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.5, 0.75, f'R^2: {r2_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel('Index', fontsize=14)\n",
    "plt.ylabel('Soil Temperature at 2 cm (°C)', fontsize=14)\n",
    "plt.title(f'STACK-R model\\'s validation set\\'s predicted vs observed values for ST2', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'data/results/ST2/{name}_cross_validation_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "dump(ST2_stacking_regressor, filename=\"data/results/ST2/ST2_STACK-R_trained_model.joblib\");\n",
    "# Fit the stacking regressor for direct fitting and prediction for all sets at default CV=5\n",
    "ST2_stacking_regressor.fit(ST2_X_train, ST2_Y_train)\n",
    "print(\"====================The Stacking Regressor Evaluation Metrics Results For ST2 Denormalized =======================\\n\")\n",
    "print(show_scores(ST2_stacking_regressor, ST2_X_train, ST2_X_valid, ST2_X_test, ST2_Y_train, ST2_Y_valid, ST2_Y_test, std_deviation,'ST2', 'STACK-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# ST2_Y_test_preds_df = predict_plot(ST2_stacking_regressor, ST2_X_train, ST2_Y_train, ST2_X_test, ST2_Y_test, ST2_X_valid, ST2_Y_valid, 'ST2', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3a2cb-9122-496e-9acf-0454efeb40b8",
   "metadata": {},
   "source": [
    "### Cross-validation stability check of the stacking regressor for ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0248c85-aa51-4a5c-b89e-f54559bf4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "ST2_cv_scores = cross_val_score(ST2_stacking_regressor, ST2_X_train, ST2_Y_train, cv=kf, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive\n",
    "ST2_cv_scores = -ST2_cv_scores\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-Validation Scores (MSE):\", ST2_cv_scores)\n",
    "print(\"Mean CV Score (MSE):\", np.mean(ST2_cv_scores))\n",
    "print(\"Standard Deviation of CV Scores:\", np.std(ST2_cv_scores))\n",
    "# Save the scores to an Excel file\n",
    "ST2_cv_scores_df = pd.DataFrame(ST2_cv_scores, columns=['MSE'])\n",
    "ST2_cv_scores_df.to_excel('data/results/ST2/ST2_10_fold_cv_scores.xlsx', index=False)\n",
    "\n",
    "##=========== Visualize the problematic Fold using histogram==================\n",
    "# Calculate mean MSE\n",
    "ST2_mean_mse = np.mean(ST2_cv_scores)\n",
    "# Identify the problematic fold\n",
    "ST2_problematic_fold_index = np.argmax(np.abs(ST2_cv_scores - ST2_mean_mse))\n",
    "# Get the indices of the data points in the problematic fold\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(ST2_X_train)):\n",
    "    if fold_index == ST2_problematic_fold_index:\n",
    "        problematic_fold_train_indices = train_index\n",
    "        problematic_fold_test_indices = test_index\n",
    "\n",
    "# Subset the data for the problematic fold\n",
    "X_problematic_fold = ST2_X_train.iloc[problematic_fold_test_indices]\n",
    "y_problematic_fold = ST2_Y_train.iloc[problematic_fold_test_indices]\n",
    "\n",
    "# Visualize or analyze features for the problematic fold\n",
    "for feature in ST2_X_train.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Histogram for the problematic fold\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_problematic_fold[feature], kde=True, bins=20, color='red')\n",
    "    plt.title(f'{feature} - Problematic Fold', fontsize=14)\n",
    "    plt.xlabel(feature, fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    # Histogram for the entire dataset\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(ST2_X_train[feature], kde=True, bins=20, color='blue')\n",
    "    plt.title(f'{feature} - Entire Dataset', fontsize=14)\n",
    "    plt.xlabel(feature, fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/results/ST2/ST2_CV_problematic_10_fold_vs_main_dataset_histograms.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074f979-92de-432c-9ca2-519c0deea0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST2_clean_dataset_denormalized['ST2'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92826a-79a7-4280-a90d-de75b4d2cfd2",
   "metadata": {},
   "source": [
    "### Partial Dependence, Individual Conditional Expectation and Residual Analysis Plots for ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6206d85-8395-4b0d-a983-05f7805dbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import statsmodels.api as sm\n",
    "from pycebox.ice import ice, ice_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# # Partial Dependence Plot\n",
    "# print('====================================================== ST2 Partial Dependence Plot')\n",
    "# ST2_feature_names = ST2_X_train.columns.tolist()\n",
    "# n_features = len(ST2_feature_names)\n",
    "# n_cols = 2\n",
    "# n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "# fig1, ax1 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat = ax1.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST2_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST2_stacking_regressor, ST2_X_train, features=[feature])\n",
    "#     display.plot(ax=axes_flat[idx])\n",
    "#     axes_flat[idx].set_title(f'Partial Dependence (PD) Plot for {feature}')\n",
    "#     axes_flat[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat)):\n",
    "#     fig1.delaxes(axes_flat[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "# plt.suptitle('Partial Dependence (PD) Plot', fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# # plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST2/ST2_final_partial_dependence_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Individual Conditional Expectation Plot (ICE)\n",
    "# print('======================== ST2 Individual Conditional Expectation Plot ===============================')\n",
    "# fig2, axes2 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat2 = axes2.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST2_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST2_stacking_regressor, ST2_X_train, features=[feature], kind='individual')\n",
    "#     display.plot(ax=axes_flat2[idx])\n",
    "#     axes_flat2[idx].set_title(f'Individual Conditional Expectation (ICE) Plot for {feature}')\n",
    "#     axes_flat2[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat2)):\n",
    "#     fig2.delaxes(axes_flat2[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.7)\n",
    "# plt.suptitle('Individual Conditional Expectation (ICE) Plot', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST2/ST2_final_individual_conditional_expectation_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# Residual Analysis\n",
    "print('========================= ST2 Residual Analysis Plot ==============================')\n",
    "ST2_Y_predictions = ST2_stacking_regressor.predict(ST2_X_test)\n",
    "ST2_residuals = ST2_Y_test - ST2_Y_predictions\n",
    "\n",
    "# Calculate the interquartile range (IQR)\n",
    "Q1 = np.percentile(ST2_residuals, 25)\n",
    "Q3 = np.percentile(ST2_residuals, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the whisker range\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Count outliers\n",
    "outliers = np.sum((ST2_residuals < lower_bound) | (ST2_residuals > upper_bound))\n",
    "total_residuals = len(ST2_residuals)\n",
    "outlier_percentage = (outliers / total_residuals) * 100\n",
    "\n",
    "# Plotting the residuals scatter plot and box-and-whisker plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Residuals analysis plot\n",
    "ax[0].scatter(ST2_Y_predictions, ST2_residuals)\n",
    "ax[0].set_xlabel('Predictions', fontsize=14)\n",
    "ax[0].set_ylabel('Residuals', fontsize=14)\n",
    "ax[0].set_title('ST2 Residuals Analysis Plot', fontsize=14)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax[0].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# Box-and-whisker plot for residuals\n",
    "sns.boxplot(y=ST2_residuals, ax=ax[1])\n",
    "ax[1].set_title('ST2 Box-and-Whisker Plot of Residuals', fontsize=14)\n",
    "ax[1].set_ylabel('ST2 Residuals', fontsize=14)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# Annotate the plot with the number of outliers and total residuals\n",
    "annotation_text = (f'Total Residuals: {total_residuals}\\n'\n",
    "                   f'Number of Outliers: {outliers}\\n'\n",
    "                   f'Percentage of Outliers: {outlier_percentage:.2f}%')\n",
    "ax[1].annotate(annotation_text, xy=(0.8, 0.87), xycoords='axes fraction',\n",
    "               fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST2/ST2_final_residual_and_boxplot_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predictor Variables\n",
    "print('========================= Residuals vs. Predictor Variables ==============================')\n",
    "for column in ST2_X_test.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.scatter(ST2_X_test[column], ST2_residuals)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel(column, fontsize=14)\n",
    "    ax.set_ylabel('Residuals', fontsize=14)\n",
    "    ax.set_title(f'Residuals vs. {column}', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/results/ST2_final_residuals_vs_{column}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the Box Plot of all features\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Number of features in the DataFrame\n",
    "num_features = dataset_denormalized_outlier_filtered.shape[1]\n",
    "# Calculate the number of rows needed to plot all features in 3 columns\n",
    "num_cols = 3\n",
    "num_rows = math.ceil(num_features / num_cols)\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "# Define colors for each column\n",
    "colors = ['green', 'purple', 'red']\n",
    "# Create a Box Plot for each feature\n",
    "for i, column in enumerate(dataset_denormalized_outlier_filtered.columns):\n",
    "    col_index = i % num_cols  # Determine the column index (0, 1, or 2)\n",
    "    sns.boxplot(data=dataset_denormalized_outlier_filtered[column], ax=axes[i], color=colors[col_index])\n",
    "    axes[i].set_title(f'Box Plot for {column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Values', fontsize=14)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[i].tick_params(axis='both', which='minor', labelsize=12)\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST2/ST2_Box_plot_of_features.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# # Q-Q Plot\n",
    "# print('========================= ST2 Q-Q Plot ==============================')\n",
    "# fig5, ax5 = plt.subplots(figsize=(10, 7))\n",
    "# sm.qqplot(ST2_residuals, line='45', ax=ax5)\n",
    "# ax5.set_title('Q-Q Plot of Residuals')\n",
    "# plt.savefig('data/results/ST2/ST2_final_Q-Q_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Histogram of residuals\n",
    "# fig6, ax6 = plt.subplots(figsize=(10, 7))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(residuals, kde=True, ax=ax6)\n",
    "# plt.xlabel('Residuals')\n",
    "# plt.title('Histogram of Residuals')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f67b0-0587-4cb6-889e-6f90b763ca42",
   "metadata": {},
   "source": [
    "### Feature importances analysis for ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ed1f5-2718-499c-8171-b9e7391ec4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# # Assuming feature_names is a list of your feature names\n",
    "# feature_names = ST2_X_train.columns.tolist()\n",
    "\n",
    "# estimators = [\n",
    "#     (\"Random Forest\", ST2_rf_model_stack),\n",
    "#     (\"Cat Boost\", ST2_cb_model_stack),\n",
    "#     (\"XG Boost\", ST2_xgb_model_stack)\n",
    "# ]\n",
    "\n",
    "# # Initialize an array to store aggregated feature importances\n",
    "# num_features = ST2_X_train.shape[1]\n",
    "# feature_importances = np.zeros(num_features, dtype=np.float64)\n",
    "\n",
    "# # Aggregate feature importances from base models\n",
    "# count = 0\n",
    "# for name, model in estimators:\n",
    "#     if hasattr(model, 'feature_importances_'):\n",
    "#         importances = np.array(model.feature_importances_, dtype=np.float64)\n",
    "#         feature_importances += importances\n",
    "#         count += 1\n",
    "#     else:\n",
    "#         print(f\"{name} does not have feature_importances_ attribute\")\n",
    "\n",
    "# # Normalize the aggregated feature importances\n",
    "# if count > 0:\n",
    "#     feature_importances /= count\n",
    "\n",
    "# # Convert feature importances to percentages\n",
    "# feature_importances_percentage = 100 * (feature_importances / feature_importances.sum())\n",
    "\n",
    "# # Sort the feature importances in descending order\n",
    "# sorted_indices = np.argsort(feature_importances_percentage)[::-1]\n",
    "# sorted_feature_importances_percentage = feature_importances_percentage[sorted_indices]\n",
    "# sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# # Plot the sorted feature importances\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# bars = plt.barh(sorted_feature_names, sorted_feature_importances_percentage)\n",
    "# plt.xlabel(\"Aggregated feature importance (%)\")\n",
    "# plt.ylabel(\"Feature Names\")\n",
    "# plt.title(\"Soil Temp (2cm) aggregated feature importances from base estimators\")\n",
    "# plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "\n",
    "# # Add annotations to the bars\n",
    "# for bar in bars:\n",
    "#     width = bar.get_width()\n",
    "#     plt.text(\n",
    "#         width + 0.5,  # Offset slightly to the right of the bar\n",
    "#         bar.get_y() + bar.get_height() / 2,  # Vertical center of the bar\n",
    "#         f'{width:.2f}%',  # Display the width (importance value) formatted to 2 decimal places\n",
    "#         va='center'\n",
    "#     )\n",
    "# # Save the figure\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('data/results/ST2/ST2_stacking_regressors_feature_importances.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# plot_features(ST2_X_train.columns, ST2_rf_model_stack.feature_importances_, 'data/results/ST2_RF_feature_analysis.png', 'ST2 RF Feature Importance Plot')\n",
    "# plot_features(ST2_X_train.columns, ST2_cb_model_stack.feature_importances_/100, 'data/results/ST2_CB_feature_analysis.png', 'ST2 CB Feature Importance Plot')\n",
    "# plot_features(ST2_X_train.columns, ST2_xgb_model_stack.feature_importances_, 'data/results/ST2_XGB_feature_analysis.png', 'ST2 XGB Feature Importance Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e562848-15e7-4492-8ba6-6a6a47aebabc",
   "metadata": {},
   "source": [
    "### Feature importance analysis for ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72141f8a-ee96-4e91-8bb4-fc5deed410d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ST2_X_train and ST2_Y_train are your training data\n",
    "\n",
    "# Fit the stacking regressor\n",
    "ST2_stacking_regressor.fit(ST2_X_train, ST2_Y_train)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = ST2_X_train.columns\n",
    "\n",
    "# Initialize an array to store feature importances\n",
    "feature_importances = np.zeros(ST2_X_train.shape[1])\n",
    "\n",
    "# Function to extract feature importances\n",
    "def get_feature_importance(model, X, y):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return np.abs(model.coef_)\n",
    "    elif isinstance(model, CatBoostRegressor):\n",
    "        return model.get_feature_importance()\n",
    "    else:\n",
    "        # Use permutation importance as a fallback for models without direct attribute\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        return result.importances_mean\n",
    "\n",
    "# Aggregate feature importances\n",
    "for name, model in ST2_stacking_regressor.named_estimators_.items():\n",
    "    importances = get_feature_importance(model, ST2_X_train, ST2_Y_train)\n",
    "    feature_importances += importances\n",
    "\n",
    "# Normalize the aggregated feature importances\n",
    "feature_importances /= len(ST2_stacking_regressor.named_estimators_)\n",
    "\n",
    "# Convert importances to percentage\n",
    "feature_importances_percentage = 100 * (feature_importances / np.sum(feature_importances))\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances_percentage\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('ST2 Stacking Regressor Feature Importances')\n",
    "\n",
    "# Add annotations\n",
    "for index, value in enumerate(importance_df['Importance']):\n",
    "    plt.text(value, index, f'{value:.2f}%', va='center')\n",
    "\n",
    "plt.savefig('data/results/ST2/ST2_stacking_regressor_feature_importances.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b832b0e-fca1-4b51-aead-67a2c2ca39b3",
   "metadata": {},
   "source": [
    "### Learning Curves evaluation using the validation and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53b3c7-d0e7-4137-af3c-6a5f0d234ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def plot_learning_curves(model, X_train, Y_train, X_valid, Y_valid, feature):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MAE to positive\n",
    "    train_errors_mae = -train_scores.mean(axis=1)\n",
    "    val_errors_mae = -valid_scores.mean(axis=1)\n",
    "    \n",
    "    train_sizes_mse, train_scores_mse, valid_scores_mse = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MSE to positive\n",
    "    train_errors_mse = -train_scores_mse.mean(axis=1)\n",
    "    val_errors_mse = -valid_scores_mse.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot MAE learning curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_sizes, train_errors_mae, \"r-\", label=\"Training MAE\")\n",
    "    plt.plot(train_sizes, val_errors_mae, \"b-\", label=\"Validation MAE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"MAE Learning Curve\")\n",
    "    \n",
    "    # Plot MSE learning curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_sizes, train_errors_mse, \"r-\", label=\"Training RMSE\")\n",
    "    plt.plot(train_sizes, val_errors_mse, \"b-\", label=\"Validation RMSE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE Learning Curve\")    \n",
    "    plt.savefig('data/results/ST2/ST2_learning_curves.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example call (ensure your data and model are defined correctly)\n",
    "plot_learning_curves(ST2_stacking_regressor, ST2_X_train, ST2_Y_train, ST2_X_valid, ST2_Y_valid, \"ST2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3fe922-dab7-4e8a-ba60-d0bd4b0181dd",
   "metadata": {},
   "source": [
    "### GridSearhCV Evaluation for all models used in the stacked regressor for ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d231cf-0131-4d60-91f0-dd5e976ef030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_cb = {\n",
    "    'iterations': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'depth': [4, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 50, 100]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    'max_leaf_nodes': [31, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [20, 50, 100],\n",
    "    'l2_regularization': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [1.0, 0.8, 0.6],\n",
    "    'colsample_bytree': [1.0, 0.8, 0.6],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 0.1, 0.01],\n",
    "    'tree_method': ['gpu_hist']  # Use GPU\n",
    "}\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# stacking_param_grid ={\n",
    "#     'rf__n_estimators': [100, 300, 500],\n",
    "#     'rf__max_depth': [None, 10, 20, 30],\n",
    "#     'hgb__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'hgb__max_iter': [100, 200, 500],\n",
    "#     'catboost__iterations': [100, 200, 500],\n",
    "#     'catboost__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'catboost__depth': [4, 6, 10],\n",
    "#     'xgb__n_estimators': [100, 200],\n",
    "#     'xgb__max_depth': [3, 5]\n",
    "# }\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "ada = AdaBoostRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestRegressor()),\n",
    "#     ('cb', CatBoostRegressor()),\n",
    "#     ('hgb', HistGradientBoostingRegressor()),    \n",
    "#     ('xgb', XGBRegressor())\n",
    "# ]\n",
    "\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=RidgeCV()\n",
    "# )\n",
    "\n",
    "# Initialize GridSearchCV for RF model\n",
    "start_time_rf = time.time()\n",
    "ST2_grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_rf_search = time.time()\n",
    "ST2_grid_search_rf.fit(ST2_X_train, ST2_Y_train)\n",
    "end_time_rf_fit = time.time()\n",
    "elapsed_time_search_rf = end_time_rf_search - start_time_rf\n",
    "elapsed_time_fit_rf = end_time_rf_fit - end_time_rf_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for RandomForestRegressor:\", ST2_grid_search_rf.best_params_)\n",
    "print(\"Best score for RandomForestRegressor:\", -ST2_grid_search_rf.best_score_)\n",
    "print(\"RandomForestRegressor GridSearchCV Time:\", elapsed_time_search_rf)\n",
    "print(\"RandomForestRegressor Fitting Time:\", elapsed_time_fit_rf)\n",
    "\n",
    "# Initialize GridSearchCV for HGB model\n",
    "start_time_hgb = time.time()\n",
    "ST2_grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=param_grid_hgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_hgb_search = time.time()\n",
    "ST2_grid_search_hgb.fit(ST2_X_train, ST2_Y_train)\n",
    "end_time_hgb_fit = time.time()\n",
    "elapsed_time_search_hgb = end_time_hgb_search - start_time_hgb\n",
    "elapsed_time_fit_hgb = end_time_hgb_fit - end_time_hgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for HistGradientBoostingRegressor:\", ST2_grid_search_hgb.best_params_)\n",
    "print(\"Best score for HistGradientBoostingRegressor:\", -ST2_grid_search_hgb.best_score_)\n",
    "print(\"HistGradientBoostingRegressor GridSearchCV Time:\", elapsed_time_search_hgb)\n",
    "print(\"HistGradientBoostingRegressor Fitting Time:\", elapsed_time_fit_hgb)\n",
    "\n",
    "# Initialize GridSearchCV for XGB model\n",
    "start_time_xgb = time.time()\n",
    "ST2_grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_xgb_search = time.time()\n",
    "ST2_grid_search_xgb.fit(ST2_X_train, ST2_Y_train)\n",
    "end_time_xgb_fit = time.time()\n",
    "elapsed_time_search_xgb = end_time_xgb_search - start_time_xgb\n",
    "elapsed_time_fit_xgb = end_time_xgb_fit - end_time_xgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for XGBRegressor:\", ST2_grid_search_xgb.best_params_)\n",
    "print(\"Best score for XGBRegressor:\", -ST2_grid_search_xgb.best_score_)\n",
    "print(\"XGBRegressor GridSearchCV Time:\", elapsed_time_search_xgb)\n",
    "print(\"XGBRegressor Fitting Time:\", elapsed_time_fit_xgb)\n",
    "\n",
    "# Initialize GridSearchCV for ADA model\n",
    "start_time_ada = time.time()\n",
    "ST2_grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_ada_search = time.time()\n",
    "ST2_grid_search_ada.fit(ST2_X_train, ST2_Y_train)\n",
    "end_time_ada_fit = time.time()\n",
    "elapsed_time_search_ada = end_time_ada_search - start_time_ada\n",
    "elapsed_time_fit_ada = end_time_ada_fit - end_time_ada_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for AdaBoostRegressor:\", ST2_grid_search_ada.best_params_)\n",
    "print(\"Best score for AdaBoostRegressor:\", -ST2_grid_search_ada.best_score_)\n",
    "print(\"AdaBoostRegressor GridSearchCV Time:\", elapsed_time_search_ada)\n",
    "print(\"AdaBoostRegressor Fitting Time:\", elapsed_time_fit_ada)\n",
    "\n",
    "# Initialize GridSearchCV for CB model\n",
    "start_time_cb = time.time()\n",
    "ST2_grid_search_cb = GridSearchCV(estimator=cb, param_grid=param_grid_cb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_cb_search = time.time()\n",
    "ST2_grid_search_cb.fit(ST2_X_train, ST2_Y_train)\n",
    "end_time_cb_fit = time.time()\n",
    "elapsed_time_search_cb = end_time_cb_search - start_time_cb\n",
    "elapsed_time_fit_cb = end_time_cb_fit - end_time_cb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for CatBoostRegressor:\", ST2_grid_search_cb.best_params_)\n",
    "print(\"Best score for CatBoost:\", -ST2_grid_search_cb.best_score_)\n",
    "print(\"CatBoostRegressor GridSearchCV Time:\", elapsed_time_search_cb)\n",
    "print(\"CatBoostRegressor Fitting Time:\", elapsed_time_fit_cb)\n",
    "\n",
    "# # Initialize GridSearchCV for Stacking model\n",
    "# start_time_stacking = time.time()\n",
    "# ST2_grid_search_stacking = GridSearchCV(estimator=stacking_regressor, param_grid=stacking_param_grid, cv=5, n_jobs=-1, verbose=0,scoring='neg_mean_absolute_error')\n",
    "# end_time_stacking_search = time.time()\n",
    "# ST2_grid_search_stacking.fit(ST2_X_train, ST2_Y_train)\n",
    "# end_time_stacking = time.time()\n",
    "# elapsed_time_search_stacking = end_time_stacking_search - start_time_stacking\n",
    "# elapsed_time_fit_stacking = end_time_stacking_fit - end_time_stacking_search\n",
    "\n",
    "# print(\"Best parameters for StackingRegressor:\", ST2_grid_search_stacking.best_params_)\n",
    "# print(\"Best score for StackingRegressor:\", -ST2_grid_search_stacking.best_score_)\n",
    "# print(\"StackingRegressor GridSearchCV Time:\", elapsed_time_search_stacking)\n",
    "# print(\"StackingRegressor Fitting Time:\", elapsed_time_fit_stacking)\n",
    "\n",
    "# Define the results of print statements as variables\n",
    "ST2_grid_search_and_fitting_results = {\n",
    "    'Model': ['RandomForestRegressor', 'HistGradientBoostingRegressor', 'AdaBoostRegressor', 'XGBRegressor'],\n",
    "    'Best Parameters': [ST2_grid_search_rf.best_params_, ST2_grid_search_hgb.best_params_, ST2_grid_search_ada.best_params_, ST2_grid_search_xgb.best_params_],\n",
    "    'Best Score': [-ST2_grid_search_rf.best_score_, -ST2_grid_search_hgb.best_score_, -ST2_grid_search_ada.best_score_, -ST2_grid_search_xgb.best_score_],\n",
    "    'GridSearchCV Time': [elapsed_time_search_rf, elapsed_time_search_hgb, elapsed_time_search_ada, elapsed_time_search_xgb],\n",
    "    'Fitting Time': [elapsed_time_fit_rf, elapsed_time_fit_hgb, elapsed_time_fit_ada, elapsed_time_fit_xgb]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_results = pd.DataFrame(ST2_grid_search_and_fitting_results)\n",
    "\n",
    "# Export DataFrame to Excel\n",
    "df_results.to_excel('data/results/ST2/ST2_ST2_grid_search_and_fitting_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de72c0-3121-4106-85ee-695360c30e84",
   "metadata": {},
   "source": [
    "### B. Stacking Regressor for Soil temperature at 5cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6465ba-d01f-48be-b621-83af37bfac1c",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155a7be-9820-4b7c-b240-5d6e1c0a4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the covariance matrix for target ST100\n",
    "ST5_dataset_correlation = ST5_clean_dataset_denormalized.drop(['ST100', 'ST50','ST20','ST10','ST5'], axis=1)\n",
    "ST5_covariance_matrix = ST5_dataset_correlation.cov()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "ST5_correlation_matrix = ST5_dataset_correlation.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST5_correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST5/ST5_denormalized_before_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Set the threshold\n",
    "threshold = 0.95\n",
    "# Find pairs of features with correlation above the threshold\n",
    "highly_correlated = np.where(np.abs(ST5_correlation_matrix) > threshold)\n",
    "highly_correlated_pairs = [(ST5_correlation_matrix.index[x], ST5_correlation_matrix.columns[y]) \n",
    "                           for x, y in zip(*highly_correlated) if x != y and x < y]\n",
    "\n",
    "print(\"Highly correlated pairs (above threshold):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)\n",
    "# Example: Removing one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for pair in highly_correlated_pairs:\n",
    "    features_to_remove.add(pair[1])  # You can choose to remove pair[0] or pair[1]\n",
    "\n",
    "# Drop the features from the dataframe\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated = ST5_dataset_correlation.drop(columns=features_to_remove)\n",
    "\n",
    "print(f\"Removed features: {features_to_remove}\")\n",
    "print(\"Shape of the reduced dataset:\", ST5_dataset_denormalized_outlier_filtered_uncorrelated.shape)\n",
    "\n",
    "# After removing the correlated features\n",
    "# Calculate the correlation matrix\n",
    "ST5_correlation_matrix_new = ST5_dataset_denormalized_outlier_filtered_uncorrelated.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST5_correlation_matrix_new, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST5/ST5_denormalized_after_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Assuming dataset_denormalized_outlier_filtered is your DataFrame\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST5_dataset_denormalized_outlier_filtered_uncorrelated.copy()\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = sm.add_constant(ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(data):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Iteratively remove features with VIF above the threshold\n",
    "def remove_high_vif_features(data, threshold=40.0):\n",
    "    while True:\n",
    "        vif_data = calculate_vif(data)\n",
    "        max_vif = vif_data['VIF'].max()\n",
    "        if max_vif > threshold:\n",
    "            # Identify the feature with the highest VIF\n",
    "            feature_to_remove = vif_data.sort_values('VIF', ascending=False)['feature'].iloc[0]\n",
    "            print(f\"Removing feature '{feature_to_remove}' with VIF: {max_vif}\")\n",
    "            data = data.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "    return data, vif_data\n",
    "\n",
    "# Remove high VIF features\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif, ST5_final_vif_data = remove_high_vif_features(ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "\n",
    "print(\"Final VIF data:\")\n",
    "print(ST5_final_vif_data)\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID'] = ST5_clean_dataset_denormalized['ID']\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST5'] = ST5_clean_dataset_denormalized['ST5']\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated['ID'] = ST5_clean_dataset_denormalized['ID']\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated['ST5'] = ST5_clean_dataset_denormalized['ST5']\n",
    "# Remove the constant term before creating the final DataFrame\n",
    "if 'const' in ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.columns:\n",
    "    ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(columns=['const'])\n",
    "\n",
    "# Store the 'ID' and 'ST5' columns with their corresponding index before PCA\n",
    "ID_index_mapping = ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID']\n",
    "ST5_index_mapping = ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST5']\n",
    "\n",
    "# Assume X is your feature dataframe\n",
    "ST5_X_pca = ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(['ST5', 'ID'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ST5_X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)  # Choose the number of components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "ST5_pca_df = pd.DataFrame(data=principal_components, columns=[f\"PC{i}\" for i in range(principal_components.shape[1])])\n",
    "\n",
    "# Merge PCA DataFrame with original DataFrame to maintain original index order\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ID_index_mapping, ST5_index_mapping, left_index=True, right_index=True)\n",
    "ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca, ST5_pca_df, left_index=True, right_index=True)\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.savefig('data/results/ST5/ST5_PCA_analysis.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef23549-f0b6-4704-98a5-c7d9f5ef3224",
   "metadata": {},
   "source": [
    "### Option 1:  ST5 Prediction by varying the dataset cases\n",
    "#### Note: Choose the dataset case at this line of the code: dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1)\n",
    "#### Dataset Cases:\n",
    "##### Case 1. dataset_denormalized_outlier_filtered\n",
    "##### case 2. ST5_clean_dataset_denormalized\n",
    "##### case 3. ST5_dataset_denormalized_outlier_filtered_uncorrelated\n",
    "##### case 4. ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif\n",
    "##### case 5. ST5_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369f405-ed0f-41f2-84b6-f7ba1d8d0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR  \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "# Choose any of the time-independent dataset cases\n",
    "# Select the dataset case here\n",
    "dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1) # Choose dataset case here\n",
    "  \n",
    "# Create function to evaluate model on few different levels\n",
    "def show_scores(model, X_train, X_valid, X_test, Y_train, Y_valid, Y_test, std, target='ST5', model_name='RF'):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    test_preds = model.predict(X_test)\n",
    "    scores = {\n",
    "              # \"Validation Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_test, test_preds),   \n",
    "              # \"Validation Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds), \n",
    "              # \"Validation Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set RMSE\": root_mean_squared_error(Y_valid, val_preds),\n",
    "              # \"Test Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Test Set R^2 Score\":r2_score(Y_valid, val_preds),  \n",
    "              # \"Test Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Test Set MAE\": mean_absolute_error(Y_test, test_preds), \n",
    "              # \"Test Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Tes Set RMSE\": root_mean_squared_error(Y_test, test_preds),\n",
    "              # \"Validation Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),             \n",
    "              # \"Validation Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['Metric', 'Value'])    \n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_excel(f'data/results/{target}/{model_name}_scores.xlsx', index=False)\n",
    "    return scores\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_plot(model, ST_X_train, ST_Y_train, ST_X_test, ST_Y_test, ST_X_validation, ST_Y_validation, name, std):\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_train_preds = model.predict(ST_X_train)\n",
    "    # Change train predictions to pandas series\n",
    "    ST_Y_train_preds_series = pd.Series(ST_Y_train_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_train_preds_series.index = ST_Y_train.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_train_sorted = ST_Y_train.sort_values().reset_index(drop=True)\n",
    "    ST_Y_train_preds_sorted = ST_Y_train_preds_series[ST_Y_train.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_train_mae = mean_absolute_error(ST_Y_train, ST_Y_train_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_train_rmse = root_mean_squared_error(ST_Y_train,ST_Y_train_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_train_r2_score = r2_score(ST_Y_train,ST_Y_train_preds)\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "    # Change validation predictions to pandas series\n",
    "    ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "    ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_valid_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_valid_rmse = root_mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_valid_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "\n",
    "    # Predict the test set which is forecast data\n",
    "    ST_Y_test_preds = model.predict(ST_X_test)\n",
    "    # Changes the predicted array values to pandas series\n",
    "    ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "    ST_Y_test_preds_series.index =ST_Y_test.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_test_sorted = ST_Y_test.sort_values().reset_index(drop=True)\n",
    "    ST_Y_test_preds_sorted = ST_Y_test_preds_series[ST_Y_test.index].sort_values().reset_index(drop=True)\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    ST_test_mae = mean_absolute_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate mean squared error\n",
    "    ST_test_rmse = root_mean_squared_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_test_r2_score = r2_score(ST_Y_test,ST_Y_test_preds)\n",
    "    \n",
    "    # Convert the Series to a DataFrame to return as dataframe\n",
    "    ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "    ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "\n",
    "\n",
    "     # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_train_sorted.index,ST_Y_train_sorted, color='blue', label=f'{name} Training Observed Values')\n",
    "    plt.plot(ST_Y_train_preds_sorted.index,ST_Y_train_preds_sorted, color='red', label=f'{name} Training Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_train_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_train_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_train_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 5 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Training Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_train_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Validation Set Observed Values')\n",
    "    plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Validation Set Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_valid_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_valid_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_valid_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 5 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Validation Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_valid_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the test sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_test_sorted.index,ST_Y_test_sorted, color='blue', label=f'{name} Test Observed Values')\n",
    "    plt.plot(ST_Y_test_preds_sorted.index,ST_Y_test_preds_sorted, color='red', label=f'{name} Test Predicted Values')\n",
    "    # Display the metrics as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_test_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_test_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_test_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 5 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Final Test Scores For {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_test_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()    \n",
    "    return ST_Y_test_preds_df\n",
    "    \n",
    "\n",
    "std_deviation = dataset_denormalized_outlier_filtered['ST5'].std()\n",
    "# Split the dataset into features and target\n",
    "many_features_dropped = ['mean_air_temperature_2m', 'relative_humidity','precipitation_mm','evaporation_mm','earth_heat_flux_MJ_m2', 'radiation_balance_w_m2', 'phosynthetic_active_radiation_mE_m2', 'albedo_RR_GR','snow_depth_cm','month','day']\n",
    "soil_features_dropped = ['ST5','ST10','ST20','ST50','ST100','ID']\n",
    "uncorrelated_dropped = ['ST5','ID']\n",
    "# ST5_X = dataset_shuffled.drop(many_features_dropped, axis=1)\n",
    "ST5_X = dataset_shuffled.drop(soil_features_dropped, axis=1)\n",
    "ST5_Y = dataset_shuffled['ST5']\n",
    "\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_5cm ).\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(dataset_shuffled))\n",
    "ST5_X_train, ST5_Y_train = ST5_X[:train_split], ST5_Y[:train_split]\n",
    "ST5_X_valid, ST5_Y_valid =ST5_X[train_split:valid_split], ST5_Y[train_split:valid_split]\n",
    "ST5_X_test, ST5_Y_test = ST5_X[valid_split:], ST5_Y[valid_split:]\n",
    "\n",
    "# A. CatBoostRegressor (CB)\n",
    "# Create CB model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_cb_model_stack = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "# Fit the model for ST5 to start with\n",
    "ST5_cb_model_stack.fit(ST5_X_train, ST5_Y_train, eval_set=(ST5_X_valid, ST5_Y_valid), early_stopping_rounds=100)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================CatBoost The Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST5_cb_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'CB'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# B. RandomForestRegressor\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_rf_model_stack = RandomForestRegressor(n_estimators=300, \n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         max_features='sqrt',\n",
    "                                         max_depth=None,\n",
    "                                         bootstrap=False,\n",
    "                                         random_state=42)\n",
    "# Fit the model for ST5 to start with\n",
    "ST5_rf_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================Random Forest The Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST5_rf_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'RF'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# C. Histogram Based Gradient Boosting Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_gbr_model_stack = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "# Fit the ST5 model for soil temp at 100 cm\n",
    "ST5_gbr_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Histogram-Based Gradient Boosting Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_gbr_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'HGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# D. XGBoost Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create XGBoost for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_xgb_model_stack = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "# Fit the ST5 model for soil temp at 100 cm\n",
    "ST5_xgb_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The XGBoost Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_xgb_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'XGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# E. AdaBoostRegressor \n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create AdaBoost Regressor for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_adb_model_stack = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "# Fit the ST5 model for soil temp at 100 cm\n",
    "ST5_adb_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The AdaBoost Regressor Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_adb_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'ADB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# F. Ridge Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_rg_model_stack = Ridge(random_state=42)\n",
    "# Fit the ST5 model for soil temp at 100 cm\n",
    "ST5_rg_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Ridge Regressor Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_rg_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'RR'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# G. Lasso Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create Lasso model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_la_model_stack = Lasso(random_state=42)\n",
    "# Fit the ST5 model for soil temp at 100cm\n",
    "ST5_la_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Lasso Regressor Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_la_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'LA'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# H. ElasticNet Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_en_model_stack = ElasticNet(random_state=42)\n",
    "# Fit the ST5 model for soil temp at 100cm\n",
    "ST5_en_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The ElasticNet Regressor Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_en_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'EN'))\n",
    "print(\"=========================================================================================================\\n\")\n",
    "\n",
    "# I. SVR-L Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-L model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_svrl_model_stack = SVR(kernel='linear')\n",
    "# Fit the ST5 model for soil temp at 100cm\n",
    "ST5_svrl_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with linear model Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_svrl_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'SVR-L'))\n",
    "print(\"==========================================================================================================\\n\")\n",
    "\n",
    "# J. SVR-R Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-R model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST5_svrr_model_stack = SVR(kernel='rbf')\n",
    "# Fit the ST5 model for soil temp at 100cm\n",
    "ST5_svrr_model_stack.fit(ST5_X_train, ST5_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with rfb model Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_svrr_model_stack, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'SVR-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# Stack of predictors on a single data set\n",
    "ST5_rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_samples_split=2,\n",
    "                                     max_features='sqrt',\n",
    "                                     max_depth=None,\n",
    "                                     bootstrap=False,\n",
    "                                     random_state=42)\n",
    "ST5_gbdt_regresssor = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "ST5_xgb_model = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "ST5_cb_regressor = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "ST5_adb_regressor = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    (\"RandomForest\", ST5_rf_regressor),\n",
    "    (\"CatBoost\", ST5_cb_regressor),\n",
    "    (\"HistGradientBoosting\", ST5_gbdt_regresssor),\n",
    "    (\"XGBoost\", ST5_xgb_model)\n",
    "]\n",
    "ST5_stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\n",
    "\n",
    "# Measure and plot the results\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for ax, (name, est) in zip(axs, estimators + [(\"Stacking Regressor\", ST5_stacking_regressor)]):\n",
    "    scorers = {\"R^2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\", \"RMSE\": \"neg_root_mean_squared_error\"}\n",
    "\n",
    "    start_time = time.time()\n",
    "    scores = cross_validate(est, ST5_X_train, ST5_Y_train, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_pred = cross_val_predict(est, ST5_X_valid, ST5_Y_valid, n_jobs=-1, verbose=0)\n",
    "    y_test = cross_val_predict(est, ST5_X_test, ST5_Y_test, n_jobs=-1, verbose=0)\n",
    "    \n",
    "    # Calculate mean and std_deviation for each scorer\n",
    "    scores_mean_std = {\n",
    "        key: (np.abs(np.mean(scores[f'test_{value}'])), np.std(scores[f'test_{value}']))\n",
    "        for key, value in scorers.items()\n",
    "    }\n",
    "\n",
    "    # Format the scores\n",
    "    formatted_scores = {\n",
    "        key: f\"{mean:.4f} ± {std_dev:.4f}\"\n",
    "        for key, (mean, std_dev) in scores_mean_std.items()\n",
    "    }\n",
    "\n",
    "    display = PredictionErrorDisplay.from_predictions(\n",
    "        y_true=ST5_Y_valid,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        ax=ax,\n",
    "        scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "        line_kwargs={\"color\": \"tab:red\"},\n",
    "    )\n",
    "    ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.4f} seconds\", fontsize=14)\n",
    "    # Set custom x-label and y-label\n",
    "    ax.set_xlabel(\"Predicted Soil Temperature at 5 cm (°C)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Observed Soil Temperature at 5 cm (°C)\", fontsize=14)\n",
    "\n",
    "    for metric_name, (mean, std_dev) in scores_mean_std.items():\n",
    "        if metric_name == 'R^2':\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {formatted_scores[metric_name]}\")\n",
    "        else:\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {mean:.4f} ± {std_dev:.4f}\")\n",
    "    \n",
    "    ax.legend(loc=\"best\", fontsize='small')\n",
    "    # Save the mean and std scores to an Excel file\n",
    "    df_scores_summary = pd.DataFrame(scores_mean_std).T\n",
    "    df_scores_summary.columns = ['Train Mean', 'Train Std Dev']\n",
    "    df_scores_summary.to_excel(f'data/results/ST5/{name}_cv_scores.xlsx', index=True)\n",
    "# Hide any unused subplots\n",
    "for i in range(len(estimators)+1, len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "# Apply tight layout\n",
    "plt.tight_layout()\n",
    "# Save the entire figure with all subplots to a file\n",
    "fig.savefig('data/results/ST5/stacked_regressors_prediction_error_plots.png', bbox_inches='tight')\n",
    "\n",
    "# Sort actual values and get sorted indices\n",
    "ST5_Y_valid_sorted = ST5_Y_valid.sort_values()\n",
    "sorted_indices = ST5_Y_valid_sorted.index\n",
    "\n",
    "# Reorder y_pred using the sorted indices\n",
    "y_pred_sorted = pd.Series(y_pred, index=ST5_Y_valid.index).loc[sorted_indices]\n",
    "\n",
    "# Calculate metrics for the validation set predictions\n",
    "mae_valid = mean_absolute_error(ST5_Y_valid, y_pred)\n",
    "rmse_valid = np.sqrt(mean_squared_error(ST5_Y_valid, y_pred))\n",
    "r2_valid = r2_score(ST5_Y_valid, y_pred)\n",
    "\n",
    "# Calculate metrics for the test set predictions\n",
    "mae_test = mean_absolute_error(ST5_Y_test, y_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(ST5_Y_test, y_test))\n",
    "r2_test = r2_score(ST5_Y_test, y_test)\n",
    "\n",
    "# Save the validation metrics to an Excel file\n",
    "validation_test_metrics = {\n",
    "    'V_R^2': [r2_valid],\n",
    "    'V_MAE': [mae_valid],\n",
    "    'V_RMSE': [rmse_valid],\n",
    "    'T_R^2': [r2_test],\n",
    "    'T_MAE': [mae_test],\n",
    "    'T_RMSE': [rmse_test]    \n",
    "}\n",
    "# Save the validation metrics to an Excel file\n",
    "\n",
    "df_metrics = pd.DataFrame(validation_test_metrics)\n",
    "df_metrics.to_excel(f'data/results/ST5/{name}_validation_test_metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the sorted actual values and corresponding predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ST5_Y_valid_sorted.values, color='blue', label='Observed Values')\n",
    "plt.plot(y_pred_sorted.values, color='red', label='Predicted Values')\n",
    "\n",
    "# Display the metrics as text annotation\n",
    "plt.text(0.1, 0.75, f'MAE: {mae_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.3, 0.75, f'RMSE: {rmse_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.5, 0.75, f'R^2: {r2_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel('Index', fontsize=14)\n",
    "plt.ylabel('Soil Temperature at 5 cm (°C)', fontsize=14)\n",
    "plt.title(f'STACK-R model\\'s validation set\\'s predicted vs observed values for ST5', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'data/results/ST5/{name}_cross_validation_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "dump(ST5_stacking_regressor, filename=\"data/results/ST5/ST5_STACK-R_trained_model.joblib\");\n",
    "# Fit the stacking regressor for direct fitting and prediction for all sets at default CV=5\n",
    "ST5_stacking_regressor.fit(ST5_X_train, ST5_Y_train)\n",
    "print(\"====================The Stacking Regressor Evaluation Metrics Results For ST5 Denormalized =======================\\n\")\n",
    "print(show_scores(ST5_stacking_regressor, ST5_X_train, ST5_X_valid, ST5_X_test, ST5_Y_train, ST5_Y_valid, ST5_Y_test, std_deviation,'ST5', 'STACK-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "# ST5_Y_test_preds_df = predict_plot(ST5_stacking_regressor, ST5_X_train, ST5_Y_train, ST5_X_test, ST5_Y_test, ST5_X_valid, ST5_Y_valid, 'ST5', std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb8f41-fc1a-4bdf-96f0-042b983c8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_denormalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d5022-6a05-43b2-8ecd-2f465bc5e8ea",
   "metadata": {},
   "source": [
    "### Cross-validation to check stability of the stacking regressor for ST5\n",
    "### NOTE: This will take time!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f58fc-ead9-4299-8153-c4988e387218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "ST5_cv_scores = cross_val_score(ST5_stacking_regressor, ST5_X_train, ST5_Y_train, cv=kf, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive\n",
    "ST5_cv_scores = -ST5_cv_scores\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-Validation Scores (MSE):\", ST5_cv_scores)\n",
    "print(\"Mean CV Score (MSE):\", np.mean(ST5_cv_scores))\n",
    "print(\"Standard Deviation of CV Scores:\", np.std(ST5_cv_scores))\n",
    "# Save the scores to an Excel file\n",
    "ST5_cv_scores_df = pd.DataFrame(ST5_cv_scores, columns=['MSE'])\n",
    "ST5_cv_scores_df.to_excel('data/results/ST5/ST5_10_fold_cv_scores.xlsx', index=False)\n",
    "\n",
    "##=========== Visualize the problematic Fold using histogram==================\n",
    "# Calculate mean MSE\n",
    "ST5_mean_mse = np.mean(ST5_cv_scores)\n",
    "# Identify the problematic fold\n",
    "ST5_problematic_fold_index = np.argmax(np.abs(ST5_cv_scores - ST5_mean_mse))\n",
    "# Get the indices of the data points in the problematic fold\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(ST5_X_train)):\n",
    "    if fold_index == ST5_problematic_fold_index:\n",
    "        problematic_fold_train_indices = train_index\n",
    "        problematic_fold_test_indices = test_index\n",
    "\n",
    "# Subset the data for the problematic fold\n",
    "X_problematic_fold = ST5_X_train.iloc[problematic_fold_test_indices]\n",
    "y_problematic_fold = ST5_Y_train.iloc[problematic_fold_test_indices]\n",
    "# Visualize or analyze features for the problematic fold\n",
    "for feature in ST5_X_train.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Histogram for the problematic fold\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_problematic_fold[feature], kde=True, bins=20, color='red')\n",
    "    plt.title(f'{feature} - Problematic Fold')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    # Histogram for the entire dataset\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(ST5_X_train[feature], kde=True, bins=20, color='blue')\n",
    "    plt.title(f'{feature} - Entire Dataset')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/results/ST5/ST5_CV_problematic_10_fold_vs_main_dataset_histograms.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d20937-6ed5-4bbb-a3d5-f1449f8f41a5",
   "metadata": {},
   "source": [
    "### Partial Dependence, Individual Conditional Expectation and Residual Analysis Plots for ST5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b73e3-7e80-48e9-9fe7-0c88b8ab190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import statsmodels.api as sm\n",
    "from pycebox.ice import ice, ice_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# # Partial Dependence Plot\n",
    "# print('====================================================== ST5 Partial Dependence Plot')\n",
    "# ST5_feature_names = ST5_X_train.columns.tolist()\n",
    "# n_features = len(ST5_feature_names)\n",
    "# n_cols = 2\n",
    "# n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "# fig1, ax1 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat = ax1.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST5_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST5_stacking_regressor, ST5_X_train, features=[feature])\n",
    "#     display.plot(ax=axes_flat[idx])\n",
    "#     axes_flat[idx].set_title(f'Partial Dependence (PD) Plot for {feature}')\n",
    "#     axes_flat[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat)):\n",
    "#     fig1.delaxes(axes_flat[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "# plt.suptitle('Partial Dependence (PD) Plot', fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# # plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST5/ST5_final_partial_dependence_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Individual Conditional Expectation Plot (ICE)\n",
    "# print('======================== ST5 Individual Conditional Expectation Plot ===============================')\n",
    "# fig2, axes2 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat2 = axes2.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST5_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST5_stacking_regressor, ST5_X_train, features=[feature], kind='individual')\n",
    "#     display.plot(ax=axes_flat2[idx])\n",
    "#     axes_flat2[idx].set_title(f'Individual Conditional Expectation (ICE) Plot for {feature}')\n",
    "#     axes_flat2[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat2)):\n",
    "#     fig2.delaxes(axes_flat2[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.7)\n",
    "# plt.suptitle('Individual Conditional Expectation (ICE) Plot', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST5/ST5_final_individual_conditional_expectation_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# Residual Analysis\n",
    "print('========================= ST5 Residual Analysis Plot ==============================')\n",
    "ST5_Y_predictions = ST5_stacking_regressor.predict(ST5_X_test)\n",
    "ST5_residuals = ST5_Y_test - ST5_Y_predictions\n",
    "\n",
    "# Calculate the interquartile range (IQR)\n",
    "Q1 = np.percentile(ST5_residuals, 25)\n",
    "Q3 = np.percentile(ST5_residuals, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the whisker range\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Count outliers\n",
    "outliers = np.sum((ST5_residuals < lower_bound) | (ST5_residuals > upper_bound))\n",
    "total_residuals = len(ST5_residuals)\n",
    "outlier_percentage = (outliers / total_residuals) * 100\n",
    "\n",
    "# Plotting the residuals scatter plot and box-and-whisker plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Residuals analysis plot\n",
    "ax[0].scatter(ST5_Y_predictions, ST5_residuals)\n",
    "ax[0].set_xlabel('Predictions', fontsize=14)\n",
    "ax[0].set_ylabel('Residuals', fontsize=14)\n",
    "ax[0].set_title('ST5 Residuals Analysis Plot', fontsize=14)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax[0].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# Box-and-whisker plot for residuals\n",
    "sns.boxplot(y=ST5_residuals, ax=ax[1])\n",
    "ax[1].set_title('ST5 Box-and-Whisker Plot of Residuals', fontsize=14)\n",
    "ax[1].set_ylabel('ST5 Residuals', fontsize=14)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# Annotate the plot with the number of outliers and total residuals\n",
    "annotation_text = (f'Total Residuals: {total_residuals}\\n'\n",
    "                   f'Number of Outliers: {outliers}\\n'\n",
    "                   f'Percentage of Outliers: {outlier_percentage:.2f}%')\n",
    "ax[1].annotate(annotation_text, xy=(0.8, 0.87), xycoords='axes fraction',\n",
    "               fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST5/ST5_final_residual_and_boxplot_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predictor Variables\n",
    "print('========================= Residuals vs. Predictor Variables ==============================')\n",
    "for column in ST5_X_test.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.scatter(ST5_X_test[column], ST5_residuals)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel(column, fontsize=14)\n",
    "    ax.set_ylabel('Residuals', fontsize=14)\n",
    "    ax.set_title(f'Residuals vs. {column}', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/results/ST5_final_residuals_vs_{column}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the Box Plot of all features\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Number of features in the DataFrame\n",
    "num_features = dataset_denormalized_outlier_filtered.shape[1]\n",
    "# Calculate the number of rows needed to plot all features in 3 columns\n",
    "num_cols = 3\n",
    "num_rows = math.ceil(num_features / num_cols)\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "# Define colors for each column\n",
    "colors = ['green', 'purple', 'red']\n",
    "# Create a Box Plot for each feature\n",
    "for i, column in enumerate(dataset_denormalized_outlier_filtered.columns):\n",
    "    col_index = i % num_cols  # Determine the column index (0, 1, or 2)\n",
    "    sns.boxplot(data=dataset_denormalized_outlier_filtered[column], ax=axes[i], color=colors[col_index])\n",
    "    axes[i].set_title(f'Box Plot for {column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Values', fontsize=14)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[i].tick_params(axis='both', which='minor', labelsize=12)\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST5/ST5_Box_plot_of_features.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# # Q-Q Plot\n",
    "# print('========================= ST5 Q-Q Plot ==============================')\n",
    "# fig5, ax5 = plt.subplots(figsize=(10, 7))\n",
    "# sm.qqplot(ST5_residuals, line='45', ax=ax5)\n",
    "# ax5.set_title('Q-Q Plot of Residuals')\n",
    "# plt.savefig('data/results/ST5/ST5_final_Q-Q_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Histogram of residuals\n",
    "# fig6, ax6 = plt.subplots(figsize=(10, 7))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(residuals, kde=True, ax=ax6)\n",
    "# plt.xlabel('Residuals')\n",
    "# plt.title('Histogram of Residuals')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ff97c-d053-49d9-b7e0-b00c6537a5bf",
   "metadata": {},
   "source": [
    "### Feature Importance analysis for ST5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401f446-2739-4c82-9c92-e402b55eb1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# # Assuming feature_names is a list of your feature names\n",
    "# feature_names = ST5_X_train.columns.tolist()\n",
    "\n",
    "# estimators = [\n",
    "#     (\"Random Forest\", ST5_rf_model_stack),\n",
    "#     (\"Cat Boost\", ST5_cb_model_stack),\n",
    "#     (\"XG Boost\", ST5_xgb_model_stack)\n",
    "# ]\n",
    "\n",
    "# # Initialize an array to store aggregated feature importances\n",
    "# num_features = ST5_X_train.shape[1]\n",
    "# feature_importances = np.zeros(num_features, dtype=np.float64)\n",
    "\n",
    "# # Aggregate feature importances from base models\n",
    "# count = 0\n",
    "# for name, model in estimators:\n",
    "#     if hasattr(model, 'feature_importances_'):\n",
    "#         importances = np.array(model.feature_importances_, dtype=np.float64)\n",
    "#         feature_importances += importances\n",
    "#         count += 1\n",
    "#     else:\n",
    "#         print(f\"{name} does not have feature_importances_ attribute\")\n",
    "\n",
    "# # Normalize the aggregated feature importances\n",
    "# if count > 0:\n",
    "#     feature_importances /= count\n",
    "\n",
    "# # Convert feature importances to percentages\n",
    "# feature_importances_percentage = 100 * (feature_importances / feature_importances.sum())\n",
    "\n",
    "# # Sort the feature importances in descending order\n",
    "# sorted_indices = np.argsort(feature_importances_percentage)[::-1]\n",
    "# sorted_feature_importances_percentage = feature_importances_percentage[sorted_indices]\n",
    "# sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# # Plot the sorted feature importances\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# bars = plt.barh(sorted_feature_names, sorted_feature_importances_percentage)\n",
    "# plt.xlabel(\"Aggregated feature importance (%)\")\n",
    "# plt.ylabel(\"Feature Names\")\n",
    "# plt.title(\"Soil Temp (5 cm) aggregated feature importances from base estimators\")\n",
    "# plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "\n",
    "# # Add annotations to the bars\n",
    "# for bar in bars:\n",
    "#     width = bar.get_width()\n",
    "#     plt.text(\n",
    "#         width + 0.5,  # Offset slightly to the right of the bar\n",
    "#         bar.get_y() + bar.get_height() / 2,  # Vertical center of the bar\n",
    "#         f'{width:.2f}%',  # Display the width (importance value) formatted to 2 decimal places\n",
    "#         va='center'\n",
    "#     )\n",
    "# # Save the figure\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('data/results/ST5/ST5_stacking_regressors_feature_importances.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# plot_features(ST5_X_train.columns, ST5_rf_model_stack.feature_importances_, 'data/results/ST5_RF_feature_analysis.png', 'ST5 RF Feature Importance Plot')\n",
    "# plot_features(ST5_X_train.columns, ST5_cb_model_stack.feature_importances_/100, 'data/results/ST5_CB_feature_analysis.png', 'ST5 CB Feature Importance Plot')\n",
    "# plot_features(ST5_X_train.columns, ST5_xgb_model_stack.feature_importances_, 'data/results/ST5_XGB_feature_analysis.png', 'ST5 XGB Feature Importance Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf1cc80-34df-4671-97f1-e647cf1a779e",
   "metadata": {},
   "source": [
    "### Feature importance analysis for ST5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43a616-e267-4005-981c-6475680dccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ST5_X_train and ST5_Y_train are your training data\n",
    "\n",
    "# Fit the stacking regressor\n",
    "ST5_stacking_regressor.fit(ST5_X_train, ST5_Y_train)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = ST5_X_train.columns\n",
    "\n",
    "# Initialize an array to store feature importances\n",
    "feature_importances = np.zeros(ST5_X_train.shape[1])\n",
    "\n",
    "# Function to extract feature importances\n",
    "def get_feature_importance(model, X, y):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return np.abs(model.coef_)\n",
    "    elif isinstance(model, CatBoostRegressor):\n",
    "        return model.get_feature_importance()\n",
    "    else:\n",
    "        # Use permutation importance as a fallback for models without direct attribute\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        return result.importances_mean\n",
    "\n",
    "# Aggregate feature importances\n",
    "for name, model in ST5_stacking_regressor.named_estimators_.items():\n",
    "    importances = get_feature_importance(model, ST5_X_train, ST5_Y_train)\n",
    "    feature_importances += importances\n",
    "\n",
    "# Normalize the aggregated feature importances\n",
    "feature_importances /= len(ST5_stacking_regressor.named_estimators_)\n",
    "\n",
    "# Convert importances to percentage\n",
    "feature_importances_percentage = 100 * (feature_importances / np.sum(feature_importances))\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances_percentage\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('ST5 Stacking Regressor Feature Importances')\n",
    "\n",
    "# Add annotations\n",
    "for index, value in enumerate(importance_df['Importance']):\n",
    "    plt.text(value, index, f'{value:.2f}%', va='center')\n",
    "\n",
    "plt.savefig('data/results/ST5/ST5_stacking_regressor_feature_importances.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b134b6-c303-4f55-8e6c-f90cc6db2068",
   "metadata": {},
   "source": [
    "### Learning curves evaluation for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ff262-aee3-41d4-9579-0be5e968f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def plot_learning_curves(model, X_train, Y_train, X_valid, Y_valid, feature):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MAE to positive\n",
    "    train_errors_mae = -train_scores.mean(axis=1)\n",
    "    val_errors_mae = -valid_scores.mean(axis=1)\n",
    "    \n",
    "    train_sizes_mse, train_scores_mse, valid_scores_mse = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MSE to positive\n",
    "    train_errors_mse = -train_scores_mse.mean(axis=1)\n",
    "    val_errors_mse = -valid_scores_mse.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot MAE learning curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_sizes, train_errors_mae, \"r-\", label=\"Training MAE\")\n",
    "    plt.plot(train_sizes, val_errors_mae, \"b-\", label=\"Validation MAE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"MAE Learning Curve\")\n",
    "    \n",
    "    # Plot MSE learning curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_sizes, train_errors_mse, \"r-\", label=\"Training RMSE\")\n",
    "    plt.plot(train_sizes, val_errors_mse, \"b-\", label=\"Validation RMSE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE Learning Curve\")    \n",
    "    plt.savefig('data/results/ST5/ST5_learning_curves.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example call (ensure your data and model are defined correctly)\n",
    "plot_learning_curves(ST5_stacking_regressor, ST5_X_train, ST5_Y_train, ST5_X_valid, ST5_Y_valid, \"ST5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef41ee3-f065-482a-a01a-24ba8d0534ec",
   "metadata": {},
   "source": [
    "### GridSearhCV Evaluation for all models used in the stacked regressor for ST5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad66c12-a8a3-4d3a-8f90-255bdbb68912",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_cb = {\n",
    "    'iterations': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'depth': [4, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 50, 100]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    'max_leaf_nodes': [31, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [20, 50, 100],\n",
    "    'l2_regularization': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [1.0, 0.8, 0.6],\n",
    "    'colsample_bytree': [1.0, 0.8, 0.6],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 0.1, 0.01],\n",
    "    'tree_method': ['gpu_hist']  # Use GPU\n",
    "}\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# stacking_param_grid ={\n",
    "#     'rf__n_estimators': [100, 300, 500],\n",
    "#     'rf__max_depth': [None, 10, 20, 30],\n",
    "#     'hgb__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'hgb__max_iter': [100, 200, 500],\n",
    "#     'catboost__iterations': [100, 200, 500],\n",
    "#     'catboost__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'catboost__depth': [4, 6, 10],\n",
    "#     'xgb__n_estimators': [100, 200],\n",
    "#     'xgb__max_depth': [3, 5]\n",
    "# }\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "ada = AdaBoostRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestRegressor()),\n",
    "#     ('cb', CatBoostRegressor()),\n",
    "#     ('hgb', HistGradientBoostingRegressor()),    \n",
    "#     ('xgb', XGBRegressor())\n",
    "# ]\n",
    "\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=RidgeCV()\n",
    "# )\n",
    "\n",
    "# Initialize GridSearchCV for RF model\n",
    "start_time_rf = time.time()\n",
    "ST5_grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_rf_search = time.time()\n",
    "ST5_grid_search_rf.fit(ST5_X_train, ST5_Y_train)\n",
    "end_time_rf_fit = time.time()\n",
    "elapsed_time_search_rf = end_time_rf_search - start_time_rf\n",
    "elapsed_time_fit_rf = end_time_rf_fit - end_time_rf_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for RandomForestRegressor:\", ST5_grid_search_rf.best_params_)\n",
    "print(\"Best score for RandomForestRegressor:\", -ST5_grid_search_rf.best_score_)\n",
    "print(\"RandomForestRegressor GridSearchCV Time:\", elapsed_time_search_rf)\n",
    "print(\"RandomForestRegressor Fitting Time:\", elapsed_time_fit_rf)\n",
    "\n",
    "# Initialize GridSearchCV for HGB model\n",
    "start_time_hgb = time.time()\n",
    "ST5_grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=param_grid_hgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_hgb_search = time.time()\n",
    "ST5_grid_search_hgb.fit(ST5_X_train, ST5_Y_train)\n",
    "end_time_hgb_fit = time.time()\n",
    "elapsed_time_search_hgb = end_time_hgb_search - start_time_hgb\n",
    "elapsed_time_fit_hgb = end_time_hgb_fit - end_time_hgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for HistGradientBoostingRegressor:\", ST5_grid_search_hgb.best_params_)\n",
    "print(\"Best score for HistGradientBoostingRegressor:\", -ST5_grid_search_hgb.best_score_)\n",
    "print(\"HistGradientBoostingRegressor GridSearchCV Time:\", elapsed_time_search_hgb)\n",
    "print(\"HistGradientBoostingRegressor Fitting Time:\", elapsed_time_fit_hgb)\n",
    "\n",
    "# Initialize GridSearchCV for XGB model\n",
    "# start_time_xgb = time.time()\n",
    "# ST5_grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "# end_time_xgb_search = time.time()\n",
    "# ST5_grid_search_xgb.fit(ST5_X_train, ST5_Y_train)\n",
    "# end_time_xgb_fit = time.time()\n",
    "# elapsed_time_search_xgb = end_time_xgb_search - start_time_xgb\n",
    "# elapsed_time_fit_xgb = end_time_xgb_fit - end_time_xgb_search\n",
    "# # Get the best parameters and scores\n",
    "# print(\"Best parameters for XGBRegressor:\", ST5_grid_search_xgb.best_params_)\n",
    "# print(\"Best score for XGBRegressor:\", -ST5_grid_search_xgb.best_score_)\n",
    "# print(\"XGBRegressor GridSearchCV Time:\", elapsed_time_search_xgb)\n",
    "# print(\"XGBRegressor Fitting Time:\", elapsed_time_fit_xgb)\n",
    "\n",
    "# Initialize GridSearchCV for ADA model\n",
    "start_time_ada = time.time()\n",
    "ST5_grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_ada_search = time.time()\n",
    "ST5_grid_search_ada.fit(ST5_X_train, ST5_Y_train)\n",
    "end_time_ada_fit = time.time()\n",
    "elapsed_time_search_ada = end_time_ada_search - start_time_ada\n",
    "elapsed_time_fit_ada = end_time_ada_fit - end_time_ada_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for AdaBoostRegressor:\", ST5_grid_search_ada.best_params_)\n",
    "print(\"Best score for AdaBoostRegressor:\", -ST5_grid_search_ada.best_score_)\n",
    "print(\"AdaBoostRegressor GridSearchCV Time:\", elapsed_time_search_ada)\n",
    "print(\"AdaBoostRegressor Fitting Time:\", elapsed_time_fit_ada)\n",
    "\n",
    "# Initialize GridSearchCV for CB model\n",
    "# start_time_cb = time.time()\n",
    "# ST5_grid_search_cb = GridSearchCV(estimator=cb, param_grid=param_grid_cb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "# end_time_cb_search = time.time()\n",
    "# ST5_grid_search_cb.fit(ST5_X_train, ST5_Y_train)\n",
    "# end_time_cb_fit = time.time()\n",
    "# elapsed_time_search_cb = end_time_cb_search - start_time_cb\n",
    "# elapsed_time_fit_cb = end_time_cb_fit - end_time_cb_search\n",
    "# # Get the best parameters and scores\n",
    "# print(\"Best parameters for CatBoostRegressor:\", ST5_grid_search_cb.best_params_)\n",
    "# print(\"Best score for CatBoost:\", -ST5_grid_search_cb.best_score_)\n",
    "# print(\"CatBoostRegressor GridSearchCV Time:\", elapsed_time_search_cb)\n",
    "# print(\"CatBoostRegressor Fitting Time:\", elapsed_time_fit_cb)\n",
    "\n",
    "# # Initialize GridSearchCV for Stacking model\n",
    "# start_time_stacking = time.time()\n",
    "# ST5_grid_search_stacking = GridSearchCV(estimator=stacking_regressor, param_grid=stacking_param_grid, cv=5, n_jobs=-1, verbose=0,scoring='neg_mean_absolute_error')\n",
    "# end_time_stacking_search = time.time()\n",
    "# ST5_grid_search_stacking.fit(ST5_X_train, ST5_Y_train)\n",
    "# end_time_stacking = time.time()\n",
    "# elapsed_time_search_stacking = end_time_stacking_search - start_time_stacking\n",
    "# elapsed_time_fit_stacking = end_time_stacking_fit - end_time_stacking_search\n",
    "\n",
    "# print(\"Best parameters for StackingRegressor:\", ST5_grid_search_stacking.best_params_)\n",
    "# print(\"Best score for StackingRegressor:\", -ST5_grid_search_stacking.best_score_)\n",
    "# print(\"StackingRegressor GridSearchCV Time:\", elapsed_time_search_stacking)\n",
    "# print(\"StackingRegressor Fitting Time:\", elapsed_time_fit_stacking)\n",
    "\n",
    "# Define the results of print statements as variables\n",
    "# ST5_grid_search_and_fitting_results = {\n",
    "#     'Model': ['RandomForestRegressor', 'HistGradientBoostingRegressor', 'AdaBoostRegressor', 'XGBRegressor'],\n",
    "#     'Best Parameters': [ST5_grid_search_rf.best_params_, ST5_grid_search_hgb.best_params_, ST5_grid_search_ada.best_params_, ST5_grid_search_xgb.best_params_],\n",
    "#     'Best Score': [-ST5_grid_search_rf.best_score_, -ST5_grid_search_hgb.best_score_, -ST5_grid_search_ada.best_score_, -ST5_grid_search_xgb.best_score_],\n",
    "#     'GridSearchCV Time': [elapsed_time_search_rf, elapsed_time_search_hgb, elapsed_time_search_ada, elapsed_time_search_xgb],\n",
    "#     'Fitting Time': [elapsed_time_fit_rf, elapsed_time_fit_hgb, elapsed_time_fit_ada, elapsed_time_fit_xgb]\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df_results = pd.DataFrame(ST5_grid_search_and_fitting_results)\n",
    "\n",
    "# # Export DataFrame to Excel\n",
    "# df_results.to_excel('data/results/ST5/ST5_grid_search_and_fitting_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ea212-7d68-47c7-8ad3-844cd5fe256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_denormalized_outlier_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16cd9f7-00f6-4b86-aeec-0282a4a41c82",
   "metadata": {},
   "source": [
    "### C. Stacking Regressor for Soil temperature at 10cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8440ce-97fa-4297-b435-5e9ec0f422cf",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c3dfb-8714-4fbc-a144-e3022cd41ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the covariance matrix for target ST100\n",
    "ST10_dataset_correlation = ST10_clean_dataset_denormalized.drop(['ST100', 'ST50','ST20','ST10'], axis=1)\n",
    "ST10_covariance_matrix = ST10_dataset_correlation.cov()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "ST10_correlation_matrix = ST10_dataset_correlation.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST10_correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST10/ST10_denormalized_before_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Set the threshold\n",
    "threshold = 0.95\n",
    "# Find pairs of features with correlation above the threshold\n",
    "highly_correlated = np.where(np.abs(ST10_correlation_matrix) > threshold)\n",
    "highly_correlated_pairs = [(ST10_correlation_matrix.index[x], ST10_correlation_matrix.columns[y]) \n",
    "                           for x, y in zip(*highly_correlated) if x != y and x < y]\n",
    "\n",
    "print(\"Highly correlated pairs (above threshold):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)\n",
    "# Example: Removing one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for pair in highly_correlated_pairs:\n",
    "    features_to_remove.add(pair[1])  # You can choose to remove pair[0] or pair[1]\n",
    "\n",
    "# Drop the features from the dataframe\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated = ST10_dataset_correlation.drop(columns=features_to_remove)\n",
    "\n",
    "print(f\"Removed features: {features_to_remove}\")\n",
    "print(\"Shape of the reduced dataset:\", ST10_dataset_denormalized_outlier_filtered_uncorrelated.shape)\n",
    "\n",
    "# After removing the correlated features\n",
    "# Calculate the correlation matrix\n",
    "ST10_correlation_matrix_new = ST10_dataset_denormalized_outlier_filtered_uncorrelated.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST10_correlation_matrix_new, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST10/ST10_denormalized_after_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Assuming dataset_denormalized_outlier_filtered is your DataFrame\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST10_dataset_denormalized_outlier_filtered_uncorrelated.copy()\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = sm.add_constant(ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(data):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Iteratively remove features with VIF above the threshold\n",
    "def remove_high_vif_features(data, threshold=40.0):\n",
    "    while True:\n",
    "        vif_data = calculate_vif(data)\n",
    "        max_vif = vif_data['VIF'].max()\n",
    "        if max_vif > threshold:\n",
    "            # Identify the feature with the highest VIF\n",
    "            feature_to_remove = vif_data.sort_values('VIF', ascending=False)['feature'].iloc[0]\n",
    "            print(f\"Removing feature '{feature_to_remove}' with VIF: {max_vif}\")\n",
    "            data = data.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "    return data, vif_data\n",
    "\n",
    "# Remove high VIF features\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif, ST10_final_vif_data = remove_high_vif_features(ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "\n",
    "print(\"Final VIF data:\")\n",
    "print(ST10_final_vif_data)\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID'] = ST10_clean_dataset_denormalized['ID']\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST10'] = ST10_clean_dataset_denormalized['ST10']\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated['ID'] = ST10_clean_dataset_denormalized['ID']\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated['ST10'] = ST10_clean_dataset_denormalized['ST10']\n",
    "# Remove the constant term before creating the final DataFrame\n",
    "if 'const' in ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.columns:\n",
    "    ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(columns=['const'])\n",
    "\n",
    "# Store the 'ID' and 'ST10' columns with their corresponding index before PCA\n",
    "ID_index_mapping = ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID']\n",
    "ST10_index_mapping = ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST10']\n",
    "\n",
    "# Assume X is your feature dataframe\n",
    "ST10_X_pca = ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(['ST10', 'ID'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ST10_X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)  # Choose the number of components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "ST10_pca_df = pd.DataFrame(data=principal_components, columns=[f\"PC{i}\" for i in range(principal_components.shape[1])])\n",
    "\n",
    "# Merge PCA DataFrame with original DataFrame to maintain original index order\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ID_index_mapping, ST10_index_mapping, left_index=True, right_index=True)\n",
    "ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca, ST10_pca_df, left_index=True, right_index=True)\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.savefig('data/results/ST10/ST10_PCA_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3d50d-bf3c-4557-84a9-b17d0d7d92e6",
   "metadata": {},
   "source": [
    "### Option 1:  ST10 Prediction by varying the dataset cases\n",
    "#### Note: Choose the dataset case at this line of the code: dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1)\n",
    "#### Dataset Cases:\n",
    "##### Case 1. dataset_denormalized_outlier_filtered\n",
    "##### case 2. ST10_clean_dataset_denormalized\n",
    "##### case 3. ST10_dataset_denormalized_outlier_filtered_uncorrelated\n",
    "##### case 4. ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif\n",
    "##### case 5. ST10_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd9d15-b33a-4839-8675-07c1bb84a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR  \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "# Choose any of the time-independent dataset cases\n",
    "# Select the dataset case here\n",
    "dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1) # Choose dataset case here\n",
    "  \n",
    "# Create function to evaluate model on few different levels\n",
    "def show_scores(model, X_train, X_valid, X_test, Y_train, Y_valid, Y_test, std, target='ST10', model_name='RF'):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    test_preds = model.predict(X_test)\n",
    "    scores = {\n",
    "              # \"Validation Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_test, test_preds),   \n",
    "              # \"Validation Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds), \n",
    "              # \"Validation Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set RMSE\": root_mean_squared_error(Y_valid, val_preds),\n",
    "              # \"Test Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Test Set R^2 Score\":r2_score(Y_valid, val_preds),  \n",
    "              # \"Test Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Test Set MAE\": mean_absolute_error(Y_test, test_preds), \n",
    "              # \"Test Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Tes Set RMSE\": root_mean_squared_error(Y_test, test_preds),\n",
    "              # \"Validation Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),             \n",
    "              # \"Validation Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['Metric', 'Value'])    \n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_excel(f'data/results/{target}/{model_name}_scores.xlsx', index=False)\n",
    "    return scores\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_plot(model, ST_X_train, ST_Y_train, ST_X_test, ST_Y_test, ST_X_validation, ST_Y_validation, name, std):\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_train_preds = model.predict(ST_X_train)\n",
    "    # Change train predictions to pandas series\n",
    "    ST_Y_train_preds_series = pd.Series(ST_Y_train_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_train_preds_series.index = ST_Y_train.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_train_sorted = ST_Y_train.sort_values().reset_index(drop=True)\n",
    "    ST_Y_train_preds_sorted = ST_Y_train_preds_series[ST_Y_train.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_train_mae = mean_absolute_error(ST_Y_train, ST_Y_train_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_train_rmse = root_mean_squared_error(ST_Y_train,ST_Y_train_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_train_r2_score = r2_score(ST_Y_train,ST_Y_train_preds)\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "    # Change validation predictions to pandas series\n",
    "    ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "    ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_valid_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_valid_rmse = root_mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_valid_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "\n",
    "    # Predict the test set which is forecast data\n",
    "    ST_Y_test_preds = model.predict(ST_X_test)\n",
    "    # Changes the predicted array values to pandas series\n",
    "    ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "    ST_Y_test_preds_series.index =ST_Y_test.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_test_sorted = ST_Y_test.sort_values().reset_index(drop=True)\n",
    "    ST_Y_test_preds_sorted = ST_Y_test_preds_series[ST_Y_test.index].sort_values().reset_index(drop=True)\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    ST_test_mae = mean_absolute_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate mean squared error\n",
    "    ST_test_rmse = root_mean_squared_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_test_r2_score = r2_score(ST_Y_test,ST_Y_test_preds)\n",
    "    \n",
    "    # Convert the Series to a DataFrame to return as dataframe\n",
    "    ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "    ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "\n",
    "\n",
    "     # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_train_sorted.index,ST_Y_train_sorted, color='blue', label=f'{name} Training Observed Values')\n",
    "    plt.plot(ST_Y_train_preds_sorted.index,ST_Y_train_preds_sorted, color='red', label=f'{name} Training Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_train_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_train_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_train_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 10 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Training Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_train_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Validation Set Observed Values')\n",
    "    plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Validation Set Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_valid_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_valid_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_valid_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 10 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Validation Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_valid_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the test sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_test_sorted.index,ST_Y_test_sorted, color='blue', label=f'{name} Test Observed Values')\n",
    "    plt.plot(ST_Y_test_preds_sorted.index,ST_Y_test_preds_sorted, color='red', label=f'{name} Test Predicted Values')\n",
    "    # Display the metrics as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_test_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_test_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_test_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 10 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Final Test Scores For {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_test_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()    \n",
    "    return ST_Y_test_preds_df\n",
    "    \n",
    "\n",
    "std_deviation = dataset_denormalized_outlier_filtered['ST10'].std()\n",
    "# Split the dataset into features and target\n",
    "many_features_dropped = ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','ST100','ST50','ST20','ST10','ID']\n",
    "soil_features_dropped = ['ST10','ST20','ST50','ST100','ID']\n",
    "uncorrelated_dropped = ['ST10','ID']\n",
    "# ST10_X = dataset_shuffled.drop(many_features_dropped, axis=1)\n",
    "ST10_X = dataset_shuffled.drop(soil_features_dropped, axis=1)\n",
    "ST10_Y = dataset_shuffled['ST10']\n",
    "\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_2cm ).\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(dataset_shuffled))\n",
    "ST10_X_train, ST10_Y_train = ST10_X[:train_split], ST10_Y[:train_split]\n",
    "ST10_X_valid, ST10_Y_valid =ST10_X[train_split:valid_split], ST10_Y[train_split:valid_split]\n",
    "ST10_X_test, ST10_Y_test = ST10_X[valid_split:], ST10_Y[valid_split:]\n",
    "\n",
    "# A. CatBoostRegressor (CB)\n",
    "# Create CB model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_cb_model_stack = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "# Fit the model for ST10 to start with\n",
    "ST10_cb_model_stack.fit(ST10_X_train, ST10_Y_train, eval_set=(ST10_X_valid, ST10_Y_valid), early_stopping_rounds=100)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================CatBoost The Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST10_cb_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'CB'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# B. RandomForestRegressor\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_rf_model_stack = RandomForestRegressor(n_estimators=300, \n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         max_features='sqrt',\n",
    "                                         max_depth=None,\n",
    "                                         bootstrap=False,\n",
    "                                         random_state=42)\n",
    "# Fit the model for ST10 to start with\n",
    "ST10_rf_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================Random Forest The Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST10_rf_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'RF'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# C. Histogram Based Gradient Boosting Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_gbr_model_stack = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "# Fit the ST10 model for soil temp at 100 cm\n",
    "ST10_gbr_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Histogram-Based Gradient Boosting Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_gbr_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'HGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# D. XGBoost Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create XGBoost for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_xgb_model_stack = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "# Fit the ST10 model for soil temp at 100 cm\n",
    "ST10_xgb_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The XGBoost Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_xgb_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'XGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# E. AdaBoostRegressor \n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create AdaBoost Regressor for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_adb_model_stack = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "# Fit the ST10 model for soil temp at 100 cm\n",
    "ST10_adb_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The AdaBoost Regressor Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_adb_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'ADB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# F. Ridge Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_rg_model_stack = Ridge(random_state=42)\n",
    "# Fit the ST10 model for soil temp at 100 cm\n",
    "ST10_rg_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Ridge Regressor Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_rg_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'RR'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# G. Lasso Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create Lasso model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_la_model_stack = Lasso(random_state=42)\n",
    "# Fit the ST10 model for soil temp at 100cm\n",
    "ST10_la_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Lasso Regressor Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_la_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'LA'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# H. ElasticNet Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_en_model_stack = ElasticNet(random_state=42)\n",
    "# Fit the ST10 model for soil temp at 100cm\n",
    "ST10_en_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The ElasticNet Regressor Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_en_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'EN'))\n",
    "print(\"=========================================================================================================\\n\")\n",
    "\n",
    "# I. SVR-L Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-L model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_svrl_model_stack = SVR(kernel='linear')\n",
    "# Fit the ST10 model for soil temp at 100cm\n",
    "ST10_svrl_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with linear model Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_svrl_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'SVR-L'))\n",
    "print(\"==========================================================================================================\\n\")\n",
    "\n",
    "# J. SVR-R Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-R model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST10_svrr_model_stack = SVR(kernel='rbf')\n",
    "# Fit the ST10 model for soil temp at 100cm\n",
    "ST10_svrr_model_stack.fit(ST10_X_train, ST10_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with rfb model Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_svrr_model_stack, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'SVR-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# Stack of predictors on a single data set\n",
    "ST10_rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_samples_split=2,\n",
    "                                     max_features='sqrt',\n",
    "                                     max_depth=None,\n",
    "                                     bootstrap=False,\n",
    "                                     random_state=42)\n",
    "ST10_gbdt_regresssor = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "ST10_xgb_model = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "ST10_cb_regressor = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "ST10_adb_regressor = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    (\"RandomForest\", ST10_rf_regressor),\n",
    "    (\"CatBoost\", ST10_cb_regressor),\n",
    "    (\"HistGradientBoosting\", ST10_gbdt_regresssor),\n",
    "    (\"XGBoost\", ST10_xgb_model)\n",
    "]\n",
    "ST10_stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\n",
    "\n",
    "# Measure and plot the results\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for ax, (name, est) in zip(axs, estimators + [(\"Stacking Regressor\", ST5_stacking_regressor)]):\n",
    "    scorers = {\"R^2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\", \"RMSE\": \"neg_root_mean_squared_error\"}\n",
    "\n",
    "    start_time = time.time()\n",
    "    scores = cross_validate(est, ST10_X_train, ST10_Y_train, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_pred = cross_val_predict(est, ST10_X_valid, ST10_Y_valid, n_jobs=-1, verbose=0)\n",
    "    y_test = cross_val_predict(est, ST10_X_test, ST10_Y_test, n_jobs=-1, verbose=0)\n",
    "    \n",
    "    # Calculate mean and std_deviation for each scorer\n",
    "    scores_mean_std = {\n",
    "        key: (np.abs(np.mean(scores[f'test_{value}'])), np.std(scores[f'test_{value}']))\n",
    "        for key, value in scorers.items()\n",
    "    }\n",
    "\n",
    "    # Format the scores\n",
    "    formatted_scores = {\n",
    "        key: f\"{mean:.4f} ± {std_dev:.4f}\"\n",
    "        for key, (mean, std_dev) in scores_mean_std.items()\n",
    "    }\n",
    "\n",
    "    display = PredictionErrorDisplay.from_predictions(\n",
    "        y_true=ST10_Y_valid,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        ax=ax,\n",
    "        scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "        line_kwargs={\"color\": \"tab:red\"},\n",
    "    )\n",
    "    ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.4f} seconds\", fontsize=14)\n",
    "    # Set custom x-label and y-label\n",
    "    ax.set_xlabel(\"Predicted Soil Temperature at 10 cm (°C)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Observed Soil Temperature at 10 cm (°C)\", fontsize=14)\n",
    "\n",
    "    for metric_name, (mean, std_dev) in scores_mean_std.items():\n",
    "        if metric_name == 'R^2':\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {formatted_scores[metric_name]}\")\n",
    "        else:\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {mean:.4f} ± {std_dev:.4f}\")\n",
    "    \n",
    "    ax.legend(loc=\"best\", fontsize='small')\n",
    "    # Save the mean and std scores to an Excel file\n",
    "    df_scores_summary = pd.DataFrame(scores_mean_std).T\n",
    "    df_scores_summary.columns = ['Train Mean', 'Train Std Dev']\n",
    "    df_scores_summary.to_excel(f'data/results/ST10/{name}_cv_scores.xlsx', index=True)\n",
    "# Hide any unused subplots\n",
    "for i in range(len(estimators)+1, len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "# Apply tight layout\n",
    "plt.tight_layout()\n",
    "# Save the entire figure with all subplots to a file\n",
    "fig.savefig('data/results/ST10/stacked_regressors_prediction_error_plots.png', bbox_inches='tight')\n",
    "\n",
    "# Sort actual values and get sorted indices\n",
    "ST10_Y_valid_sorted = ST10_Y_valid.sort_values()\n",
    "sorted_indices = ST10_Y_valid_sorted.index\n",
    "\n",
    "# Reorder y_pred using the sorted indices\n",
    "y_pred_sorted = pd.Series(y_pred, index=ST10_Y_valid.index).loc[sorted_indices]\n",
    "\n",
    "# Calculate metrics for the validation set predictions\n",
    "mae_valid = mean_absolute_error(ST10_Y_valid, y_pred)\n",
    "rmse_valid = np.sqrt(mean_squared_error(ST10_Y_valid, y_pred))\n",
    "r2_valid = r2_score(ST10_Y_valid, y_pred)\n",
    "\n",
    "# Calculate metrics for the test set predictions\n",
    "mae_test = mean_absolute_error(ST10_Y_test, y_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(ST10_Y_test, y_test))\n",
    "r2_test = r2_score(ST10_Y_test, y_test)\n",
    "\n",
    "# Save the validation metrics to an Excel file\n",
    "validation_test_metrics = {\n",
    "    'V_R^2': [r2_valid],\n",
    "    'V_MAE': [mae_valid],\n",
    "    'V_RMSE': [rmse_valid],\n",
    "    'T_R^2': [r2_test],\n",
    "    'T_MAE': [mae_test],\n",
    "    'T_RMSE': [rmse_test]    \n",
    "}\n",
    "# Save the validation metrics to an Excel file\n",
    "\n",
    "df_metrics = pd.DataFrame(validation_test_metrics)\n",
    "df_metrics.to_excel(f'data/results/ST10/{name}_validation_test_metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the sorted actual values and corresponding predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ST10_Y_valid_sorted.values, color='blue', label='Observed Values')\n",
    "plt.plot(y_pred_sorted.values, color='red', label='Predicted Values')\n",
    "\n",
    "# Display the metrics as text annotation\n",
    "plt.text(0.1, 0.75, f'MAE: {mae_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.3, 0.75, f'RMSE: {rmse_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.5, 0.75, f'R^2: {r2_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel('Index', fontsize=14)\n",
    "plt.ylabel('Soil Temperature at 10 cm (°C)', fontsize=14)\n",
    "plt.title(f'STACK-R model\\'s validation set\\'s predicted vs observed values for ST10', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'data/results/ST10/{name}_cross_validation_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "dump(ST10_stacking_regressor, filename=\"data/results/ST10/ST10_STACK-R_trained_model.joblib\");\n",
    "# Fit the stacking regressor for direct fitting and prediction for all sets at default CV=5\n",
    "ST10_stacking_regressor.fit(ST10_X_train, ST10_Y_train)\n",
    "print(\"====================The Stacking Regressor Evaluation Metrics Results For ST10 Denormalized =======================\\n\")\n",
    "print(show_scores(ST10_stacking_regressor, ST10_X_train, ST10_X_valid, ST10_X_test, ST10_Y_train, ST10_Y_valid, ST10_Y_test, std_deviation,'ST10', 'STACK-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "# ST10_Y_test_preds_df = predict_plot(ST10_stacking_regressor, ST10_X_train, ST10_Y_train, ST10_X_test, ST10_Y_test, ST10_X_valid, ST10_Y_valid, 'ST10', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd7c73-7932-4ace-9cca-1a66ee6bbdf1",
   "metadata": {},
   "source": [
    "### Cross-validation to check stability of the stacking regressor for ST10\n",
    "### NOTE: This will take time!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c5c11-874f-4a43-8d74-457a8feac72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "ST10_cv_scores = cross_val_score(ST10_stacking_regressor, ST10_X_train, ST10_Y_train, cv=kf, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive\n",
    "ST10_cv_scores = -ST10_cv_scores\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-Validation Scores (MSE):\", ST10_cv_scores)\n",
    "print(\"Mean CV Score (MSE):\", np.mean(ST10_cv_scores))\n",
    "print(\"Standard Deviation of CV Scores:\", np.std(ST10_cv_scores))\n",
    "# Save the scores to an Excel file\n",
    "ST10_cv_scores_df = pd.DataFrame(ST10_cv_scores, columns=['MSE'])\n",
    "ST10_cv_scores_df.to_excel('data/results/ST10/ST10_10_fold_cv_scores.xlsx', index=False)\n",
    "\n",
    "##=========== Visualize the problematic Fold using histogram==================\n",
    "# Calculate mean MSE\n",
    "ST10_mean_mse = np.mean(ST10_cv_scores)\n",
    "# Identify the problematic fold\n",
    "ST10_problematic_fold_index = np.argmax(np.abs(ST10_cv_scores - ST10_mean_mse))\n",
    "# Get the indices of the data points in the problematic fold\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(ST10_X_train)):\n",
    "    if fold_index == ST10_problematic_fold_index:\n",
    "        problematic_fold_train_indices = train_index\n",
    "        problematic_fold_test_indices = test_index\n",
    "\n",
    "# Subset the data for the problematic fold\n",
    "X_problematic_fold = ST10_X_train.iloc[problematic_fold_test_indices]\n",
    "y_problematic_fold = ST10_Y_train.iloc[problematic_fold_test_indices]\n",
    "# Visualize or analyze features for the problematic fold\n",
    "for feature in ST10_X_train.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Histogram for the problematic fold\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_problematic_fold[feature], kde=True, bins=20, color='red')\n",
    "    plt.title(f'{feature} - Problematic Fold')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    # Histogram for the entire dataset\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(ST10_X_train[feature], kde=True, bins=20, color='blue')\n",
    "    plt.title(f'{feature} - Entire Dataset')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/results/ST10/ST10_CV_problematic_10_fold_vs_main_dataset_histograms.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b85f6a-3927-4a0a-879d-95006a0c3437",
   "metadata": {},
   "source": [
    "### Partial Dependence, Individual Conditional Expectation and Residual Analysis Plots for ST10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be78c64-d693-4e59-a4bc-29058fe766da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import statsmodels.api as sm\n",
    "from pycebox.ice import ice, ice_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# # Partial Dependence Plot\n",
    "# print('====================================================== ST10 Partial Dependence Plot')\n",
    "# ST10_feature_names = ST10_X_train.columns.tolist()\n",
    "# n_features = len(ST10_feature_names)\n",
    "# n_cols = 2\n",
    "# n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "# fig1, ax1 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat = ax1.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST10_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST10_stacking_regressor, ST10_X_train, features=[feature])\n",
    "#     display.plot(ax=axes_flat[idx])\n",
    "#     axes_flat[idx].set_title(f'Partial Dependence (PD) Plot for {feature}')\n",
    "#     axes_flat[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat)):\n",
    "#     fig1.delaxes(axes_flat[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "# plt.suptitle('Partial Dependence (PD) Plot', fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# # plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST10/ST10_final_partial_dependence_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Individual Conditional Expectation Plot (ICE)\n",
    "# print('======================== ST10 Individual Conditional Expectation Plot ===============================')\n",
    "# fig2, axes2 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat2 = axes2.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST10_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST10_stacking_regressor, ST10_X_train, features=[feature], kind='individual')\n",
    "#     display.plot(ax=axes_flat2[idx])\n",
    "#     axes_flat2[idx].set_title(f'Individual Conditional Expectation (ICE) Plot for {feature}')\n",
    "#     axes_flat2[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat2)):\n",
    "#     fig2.delaxes(axes_flat2[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.7)\n",
    "# plt.suptitle('Individual Conditional Expectation (ICE) Plot', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST10/ST10_final_individual_conditional_expectation_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# Residual Analysis\n",
    "print('========================= ST10 Residual Analysis Plot ==============================')\n",
    "ST10_Y_predictions = ST10_stacking_regressor.predict(ST10_X_test)\n",
    "ST10_residuals = ST10_Y_test - ST10_Y_predictions\n",
    "\n",
    "# Calculate the interquartile range (IQR)\n",
    "Q1 = np.percentile(ST10_residuals, 25)\n",
    "Q3 = np.percentile(ST10_residuals, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the whisker range\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Count outliers\n",
    "outliers = np.sum((ST10_residuals < lower_bound) | (ST10_residuals > upper_bound))\n",
    "total_residuals = len(ST10_residuals)\n",
    "outlier_percentage = (outliers / total_residuals) * 100\n",
    "\n",
    "# Plotting the residuals scatter plot and box-and-whisker plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Residuals analysis plot\n",
    "ax[0].scatter(ST10_Y_predictions, ST10_residuals)\n",
    "ax[0].set_xlabel('Predictions', fontsize=14)\n",
    "ax[0].set_ylabel('Residuals', fontsize=14)\n",
    "ax[0].set_title('ST10 Residuals Analysis Plot', fontsize=14)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax[0].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# Box-and-whisker plot for residuals\n",
    "sns.boxplot(y=ST10_residuals, ax=ax[1])\n",
    "ax[1].set_title('ST10 Box-and-Whisker Plot of Residuals', fontsize=14)\n",
    "ax[1].set_ylabel('ST10 Residuals', fontsize=14)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# Annotate the plot with the number of outliers and total residuals\n",
    "annotation_text = (f'Total Residuals: {total_residuals}\\n'\n",
    "                   f'Number of Outliers: {outliers}\\n'\n",
    "                   f'Percentage of Outliers: {outlier_percentage:.2f}%')\n",
    "ax[1].annotate(annotation_text, xy=(0.8, 0.85), xycoords='axes fraction',\n",
    "               fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST10/ST10_final_residual_and_boxplot_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predictor Variables\n",
    "print('========================= Residuals vs. Predictor Variables ==============================')\n",
    "for column in ST10_X_test.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.scatter(ST10_X_test[column], ST10_residuals)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel(column, fontsize=14)\n",
    "    ax.set_ylabel('Residuals', fontsize=14)\n",
    "    ax.set_title(f'Residuals vs. {column}', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/results/ST10_final_residuals_vs_{column}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the Box Plot of all features\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Number of features in the DataFrame\n",
    "num_features = dataset_denormalized_outlier_filtered.shape[1]\n",
    "# Calculate the number of rows needed to plot all features in 3 columns\n",
    "num_cols = 3\n",
    "num_rows = math.ceil(num_features / num_cols)\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "# Define colors for each column\n",
    "colors = ['green', 'purple', 'red']\n",
    "# Create a Box Plot for each feature\n",
    "for i, column in enumerate(dataset_denormalized_outlier_filtered.columns):\n",
    "    col_index = i % num_cols  # Determine the column index (0, 1, or 2)\n",
    "    sns.boxplot(data=dataset_denormalized_outlier_filtered[column], ax=axes[i], color=colors[col_index])\n",
    "    axes[i].set_title(f'Box Plot for {column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Values', fontsize=14)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[i].tick_params(axis='both', which='minor', labelsize=12)\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST10/ST10_Box_plot_of_features.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# # Q-Q Plot\n",
    "# print('========================= ST10 Q-Q Plot ==============================')\n",
    "# fig5, ax5 = plt.subplots(figsize=(10, 7))\n",
    "# sm.qqplot(ST10_residuals, line='45', ax=ax5)\n",
    "# ax5.set_title('Q-Q Plot of Residuals')\n",
    "# plt.savefig('data/results/ST10/ST10_final_Q-Q_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Histogram of residuals\n",
    "# fig6, ax6 = plt.subplots(figsize=(10, 7))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(residuals, kde=True, ax=ax6)\n",
    "# plt.xlabel('Residuals')\n",
    "# plt.title('Histogram of Residuals')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a517c-f115-423b-a53d-a8442630b857",
   "metadata": {},
   "source": [
    "### Feature importance analysis for ST10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a38db5-2923-43b8-9e51-2d4fd179bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ST10_X_train and ST10_Y_train are your training data\n",
    "\n",
    "# Fit the stacking regressor\n",
    "ST10_stacking_regressor.fit(ST10_X_train, ST10_Y_train)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = ST10_X_train.columns\n",
    "\n",
    "# Initialize an array to store feature importances\n",
    "feature_importances = np.zeros(ST10_X_train.shape[1])\n",
    "\n",
    "# Function to extract feature importances\n",
    "def get_feature_importance(model, X, y):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return np.abs(model.coef_)\n",
    "    elif isinstance(model, CatBoostRegressor):\n",
    "        return model.get_feature_importance()\n",
    "    else:\n",
    "        # Use permutation importance as a fallback for models without direct attribute\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        return result.importances_mean\n",
    "\n",
    "# Aggregate feature importances\n",
    "for name, model in ST10_stacking_regressor.named_estimators_.items():\n",
    "    importances = get_feature_importance(model, ST10_X_train, ST10_Y_train)\n",
    "    feature_importances += importances\n",
    "\n",
    "# Normalize the aggregated feature importances\n",
    "feature_importances /= len(ST10_stacking_regressor.named_estimators_)\n",
    "\n",
    "# Convert importances to percentage\n",
    "feature_importances_percentage = 100 * (feature_importances / np.sum(feature_importances))\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances_percentage\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('ST10 Stacking Regressor Feature Importances')\n",
    "\n",
    "# Add annotations\n",
    "for index, value in enumerate(importance_df['Importance']):\n",
    "    plt.text(value, index, f'{value:.2f}%', va='center')\n",
    "\n",
    "plt.savefig('data/results/ST10/ST10_stacking_regressor_feature_importances.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5920ffe-f3fa-4712-91fb-6942c0117532",
   "metadata": {},
   "source": [
    "### Learning curve evaluations for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81d54f-093c-47f6-b5d8-347380ca0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def plot_learning_curves(model, X_train, Y_train, X_valid, Y_valid, feature):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MAE to positive\n",
    "    train_errors_mae = -train_scores.mean(axis=1)\n",
    "    val_errors_mae = -valid_scores.mean(axis=1)\n",
    "    \n",
    "    train_sizes_mse, train_scores_mse, valid_scores_mse = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MSE to positive\n",
    "    train_errors_mse = -train_scores_mse.mean(axis=1)\n",
    "    val_errors_mse = -valid_scores_mse.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot MAE learning curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_sizes, train_errors_mae, \"r-\", label=\"Training MAE\")\n",
    "    plt.plot(train_sizes, val_errors_mae, \"b-\", label=\"Validation MAE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"MAE Learning Curve\")\n",
    "    \n",
    "    # Plot MSE learning curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_sizes, train_errors_mse, \"r-\", label=\"Training RMSE\")\n",
    "    plt.plot(train_sizes, val_errors_mse, \"b-\", label=\"Validation RMSE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE Learning Curve\")    \n",
    "    plt.savefig('data/results/ST10/ST10_learning_curves.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example call (ensure your data and model are defined correctly)\n",
    "plot_learning_curves(ST10_stacking_regressor, ST10_X_train, ST10_Y_train, ST10_X_valid, ST10_Y_valid, \"ST10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea06168-7fe7-454a-a051-c3a09c4e9293",
   "metadata": {},
   "source": [
    "### GridSearhCV Evaluation for all models used in the stacked regressor for ST10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5eb01-754f-4f07-b703-68483e869439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_cb = {\n",
    "    'iterations': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'depth': [4, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 50, 100]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    'max_leaf_nodes': [31, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [20, 50, 100],\n",
    "    'l2_regularization': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [1.0, 0.8, 0.6],\n",
    "    'colsample_bytree': [1.0, 0.8, 0.6],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 0.1, 0.01],\n",
    "    'tree_method': ['gpu_hist']  # Use GPU\n",
    "}\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# stacking_param_grid ={\n",
    "#     'rf__n_estimators': [100, 300, 500],\n",
    "#     'rf__max_depth': [None, 10, 20, 30],\n",
    "#     'hgb__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'hgb__max_iter': [100, 200, 500],\n",
    "#     'catboost__iterations': [100, 200, 500],\n",
    "#     'catboost__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'catboost__depth': [4, 6, 10],\n",
    "#     'xgb__n_estimators': [100, 200],\n",
    "#     'xgb__max_depth': [3, 5]\n",
    "# }\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "ada = AdaBoostRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestRegressor()),\n",
    "#     ('cb', CatBoostRegressor()),\n",
    "#     ('hgb', HistGradientBoostingRegressor()),    \n",
    "#     ('xgb', XGBRegressor())\n",
    "# ]\n",
    "\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=RidgeCV()\n",
    "# )\n",
    "\n",
    "# Initialize GridSearchCV for RF model\n",
    "start_time_rf = time.time()\n",
    "ST10_grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_rf_search = time.time()\n",
    "ST10_grid_search_rf.fit(ST10_X_train, ST10_Y_train)\n",
    "end_time_rf_fit = time.time()\n",
    "elapsed_time_search_rf = end_time_rf_search - start_time_rf\n",
    "elapsed_time_fit_rf = end_time_rf_fit - end_time_rf_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for RandomForestRegressor:\", ST10_grid_search_rf.best_params_)\n",
    "print(\"Best score for RandomForestRegressor:\", -ST10_grid_search_rf.best_score_)\n",
    "print(\"RandomForestRegressor GridSearchCV Time:\", elapsed_time_search_rf)\n",
    "print(\"RandomForestRegressor Fitting Time:\", elapsed_time_fit_rf)\n",
    "\n",
    "# Initialize GridSearchCV for HGB model\n",
    "start_time_hgb = time.time()\n",
    "ST10_grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=param_grid_hgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_hgb_search = time.time()\n",
    "ST10_grid_search_hgb.fit(ST10_X_train, ST10_Y_train)\n",
    "end_time_hgb_fit = time.time()\n",
    "elapsed_time_search_hgb = end_time_hgb_search - start_time_hgb\n",
    "elapsed_time_fit_hgb = end_time_hgb_fit - end_time_hgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for HistGradientBoostingRegressor:\", ST10_grid_search_hgb.best_params_)\n",
    "print(\"Best score for HistGradientBoostingRegressor:\", -ST10_grid_search_hgb.best_score_)\n",
    "print(\"HistGradientBoostingRegressor GridSearchCV Time:\", elapsed_time_search_hgb)\n",
    "print(\"HistGradientBoostingRegressor Fitting Time:\", elapsed_time_fit_hgb)\n",
    "\n",
    "# # Initialize GridSearchCV for XGB model\n",
    "# start_time_xgb = time.time()\n",
    "# ST10_grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "# end_time_xgb_search = time.time()\n",
    "# ST10_grid_search_xgb.fit(ST10_X_train, ST10_Y_train)\n",
    "# end_time_xgb_fit = time.time()\n",
    "# elapsed_time_search_xgb = end_time_xgb_search - start_time_xgb\n",
    "# elapsed_time_fit_xgb = end_time_xgb_fit - end_time_xgb_search\n",
    "# # Get the best parameters and scores\n",
    "# print(\"Best parameters for XGBRegressor:\", ST10_grid_search_xgb.best_params_)\n",
    "# print(\"Best score for XGBRegressor:\", -ST10_grid_search_xgb.best_score_)\n",
    "# print(\"XGBRegressor GridSearchCV Time:\", elapsed_time_search_xgb)\n",
    "# print(\"XGBRegressor Fitting Time:\", elapsed_time_fit_xgb)\n",
    "\n",
    "# Initialize GridSearchCV for ADA model\n",
    "start_time_ada = time.time()\n",
    "ST10_grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_ada_search = time.time()\n",
    "ST10_grid_search_ada.fit(ST10_X_train, ST10_Y_train)\n",
    "end_time_ada_fit = time.time()\n",
    "elapsed_time_search_ada = end_time_ada_search - start_time_ada\n",
    "elapsed_time_fit_ada = end_time_ada_fit - end_time_ada_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for AdaBoostRegressor:\", ST10_grid_search_ada.best_params_)\n",
    "print(\"Best score for AdaBoostRegressor:\", -ST10_grid_search_ada.best_score_)\n",
    "print(\"AdaBoostRegressor GridSearchCV Time:\", elapsed_time_search_ada)\n",
    "print(\"AdaBoostRegressor Fitting Time:\", elapsed_time_fit_ada)\n",
    "\n",
    "# # Initialize GridSearchCV for CB model\n",
    "# start_time_cb = time.time()\n",
    "# ST10_grid_search_cb = GridSearchCV(estimator=cb, param_grid=param_grid_cb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "# end_time_cb_search = time.time()\n",
    "# ST10_grid_search_cb.fit(ST10_X_train, ST10_Y_train)\n",
    "# end_time_cb_fit = time.time()\n",
    "# elapsed_time_search_cb = end_time_cb_search - start_time_cb\n",
    "# elapsed_time_fit_cb = end_time_cb_fit - end_time_cb_search\n",
    "# # Get the best parameters and scores\n",
    "# print(\"Best parameters for CatBoostRegressor:\", ST10_grid_search_cb.best_params_)\n",
    "# print(\"Best score for CatBoost:\", -ST10_grid_search_cb.best_score_)\n",
    "# print(\"CatBoostRegressor GridSearchCV Time:\", elapsed_time_search_cb)\n",
    "# print(\"CatBoostRegressor Fitting Time:\", elapsed_time_fit_cb)\n",
    "\n",
    "# # Initialize GridSearchCV for Stacking model\n",
    "# start_time_stacking = time.time()\n",
    "# ST10_grid_search_stacking = GridSearchCV(estimator=stacking_regressor, param_grid=stacking_param_grid, cv=5, n_jobs=-1, verbose=0,scoring='neg_mean_absolute_error')\n",
    "# end_time_stacking_search = time.time()\n",
    "# ST10_grid_search_stacking.fit(ST10_X_train, ST10_Y_train)\n",
    "# end_time_stacking = time.time()\n",
    "# elapsed_time_search_stacking = end_time_stacking_search - start_time_stacking\n",
    "# elapsed_time_fit_stacking = end_time_stacking_fit - end_time_stacking_search\n",
    "\n",
    "# print(\"Best parameters for StackingRegressor:\", ST10_grid_search_stacking.best_params_)\n",
    "# print(\"Best score for StackingRegressor:\", -ST10_grid_search_stacking.best_score_)\n",
    "# print(\"StackingRegressor GridSearchCV Time:\", elapsed_time_search_stacking)\n",
    "# print(\"StackingRegressor Fitting Time:\", elapsed_time_fit_stacking)\n",
    "\n",
    "# # Define the results of print statements as variables\n",
    "# ST10_grid_search_and_fitting_results = {\n",
    "#     'Model': ['RandomForestRegressor', 'HistGradientBoostingRegressor', 'AdaBoostRegressor', 'XGBRegressor'],\n",
    "#     'Best Parameters': [ST10_grid_search_rf.best_params_, ST10_grid_search_hgb.best_params_, ST10_grid_search_ada.best_params_, ST10_grid_search_xgb.best_params_],\n",
    "#     'Best Score': [-ST10_grid_search_rf.best_score_, -ST10_grid_search_hgb.best_score_, -ST10_grid_search_ada.best_score_, -ST10_grid_search_xgb.best_score_],\n",
    "#     'GridSearchCV Time': [elapsed_time_search_rf, elapsed_time_search_hgb, elapsed_time_search_ada, elapsed_time_search_xgb],\n",
    "#     'Fitting Time': [elapsed_time_fit_rf, elapsed_time_fit_hgb, elapsed_time_fit_ada, elapsed_time_fit_xgb]\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df_results = pd.DataFrame(ST10_grid_search_and_fitting_results)\n",
    "\n",
    "# # Export DataFrame to Excel\n",
    "# df_results.to_excel('data/results/ST10/ST10_grid_search_and_fitting_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c99c9a-94c0-4755-990a-4928262f4a37",
   "metadata": {},
   "source": [
    "### D. Stacking Regressor for Soil temperature at 20cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe804a2-c324-4680-85b2-b9d674342a46",
   "metadata": {},
   "source": [
    "### Correllation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10160784-bef3-4122-9176-6ce34e2c0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the covariance matrix for target ST200\n",
    "ST20_dataset_correlation = ST20_clean_dataset_denormalized.drop(['ST100', 'ST50','ST20'], axis=1)\n",
    "ST20_covariance_matrix = ST20_dataset_correlation.cov()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "ST20_correlation_matrix = ST20_dataset_correlation.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST20_correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST20/ST20_denormalized_before_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Set the threshold\n",
    "threshold = 0.95\n",
    "# Find pairs of features with correlation above the threshold\n",
    "highly_correlated = np.where(np.abs(ST20_correlation_matrix) > threshold)\n",
    "highly_correlated_pairs = [(ST20_correlation_matrix.index[x], ST20_correlation_matrix.columns[y]) \n",
    "                           for x, y in zip(*highly_correlated) if x != y and x < y]\n",
    "\n",
    "print(\"Highly correlated pairs (above threshold):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)\n",
    "# Example: Removing one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for pair in highly_correlated_pairs:\n",
    "    features_to_remove.add(pair[1])  # You can choose to remove pair[0] or pair[1]\n",
    "\n",
    "# Drop the features from the dataframe\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated = ST20_dataset_correlation.drop(columns=features_to_remove)\n",
    "\n",
    "print(f\"Removed features: {features_to_remove}\")\n",
    "print(\"Shape of the reduced dataset:\", ST20_dataset_denormalized_outlier_filtered_uncorrelated.shape)\n",
    "\n",
    "# After removing the correlated features\n",
    "# Calculate the correlation matrix\n",
    "ST20_correlation_matrix_new = ST20_dataset_denormalized_outlier_filtered_uncorrelated.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST20_correlation_matrix_new, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST20/ST20_denormalized_after_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Assuming dataset_denormalized_outlier_filtered is your DataFrame\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST20_dataset_denormalized_outlier_filtered_uncorrelated.copy()\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = sm.add_constant(ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(data):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Iteratively remove features with VIF above the threshold\n",
    "def remove_high_vif_features(data, threshold=40.0):\n",
    "    while True:\n",
    "        vif_data = calculate_vif(data)\n",
    "        max_vif = vif_data['VIF'].max()\n",
    "        if max_vif > threshold:\n",
    "            # Identify the feature with the highest VIF\n",
    "            feature_to_remove = vif_data.sort_values('VIF', ascending=False)['feature'].iloc[0]\n",
    "            print(f\"Removing feature '{feature_to_remove}' with VIF: {max_vif}\")\n",
    "            data = data.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "    return data, vif_data\n",
    "\n",
    "# Remove high VIF features\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif, ST20_final_vif_data = remove_high_vif_features(ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "\n",
    "print(\"Final VIF data:\")\n",
    "print(ST20_final_vif_data)\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID'] = ST20_clean_dataset_denormalized['ID']\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST20'] = ST20_clean_dataset_denormalized['ST20']\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated['ID'] = ST20_clean_dataset_denormalized['ID']\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated['ST20'] = ST20_clean_dataset_denormalized['ST20']\n",
    "# Remove the constant term before creating the final DataFrame\n",
    "if 'const' in ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.columns:\n",
    "    ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(columns=['const'])\n",
    "\n",
    "# Store the 'ID' and 'ST20' columns with their corresponding index before PCA\n",
    "ID_index_mapping = ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID']\n",
    "ST20_index_mapping = ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST20']\n",
    "\n",
    "# Assume X is your feature dataframe\n",
    "ST20_X_pca = ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(['ST20', 'ID'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ST20_X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)  # Choose the number of components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "ST20_pca_df = pd.DataFrame(data=principal_components, columns=[f\"PC{i}\" for i in range(principal_components.shape[1])])\n",
    "\n",
    "# Merge PCA DataFrame with original DataFrame to maintain original index order\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ID_index_mapping, ST20_index_mapping, left_index=True, right_index=True)\n",
    "ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca, ST20_pca_df, left_index=True, right_index=True)\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.savefig('data/results/ST20/ST20_PCA_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f083ba7-4959-4692-a908-48f5ef49a88f",
   "metadata": {},
   "source": [
    "### Option 1:  ST20 Prediction by varying the dataset cases\n",
    "#### Note: Choose the dataset case at this line of the code: dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1)\n",
    "#### Dataset Cases:\n",
    "##### Case 1. dataset_denormalized_outlier_filtered\n",
    "##### case 2. ST20_clean_dataset_denormalized\n",
    "##### case 3. ST20_dataset_denormalized_outlier_filtered_uncorrelated\n",
    "##### case 4. ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif\n",
    "##### case 5. ST20_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454955ee-a7f0-46bd-9cc5-19484a7f8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR  \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Choose any of the time-independent dataset cases\n",
    "# Select the dataset case here\n",
    "dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1) # Choose dataset case here\n",
    "  \n",
    "# Create function to evaluate model on few different levels\n",
    "def show_scores(model, X_train, X_valid, X_test, Y_train, Y_valid, Y_test, std, target='ST20', model_name='RF'):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    test_preds = model.predict(X_test)\n",
    "    scores = {\n",
    "              # \"Validation Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_test, test_preds),   \n",
    "              # \"Validation Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds), \n",
    "              # \"Validation Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set RMSE\": root_mean_squared_error(Y_valid, val_preds),\n",
    "              # \"Test Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Test Set R^2 Score\":r2_score(Y_valid, val_preds),  \n",
    "              # \"Test Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Test Set MAE\": mean_absolute_error(Y_test, test_preds), \n",
    "              # \"Test Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Tes Set RMSE\": root_mean_squared_error(Y_test, test_preds),\n",
    "              # \"Validation Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),             \n",
    "              # \"Validation Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['Metric', 'Value'])    \n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_excel(f'data/results/{target}/{model_name}_scores.xlsx', index=False)\n",
    "    return scores\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_plot(model, ST_X_train, ST_Y_train, ST_X_test, ST_Y_test, ST_X_validation, ST_Y_validation, name, std):\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_train_preds = model.predict(ST_X_train)\n",
    "    # Change train predictions to pandas series\n",
    "    ST_Y_train_preds_series = pd.Series(ST_Y_train_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_train_preds_series.index = ST_Y_train.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_train_sorted = ST_Y_train.sort_values().reset_index(drop=True)\n",
    "    ST_Y_train_preds_sorted = ST_Y_train_preds_series[ST_Y_train.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_train_mae = mean_absolute_error(ST_Y_train, ST_Y_train_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_train_rmse = root_mean_squared_error(ST_Y_train,ST_Y_train_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_train_r2_score = r2_score(ST_Y_train,ST_Y_train_preds)\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "    # Change validation predictions to pandas series\n",
    "    ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "    ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_valid_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_valid_rmse = root_mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_valid_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "\n",
    "    # Predict the test set which is forecast data\n",
    "    ST_Y_test_preds = model.predict(ST_X_test)\n",
    "    # Changes the predicted array values to pandas series\n",
    "    ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "    ST_Y_test_preds_series.index =ST_Y_test.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_test_sorted = ST_Y_test.sort_values().reset_index(drop=True)\n",
    "    ST_Y_test_preds_sorted = ST_Y_test_preds_series[ST_Y_test.index].sort_values().reset_index(drop=True)\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    ST_test_mae = mean_absolute_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate mean squared error\n",
    "    ST_test_rmse = root_mean_squared_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_test_r2_score = r2_score(ST_Y_test,ST_Y_test_preds)\n",
    "    \n",
    "    # Convert the Series to a DataFrame to return as dataframe\n",
    "    ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "    ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "\n",
    "\n",
    "     # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_train_sorted.index,ST_Y_train_sorted, color='blue', label=f'{name} Training Observed Values')\n",
    "    plt.plot(ST_Y_train_preds_sorted.index,ST_Y_train_preds_sorted, color='red', label=f'{name} Training Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_train_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_train_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_train_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 20 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Training Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_train_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Validation Set Observed Values')\n",
    "    plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Validation Set Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_valid_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_valid_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_valid_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 20 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Validation Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_valid_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the test sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_test_sorted.index,ST_Y_test_sorted, color='blue', label=f'{name} Test Observed Values')\n",
    "    plt.plot(ST_Y_test_preds_sorted.index,ST_Y_test_preds_sorted, color='red', label=f'{name} Test Predicted Values')\n",
    "    # Display the metrics as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_test_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_test_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_test_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 20 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Final Test Scores For {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_test_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()    \n",
    "    return ST_Y_test_preds_df\n",
    "    \n",
    "std_deviation = dataset_denormalized_outlier_filtered['ST20'].std()\n",
    "# Split the dataset into features and target\n",
    "many_features_dropped =  ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','ST100','ST50','ST20','ID']\n",
    "soil_features_dropped = ['ST20','ST50','ST100','ID']\n",
    "uncorrelated_dropped = ['ST20','ID']\n",
    "# ST20_X = dataset_shuffled.drop(many_features_dropped, axis=1)\n",
    "ST20_X = dataset_shuffled.drop(soil_features_dropped, axis=1)\n",
    "ST20_Y = dataset_shuffled['ST20']\n",
    "\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_2cm ).\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(dataset_shuffled))\n",
    "ST20_X_train, ST20_Y_train = ST20_X[:train_split], ST20_Y[:train_split]\n",
    "ST20_X_valid, ST20_Y_valid =ST20_X[train_split:valid_split], ST20_Y[train_split:valid_split]\n",
    "ST20_X_test, ST20_Y_test = ST20_X[valid_split:], ST20_Y[valid_split:]\n",
    "\n",
    "# A. CatBoostRegressor (CB)\n",
    "# Create CB model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_cb_model_stack = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "# Fit the model for ST20 to start with\n",
    "ST20_cb_model_stack.fit(ST20_X_train, ST20_Y_train, eval_set=(ST20_X_valid, ST20_Y_valid), early_stopping_rounds=100)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================CatBoost The Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST20_cb_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'CB'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# B. RandomForestRegressor\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_rf_model_stack = RandomForestRegressor(n_estimators=300, \n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         max_features='sqrt',\n",
    "                                         max_depth=None,\n",
    "                                         bootstrap=False,\n",
    "                                         random_state=42)\n",
    "# Fit the model for ST20 to start with\n",
    "ST20_rf_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================Random Forest The Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST20_rf_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'RF'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# C. Histogram Based Gradient Boosting Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_gbr_model_stack = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "# Fit the ST20 model for soil temp at 100 cm\n",
    "ST20_gbr_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Histogram-Based Gradient Boosting Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_gbr_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'HGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# D. XGBoost Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create XGBoost for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_xgb_model_stack = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "# Fit the ST20 model for soil temp at 100 cm\n",
    "ST20_xgb_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The XGBoost Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_xgb_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'XGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# E. AdaBoostRegressor \n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create AdaBoost Regressor for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_adb_model_stack = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "# Fit the ST20 model for soil temp at 100 cm\n",
    "ST20_adb_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The AdaBoost Regressor Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_adb_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'ADB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# F. Ridge Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_rg_model_stack = Ridge(random_state=42)\n",
    "# Fit the ST20 model for soil temp at 100 cm\n",
    "ST20_rg_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Ridge Regressor Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_rg_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'RR'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# G. Lasso Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create Lasso model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_la_model_stack = Lasso(random_state=42)\n",
    "# Fit the ST20 model for soil temp at 100cm\n",
    "ST20_la_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Lasso Regressor Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_la_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'LA'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# H. ElasticNet Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_en_model_stack = ElasticNet(random_state=42)\n",
    "# Fit the ST20 model for soil temp at 100cm\n",
    "ST20_en_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The ElasticNet Regressor Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_en_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'EN'))\n",
    "print(\"=========================================================================================================\\n\")\n",
    "\n",
    "# I. SVR-L Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-L model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_svrl_model_stack = SVR(kernel='linear')\n",
    "# Fit the ST20 model for soil temp at 100cm\n",
    "ST20_svrl_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with linear model Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_svrl_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'SVR-L'))\n",
    "print(\"==========================================================================================================\\n\")\n",
    "\n",
    "# J. SVR-R Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-R model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST20_svrr_model_stack = SVR(kernel='rbf')\n",
    "# Fit the ST20 model for soil temp at 100cm\n",
    "ST20_svrr_model_stack.fit(ST20_X_train, ST20_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with rfb model Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_svrr_model_stack, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'SVR-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# Stack of predictors on a single data set\n",
    "ST20_rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_samples_split=2,\n",
    "                                     max_features='sqrt',\n",
    "                                     max_depth=None,\n",
    "                                     bootstrap=False,\n",
    "                                     random_state=42)\n",
    "ST20_gbdt_regresssor = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "ST20_xgb_model = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "ST20_cb_regressor = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "ST20_adb_regressor = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    (\"RandomForest\", ST20_rf_regressor),\n",
    "    (\"CatBoost\", ST20_cb_regressor),\n",
    "    (\"HistGradientBoosting\", ST20_gbdt_regresssor),\n",
    "    (\"XGBoost\", ST20_xgb_model)\n",
    "]\n",
    "ST20_stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\n",
    "\n",
    "# Measure and plot the results\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for ax, (name, est) in zip(axs, estimators + [(\"Stacking Regressor\", ST5_stacking_regressor)]):\n",
    "    scorers = {\"R^2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\", \"RMSE\": \"neg_root_mean_squared_error\"}\n",
    "\n",
    "    start_time = time.time()\n",
    "    scores = cross_validate(est, ST20_X_train, ST20_Y_train, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_pred = cross_val_predict(est, ST20_X_valid, ST20_Y_valid, n_jobs=-1, verbose=0)\n",
    "    y_test = cross_val_predict(est, ST20_X_test, ST20_Y_test, n_jobs=-1, verbose=0)\n",
    "    \n",
    "    # Calculate mean and std_deviation for each scorer\n",
    "    scores_mean_std = {\n",
    "        key: (np.abs(np.mean(scores[f'test_{value}'])), np.std(scores[f'test_{value}']))\n",
    "        for key, value in scorers.items()\n",
    "    }\n",
    "\n",
    "    # Format the scores\n",
    "    formatted_scores = {\n",
    "        key: f\"{mean:.4f} ± {std_dev:.4f}\"\n",
    "        for key, (mean, std_dev) in scores_mean_std.items()\n",
    "    }\n",
    "\n",
    "    display = PredictionErrorDisplay.from_predictions(\n",
    "        y_true=ST20_Y_valid,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        ax=ax,\n",
    "        scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "        line_kwargs={\"color\": \"tab:red\"},\n",
    "    )\n",
    "    ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.4f} seconds\", fontsize=14)\n",
    "    # Set custom x-label and y-label\n",
    "    ax.set_xlabel(\"Predicted Soil Temperature at 20 cm (°C)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Observed Soil Temperature at 20 cm (°C)\", fontsize=14)\n",
    "\n",
    "    for metric_name, (mean, std_dev) in scores_mean_std.items():\n",
    "        if metric_name == 'R^2':\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {formatted_scores[metric_name]}\")\n",
    "        else:\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {mean:.4f} ± {std_dev:.4f}\")\n",
    "    \n",
    "    ax.legend(loc=\"best\", fontsize='small')\n",
    "    # Save the mean and std scores to an Excel file\n",
    "    df_scores_summary = pd.DataFrame(scores_mean_std).T\n",
    "    df_scores_summary.columns = ['Train Mean', 'Train Std Dev']\n",
    "    df_scores_summary.to_excel(f'data/results/ST20/{name}_cv_scores.xlsx', index=True)\n",
    "# Hide any unused subplots\n",
    "for i in range(len(estimators)+1, len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "# Apply tight layout\n",
    "plt.tight_layout()\n",
    "# Save the entire figure with all subplots to a file\n",
    "fig.savefig('data/results/ST20/stacked_regressors_prediction_error_plots.png', bbox_inches='tight')\n",
    "\n",
    "# Sort actual values and get sorted indices\n",
    "ST20_Y_valid_sorted = ST20_Y_valid.sort_values()\n",
    "sorted_indices = ST20_Y_valid_sorted.index\n",
    "\n",
    "# Reorder y_pred using the sorted indices\n",
    "y_pred_sorted = pd.Series(y_pred, index=ST20_Y_valid.index).loc[sorted_indices]\n",
    "\n",
    "# Calculate metrics for the validation set predictions\n",
    "mae_valid = mean_absolute_error(ST20_Y_valid, y_pred)\n",
    "rmse_valid = np.sqrt(mean_squared_error(ST20_Y_valid, y_pred))\n",
    "r2_valid = r2_score(ST20_Y_valid, y_pred)\n",
    "\n",
    "# Calculate metrics for the test set predictions\n",
    "mae_test = mean_absolute_error(ST20_Y_test, y_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(ST20_Y_test, y_test))\n",
    "r2_test = r2_score(ST20_Y_test, y_test)\n",
    "\n",
    "# Save the validation metrics to an Excel file\n",
    "validation_test_metrics = {\n",
    "    'V_R^2': [r2_valid],\n",
    "    'V_MAE': [mae_valid],\n",
    "    'V_RMSE': [rmse_valid],\n",
    "    'T_R^2': [r2_test],\n",
    "    'T_MAE': [mae_test],\n",
    "    'T_RMSE': [rmse_test]    \n",
    "}\n",
    "# Save the validation metrics to an Excel file\n",
    "\n",
    "df_metrics = pd.DataFrame(validation_test_metrics)\n",
    "df_metrics.to_excel(f'data/results/ST20/{name}_validation_test_metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the sorted actual values and corresponding predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ST20_Y_valid_sorted.values, color='blue', label='Observed Values')\n",
    "plt.plot(y_pred_sorted.values, color='red', label='Predicted Values')\n",
    "\n",
    "# Display the metrics as text annotation\n",
    "plt.text(0.1, 0.75, f'MAE: {mae_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.3, 0.75, f'RMSE: {rmse_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.5, 0.75, f'R^2: {r2_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel('Index', fontsize=14)\n",
    "plt.ylabel('Soil Temperature at 20 cm (°C)', fontsize=14)\n",
    "plt.title(f'STACK-R model\\'s validation set\\'s predicted vs observed values for ST20', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'data/results/ST20/{name}_cross_validation_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "dump(ST20_stacking_regressor, filename=\"data/results/ST20/ST20_STACK-R_trained_model.joblib\");\n",
    "# Fit the stacking regressor for direct fitting and prediction for all sets at default CV=5\n",
    "ST20_stacking_regressor.fit(ST20_X_train, ST20_Y_train)\n",
    "print(\"====================The Stacking Regressor Evaluation Metrics Results For ST20 Denormalized =======================\\n\")\n",
    "print(show_scores(ST20_stacking_regressor, ST20_X_train, ST20_X_valid, ST20_X_test, ST20_Y_train, ST20_Y_valid, ST20_Y_test, std_deviation,'ST20', 'STACK-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "# ST20_Y_test_preds_df = predict_plot(ST20_stacking_regressor, ST20_X_train, ST20_Y_train, ST20_X_test, ST20_Y_test, ST20_X_valid, ST20_Y_valid, 'ST20', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed764e5e-c156-4039-a843-b8ef8a92f71c",
   "metadata": {},
   "source": [
    "### Cross-validation to check stability of the stacking regressor for ST20\n",
    "### NOTE: This will take time!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc776b3-a63d-43bf-ae16-8cfe10b9bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "ST20_cv_scores = cross_val_score(ST20_stacking_regressor, ST20_X_train, ST20_Y_train, cv=kf, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive\n",
    "ST20_cv_scores = -ST20_cv_scores\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-Validation Scores (MSE):\", ST20_cv_scores)\n",
    "print(\"Mean CV Score (MSE):\", np.mean(ST20_cv_scores))\n",
    "print(\"Standard Deviation of CV Scores:\", np.std(ST20_cv_scores))\n",
    "# Save the scores to an Excel file\n",
    "ST20_cv_scores_df = pd.DataFrame(ST20_cv_scores, columns=['MSE'])\n",
    "ST20_cv_scores_df.to_excel('data/results/ST20/ST20_10_fold_cv_scores.xlsx', index=False)\n",
    "\n",
    "##=========== Visualize the problematic Fold using histogram==================\n",
    "# Calculate mean MSE\n",
    "ST20_mean_mse = np.mean(ST20_cv_scores)\n",
    "# Identify the problematic fold\n",
    "ST20_problematic_fold_index = np.argmax(np.abs(ST20_cv_scores - ST20_mean_mse))\n",
    "# Get the indices of the data points in the problematic fold\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(ST20_X_train)):\n",
    "    if fold_index == ST20_problematic_fold_index:\n",
    "        problematic_fold_train_indices = train_index\n",
    "        problematic_fold_test_indices = test_index\n",
    "\n",
    "# Subset the data for the problematic fold\n",
    "X_problematic_fold = ST20_X_train.iloc[problematic_fold_test_indices]\n",
    "y_problematic_fold = ST20_Y_train.iloc[problematic_fold_test_indices]\n",
    "# Visualize or analyze features for the problematic fold\n",
    "for feature in ST20_X_train.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Histogram for the problematic fold\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_problematic_fold[feature], kde=True, bins=20, color='red')\n",
    "    plt.title(f'{feature} - Problematic Fold')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    # Histogram for the entire dataset\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(ST20_X_train[feature], kde=True, bins=20, color='blue')\n",
    "    plt.title(f'{feature} - Entire Dataset')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/results/ST20/ST20_CV_problematic_10_fold_vs_main_dataset_histograms.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8aaf8e-2bcb-4cbd-a289-bd407071774d",
   "metadata": {},
   "source": [
    "### Partial Dependence, Individual Conditional Expectation and Residual Analysis Plots for ST20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733795fd-c352-4ddd-b59d-5f225a65142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import statsmodels.api as sm\n",
    "from pycebox.ice import ice, ice_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# # Partial Dependence Plot\n",
    "# print('====================================================== ST20 Partial Dependence Plot')\n",
    "# ST20_feature_names = ST20_X_train.columns.tolist()\n",
    "# n_features = len(ST20_feature_names)\n",
    "# n_cols = 2\n",
    "# n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "# fig1, ax1 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat = ax1.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST20_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST20_stacking_regressor, ST20_X_train, features=[feature])\n",
    "#     display.plot(ax=axes_flat[idx])\n",
    "#     axes_flat[idx].set_title(f'Partial Dependence (PD) Plot for {feature}')\n",
    "#     axes_flat[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat)):\n",
    "#     fig1.delaxes(axes_flat[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "# plt.suptitle('Partial Dependence (PD) Plot', fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# # plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST20/ST20_final_partial_dependence_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Individual Conditional Expectation Plot (ICE)\n",
    "# print('======================== ST20 Individual Conditional Expectation Plot ===============================')\n",
    "# fig2, axes2 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat2 = axes2.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST20_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST20_stacking_regressor, ST20_X_train, features=[feature], kind='individual')\n",
    "#     display.plot(ax=axes_flat2[idx])\n",
    "#     axes_flat2[idx].set_title(f'Individual Conditional Expectation (ICE) Plot for {feature}')\n",
    "#     axes_flat2[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat2)):\n",
    "#     fig2.delaxes(axes_flat2[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.7)\n",
    "# plt.suptitle('Individual Conditional Expectation (ICE) Plot', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST20/ST20_final_individual_conditional_expectation_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# Residual Analysis\n",
    "print('========================= ST20 Residual Analysis Plot ==============================')\n",
    "ST20_Y_predictions = ST20_stacking_regressor.predict(ST20_X_test)\n",
    "ST20_residuals = ST20_Y_test - ST20_Y_predictions\n",
    "\n",
    "# Calculate the interquartile range (IQR)\n",
    "Q1 = np.percentile(ST20_residuals, 25)\n",
    "Q3 = np.percentile(ST20_residuals, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the whisker range\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Count outliers\n",
    "outliers = np.sum((ST20_residuals < lower_bound) | (ST20_residuals > upper_bound))\n",
    "total_residuals = len(ST20_residuals)\n",
    "outlier_percentage = (outliers / total_residuals) * 100\n",
    "\n",
    "# Plotting the residuals scatter plot and box-and-whisker plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Residuals analysis plot\n",
    "ax[0].scatter(ST20_Y_predictions, ST20_residuals)\n",
    "ax[0].set_xlabel('Predictions', fontsize=14)\n",
    "ax[0].set_ylabel('Residuals', fontsize=14)\n",
    "ax[0].set_title('ST20 Residuals Analysis Plot', fontsize=14)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax[0].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# Box-and-whisker plot for residuals\n",
    "sns.boxplot(y=ST20_residuals, ax=ax[1])\n",
    "ax[1].set_title('ST20 Box-and-Whisker Plot of Residuals', fontsize=14)\n",
    "ax[1].set_ylabel('ST20 Residuals', fontsize=14)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# Annotate the plot with the number of outliers and total residuals\n",
    "annotation_text = (f'Total Residuals: {total_residuals}\\n'\n",
    "                   f'Number of Outliers: {outliers}\\n'\n",
    "                   f'Percentage of Outliers: {outlier_percentage:.2f}%')\n",
    "ax[1].annotate(annotation_text, xy=(0.8, 0.85), xycoords='axes fraction',\n",
    "               fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST20/ST20_final_residual_and_boxplot_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predictor Variables\n",
    "print('========================= Residuals vs. Predictor Variables ==============================')\n",
    "for column in ST20_X_test.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.scatter(ST20_X_test[column], ST20_residuals)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel(column, fontsize=14)\n",
    "    ax.set_ylabel('Residuals', fontsize=14)\n",
    "    ax.set_title(f'Residuals vs. {column}', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/results/ST20_final_residuals_vs_{column}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the Box Plot of all features\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Number of features in the DataFrame\n",
    "num_features = dataset_denormalized_outlier_filtered.shape[1]\n",
    "# Calculate the number of rows needed to plot all features in 3 columns\n",
    "num_cols = 3\n",
    "num_rows = math.ceil(num_features / num_cols)\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "# Define colors for each column\n",
    "colors = ['green', 'purple', 'red']\n",
    "# Create a Box Plot for each feature\n",
    "for i, column in enumerate(dataset_denormalized_outlier_filtered.columns):\n",
    "    col_index = i % num_cols  # Determine the column index (0, 1, or 2)\n",
    "    sns.boxplot(data=dataset_denormalized_outlier_filtered[column], ax=axes[i], color=colors[col_index])\n",
    "    axes[i].set_title(f'Box Plot for {column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Values', fontsize=14)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[i].tick_params(axis='both', which='minor', labelsize=12)\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST20/ST20_Box_plot_of_features.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# # Q-Q Plot\n",
    "# print('========================= ST20 Q-Q Plot ==============================')\n",
    "# fig5, ax5 = plt.subplots(figsize=(10, 7))\n",
    "# sm.qqplot(ST20_residuals, line='45', ax=ax5)\n",
    "# ax5.set_title('Q-Q Plot of Residuals')\n",
    "# plt.savefig('data/results/ST20/ST20_final_Q-Q_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Histogram of residuals\n",
    "# fig6, ax6 = plt.subplots(figsize=(10, 7))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(residuals, kde=True, ax=ax6)\n",
    "# plt.xlabel('Residuals')\n",
    "# plt.title('Histogram of Residuals')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd086d-133e-43d9-b25f-daf9e65dd1bf",
   "metadata": {},
   "source": [
    "### Feature Importance analysis ST20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1aaa0-9c58-48a8-8831-692d49c719d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ST20_X_train and ST20_Y_train are your training data\n",
    "\n",
    "# Fit the stacking regressor\n",
    "ST20_stacking_regressor.fit(ST20_X_train, ST20_Y_train)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = ST20_X_train.columns\n",
    "\n",
    "# Initialize an array to store feature importances\n",
    "feature_importances = np.zeros(ST20_X_train.shape[1])\n",
    "\n",
    "# Function to extract feature importances\n",
    "def get_feature_importance(model, X, y):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return np.abs(model.coef_)\n",
    "    elif isinstance(model, CatBoostRegressor):\n",
    "        return model.get_feature_importance()\n",
    "    else:\n",
    "        # Use permutation importance as a fallback for models without direct attribute\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        return result.importances_mean\n",
    "\n",
    "# Aggregate feature importances\n",
    "for name, model in ST20_stacking_regressor.named_estimators_.items():\n",
    "    importances = get_feature_importance(model, ST20_X_train, ST20_Y_train)\n",
    "    feature_importances += importances\n",
    "\n",
    "# Normalize the aggregated feature importances\n",
    "feature_importances /= len(ST20_stacking_regressor.named_estimators_)\n",
    "\n",
    "# Convert importances to percentage\n",
    "feature_importances_percentage = 100 * (feature_importances / np.sum(feature_importances))\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances_percentage\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('ST20 Stacking Regressor Feature Importances')\n",
    "\n",
    "# Add annotations\n",
    "for index, value in enumerate(importance_df['Importance']):\n",
    "    plt.text(value, index, f'{value:.2f}%', va='center')\n",
    "\n",
    "plt.savefig('data/results/ST20/ST20_stacking_regressor_feature_importances.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d553066-4c28-42a3-a5ac-0e4b86688df5",
   "metadata": {},
   "source": [
    "### Learning curves evaluation for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf847c1-c443-4c23-a8c3-bea3182e0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def plot_learning_curves(model, X_train, Y_train, X_valid, Y_valid, feature):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MAE to positive\n",
    "    train_errors_mae = -train_scores.mean(axis=1)\n",
    "    val_errors_mae = -valid_scores.mean(axis=1)\n",
    "    \n",
    "    train_sizes_mse, train_scores_mse, valid_scores_mse = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MSE to positive\n",
    "    train_errors_mse = -train_scores_mse.mean(axis=1)\n",
    "    val_errors_mse = -valid_scores_mse.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot MAE learning curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_sizes, train_errors_mae, \"r-\", label=\"Training MAE\")\n",
    "    plt.plot(train_sizes, val_errors_mae, \"b-\", label=\"Validation MAE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"MAE Learning Curve\")\n",
    "    \n",
    "    # Plot MSE learning curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_sizes, train_errors_mse, \"r-\", label=\"Training RMSE\")\n",
    "    plt.plot(train_sizes, val_errors_mse, \"b-\", label=\"Validation RMSE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE Learning Curve\")    \n",
    "    plt.savefig('data/results/ST20/ST20_learning_curves.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example call (ensure your data and model are defined correctly)\n",
    "plot_learning_curves(ST20_stacking_regressor, ST20_X_train, ST20_Y_train, ST20_X_valid, ST20_Y_valid, \"ST20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef80b2-61dd-4cdd-ba4b-5d5b4105b0e5",
   "metadata": {},
   "source": [
    "### GridSearhCV Evaluation for all models used in the stacked regressor for ST20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953cb41f-4b61-40d6-8736-982343bffb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_cb = {\n",
    "    'iterations': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'depth': [4, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 50, 100]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    'max_leaf_nodes': [31, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [20, 50, 100],\n",
    "    'l2_regularization': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [1.0, 0.8, 0.6],\n",
    "    'colsample_bytree': [1.0, 0.8, 0.6],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 0.1, 0.01],\n",
    "    'tree_method': ['gpu_hist']  # Use GPU\n",
    "}\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# stacking_param_grid ={\n",
    "#     'rf__n_estimators': [100, 300, 500],\n",
    "#     'rf__max_depth': [None, 10, 20, 30],\n",
    "#     'hgb__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'hgb__max_iter': [100, 200, 500],\n",
    "#     'catboost__iterations': [100, 200, 500],\n",
    "#     'catboost__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'catboost__depth': [4, 6, 10],\n",
    "#     'xgb__n_estimators': [100, 200],\n",
    "#     'xgb__max_depth': [3, 5]\n",
    "# }\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "ada = AdaBoostRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestRegressor()),\n",
    "#     ('cb', CatBoostRegressor()),\n",
    "#     ('hgb', HistGradientBoostingRegressor()),    \n",
    "#     ('xgb', XGBRegressor())\n",
    "# ]\n",
    "\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=RidgeCV()\n",
    "# )\n",
    "\n",
    "# Initialize GridSearchCV for RF model\n",
    "start_time_rf = time.time()\n",
    "ST20_grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_rf_search = time.time()\n",
    "ST20_grid_search_rf.fit(ST20_X_train, ST20_Y_train)\n",
    "end_time_rf_fit = time.time()\n",
    "elapsed_time_search_rf = end_time_rf_search - start_time_rf\n",
    "elapsed_time_fit_rf = end_time_rf_fit - end_time_rf_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for RandomForestRegressor:\", ST20_grid_search_rf.best_params_)\n",
    "print(\"Best score for RandomForestRegressor:\", -ST20_grid_search_rf.best_score_)\n",
    "print(\"RandomForestRegressor GridSearchCV Time:\", elapsed_time_search_rf)\n",
    "print(\"RandomForestRegressor Fitting Time:\", elapsed_time_fit_rf)\n",
    "\n",
    "# Initialize GridSearchCV for HGB model\n",
    "start_time_hgb = time.time()\n",
    "ST20_grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=param_grid_hgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_hgb_search = time.time()\n",
    "ST20_grid_search_hgb.fit(ST20_X_train, ST20_Y_train)\n",
    "end_time_hgb_fit = time.time()\n",
    "elapsed_time_search_hgb = end_time_hgb_search - start_time_hgb\n",
    "elapsed_time_fit_hgb = end_time_hgb_fit - end_time_hgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for HistGradientBoostingRegressor:\", ST20_grid_search_hgb.best_params_)\n",
    "print(\"Best score for HistGradientBoostingRegressor:\", -ST20_grid_search_hgb.best_score_)\n",
    "print(\"HistGradientBoostingRegressor GridSearchCV Time:\", elapsed_time_search_hgb)\n",
    "print(\"HistGradientBoostingRegressor Fitting Time:\", elapsed_time_fit_hgb)\n",
    "\n",
    "# # Initialize GridSearchCV for XGB model\n",
    "# start_time_xgb = time.time()\n",
    "# ST20_grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "# end_time_xgb_search = time.time()\n",
    "# ST20_grid_search_xgb.fit(ST20_X_train, ST20_Y_train)\n",
    "# end_time_xgb_fit = time.time()\n",
    "# elapsed_time_search_xgb = end_time_xgb_search - start_time_xgb\n",
    "# elapsed_time_fit_xgb = end_time_xgb_fit - end_time_xgb_search\n",
    "# # Get the best parameters and scores\n",
    "# print(\"Best parameters for XGBRegressor:\", ST20_grid_search_xgb.best_params_)\n",
    "# print(\"Best score for XGBRegressor:\", -ST20_grid_search_xgb.best_score_)\n",
    "# print(\"XGBRegressor GridSearchCV Time:\", elapsed_time_search_xgb)\n",
    "# print(\"XGBRegressor Fitting Time:\", elapsed_time_fit_xgb)\n",
    "\n",
    "# Initialize GridSearchCV for ADA model\n",
    "start_time_ada = time.time()\n",
    "ST20_grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_ada_search = time.time()\n",
    "ST20_grid_search_ada.fit(ST20_X_train, ST20_Y_train)\n",
    "end_time_ada_fit = time.time()\n",
    "elapsed_time_search_ada = end_time_ada_search - start_time_ada\n",
    "elapsed_time_fit_ada = end_time_ada_fit - end_time_ada_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for AdaBoostRegressor:\", ST20_grid_search_ada.best_params_)\n",
    "print(\"Best score for AdaBoostRegressor:\", -ST20_grid_search_ada.best_score_)\n",
    "print(\"AdaBoostRegressor GridSearchCV Time:\", elapsed_time_search_ada)\n",
    "print(\"AdaBoostRegressor Fitting Time:\", elapsed_time_fit_ada)\n",
    "\n",
    "# # Initialize GridSearchCV for CB model\n",
    "# start_time_cb = time.time()\n",
    "# ST20_grid_search_cb = GridSearchCV(estimator=cb, param_grid=param_grid_cb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "# end_time_cb_search = time.time()\n",
    "# ST20_grid_search_cb.fit(ST20_X_train, ST20_Y_train)\n",
    "# end_time_cb_fit = time.time()\n",
    "# elapsed_time_search_cb = end_time_cb_search - start_time_cb\n",
    "# elapsed_time_fit_cb = end_time_cb_fit - end_time_cb_search\n",
    "# # Get the best parameters and scores\n",
    "# print(\"Best parameters for CatBoostRegressor:\", ST20_grid_search_cb.best_params_)\n",
    "# print(\"Best score for CatBoost:\", -ST20_grid_search_cb.best_score_)\n",
    "# print(\"CatBoostRegressor GridSearchCV Time:\", elapsed_time_search_cb)\n",
    "# print(\"CatBoostRegressor Fitting Time:\", elapsed_time_fit_cb)\n",
    "\n",
    "# # Initialize GridSearchCV for Stacking model\n",
    "# start_time_stacking = time.time()\n",
    "# ST20_grid_search_stacking = GridSearchCV(estimator=stacking_regressor, param_grid=stacking_param_grid, cv=5, n_jobs=-1, verbose=0,scoring='neg_mean_absolute_error')\n",
    "# end_time_stacking_search = time.time()\n",
    "# ST20_grid_search_stacking.fit(ST20_X_train, ST20_Y_train)\n",
    "# end_time_stacking = time.time()\n",
    "# elapsed_time_search_stacking = end_time_stacking_search - start_time_stacking\n",
    "# elapsed_time_fit_stacking = end_time_stacking_fit - end_time_stacking_search\n",
    "\n",
    "# print(\"Best parameters for StackingRegressor:\", ST20_grid_search_stacking.best_params_)\n",
    "# print(\"Best score for StackingRegressor:\", -ST20_grid_search_stacking.best_score_)\n",
    "# print(\"StackingRegressor GridSearchCV Time:\", elapsed_time_search_stacking)\n",
    "# print(\"StackingRegressor Fitting Time:\", elapsed_time_fit_stacking)\n",
    "\n",
    "# # Define the results of print statements as variables\n",
    "# ST20_grid_search_and_fitting_results = {\n",
    "#     'Model': ['RandomForestRegressor', 'HistGradientBoostingRegressor', 'AdaBoostRegressor', 'XGBRegressor'],\n",
    "#     'Best Parameters': [ST20_grid_search_rf.best_params_, ST20_grid_search_hgb.best_params_, ST20_grid_search_ada.best_params_, ST20_grid_search_xgb.best_params_],\n",
    "#     'Best Score': [-ST20_grid_search_rf.best_score_, -ST20_grid_search_hgb.best_score_, -ST20_grid_search_ada.best_score_, -ST20_grid_search_xgb.best_score_],\n",
    "#     'GridSearchCV Time': [elapsed_time_search_rf, elapsed_time_search_hgb, elapsed_time_search_ada, elapsed_time_search_xgb],\n",
    "#     'Fitting Time': [elapsed_time_fit_rf, elapsed_time_fit_hgb, elapsed_time_fit_ada, elapsed_time_fit_xgb]\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df_results = pd.DataFrame(ST20_grid_search_and_fitting_results)\n",
    "\n",
    "# # Export DataFrame to Excel\n",
    "# df_results.to_excel('data/results/ST20/ST20_grid_search_and_fitting_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b0f93-4f1d-4d53-a6a5-907c2f5d345b",
   "metadata": {},
   "source": [
    "### E. Stacking Regressor for Soil temperature at 50cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4f60a-0fc1-4a2b-a578-42bb42535ae0",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e202487-5230-40f3-868f-a14d5e5c6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the covariance matrix for target ST100\n",
    "ST50_dataset_correlation = ST50_clean_dataset_denormalized.drop(['ST100', 'ST50'], axis=1)\n",
    "ST50_covariance_matrix = ST50_dataset_correlation.cov()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "ST50_correlation_matrix = ST50_dataset_correlation.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST50_correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST50/ST50_denormalized_before_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Set the threshold\n",
    "threshold = 0.95\n",
    "# Find pairs of features with correlation above the threshold\n",
    "highly_correlated = np.where(np.abs(ST50_correlation_matrix) > threshold)\n",
    "highly_correlated_pairs = [(ST50_correlation_matrix.index[x], ST50_correlation_matrix.columns[y]) \n",
    "                           for x, y in zip(*highly_correlated) if x != y and x < y]\n",
    "\n",
    "print(\"Highly correlated pairs (above threshold):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)\n",
    "# Example: Removing one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for pair in highly_correlated_pairs:\n",
    "    features_to_remove.add(pair[1])  # You can choose to remove pair[0] or pair[1]\n",
    "\n",
    "# Drop the features from the dataframe\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated = ST50_dataset_correlation.drop(columns=features_to_remove)\n",
    "\n",
    "print(f\"Removed features: {features_to_remove}\")\n",
    "print(\"Shape of the reduced dataset:\", ST50_dataset_denormalized_outlier_filtered_uncorrelated.shape)\n",
    "\n",
    "# After removing the correlated features\n",
    "# Calculate the correlation matrix\n",
    "ST50_correlation_matrix_new = ST50_dataset_denormalized_outlier_filtered_uncorrelated.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST50_correlation_matrix_new, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/results/ST50/ST50_denormalized_after_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Assuming dataset_denormalized_outlier_filtered is your DataFrame\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST50_dataset_denormalized_outlier_filtered_uncorrelated.copy()\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = sm.add_constant(ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(data):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Iteratively remove features with VIF above the threshold\n",
    "def remove_high_vif_features(data, threshold=40.0):\n",
    "    while True:\n",
    "        vif_data = calculate_vif(data)\n",
    "        max_vif = vif_data['VIF'].max()\n",
    "        if max_vif > threshold:\n",
    "            # Identify the feature with the highest VIF\n",
    "            feature_to_remove = vif_data.sort_values('VIF', ascending=False)['feature'].iloc[0]\n",
    "            print(f\"Removing feature '{feature_to_remove}' with VIF: {max_vif}\")\n",
    "            data = data.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "    return data, vif_data\n",
    "\n",
    "# Remove high VIF features\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif, ST50_final_vif_data = remove_high_vif_features(ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "\n",
    "print(\"Final VIF data:\")\n",
    "print(ST50_final_vif_data)\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID'] = ST50_clean_dataset_denormalized['ID']\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST50'] = ST50_clean_dataset_denormalized['ST50']\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated['ID'] = ST50_clean_dataset_denormalized['ID']\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated['ST50'] = ST50_clean_dataset_denormalized['ST50']\n",
    "# Remove the constant term before creating the final DataFrame\n",
    "if 'const' in ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.columns:\n",
    "    ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(columns=['const'])\n",
    "\n",
    "# Store the 'ID' and 'ST50' columns with their corresponding index before PCA\n",
    "ID_index_mapping = ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID']\n",
    "ST50_index_mapping = ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST50']\n",
    "\n",
    "# Assume X is your feature dataframe\n",
    "ST50_X_pca = ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(['ST50', 'ID'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ST50_X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)  # Choose the number of components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "ST50_pca_df = pd.DataFrame(data=principal_components, columns=[f\"PC{i}\" for i in range(principal_components.shape[1])])\n",
    "\n",
    "# Merge PCA DataFrame with original DataFrame to maintain original index order\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ID_index_mapping, ST50_index_mapping, left_index=True, right_index=True)\n",
    "ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca, ST50_pca_df, left_index=True, right_index=True)\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.savefig('data/results/ST50/ST50_PCA_analysis.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0a58e-8228-477a-9aa3-8636a8b72830",
   "metadata": {},
   "source": [
    "### Option 1:  ST50 Prediction by varying the dataset cases\n",
    "#### Note: Choose the dataset case at this line of the code: dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1)\n",
    "#### Dataset Cases:\n",
    "##### Case 1. dataset_denormalized_outlier_filtered\n",
    "##### case 2. ST50_clean_dataset_denormalized\n",
    "##### case 3. ST50_dataset_denormalized_outlier_filtered_uncorrelated\n",
    "##### case 4. ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif\n",
    "##### case 5. ST50_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb7f59-616f-4121-8e82-add4aedd7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR  \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Choose any of the time-independent dataset cases\n",
    "# Select the dataset case here\n",
    "dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1) # Choose dataset case here\n",
    "  \n",
    "# Create function to evaluate model on few different levels\n",
    "def show_scores(model, X_train, X_valid, X_test, Y_train, Y_valid, Y_test, std, target='ST50', model_name='RF'):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    test_preds = model.predict(X_test)\n",
    "    scores = {\n",
    "              # \"Validation Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_test, test_preds),   \n",
    "              # \"Validation Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds), \n",
    "              # \"Validation Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set RMSE\": root_mean_squared_error(Y_valid, val_preds),\n",
    "              # \"Test Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Test Set R^2 Score\":r2_score(Y_valid, val_preds),  \n",
    "              # \"Test Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Test Set MAE\": mean_absolute_error(Y_test, test_preds), \n",
    "              # \"Test Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Tes Set RMSE\": root_mean_squared_error(Y_test, test_preds),\n",
    "              # \"Validation Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),             \n",
    "              # \"Validation Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['Metric', 'Value'])    \n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_excel(f'data/results/{target}/{model_name}_scores.xlsx', index=False)\n",
    "    return scores\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_plot(model, ST_X_train, ST_Y_train, ST_X_test, ST_Y_test, ST_X_validation, ST_Y_validation, name, std):\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_train_preds = model.predict(ST_X_train)\n",
    "    # Change train predictions to pandas series\n",
    "    ST_Y_train_preds_series = pd.Series(ST_Y_train_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_train_preds_series.index = ST_Y_train.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_train_sorted = ST_Y_train.sort_values().reset_index(drop=True)\n",
    "    ST_Y_train_preds_sorted = ST_Y_train_preds_series[ST_Y_train.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_train_mae = mean_absolute_error(ST_Y_train, ST_Y_train_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_train_rmse = root_mean_squared_error(ST_Y_train,ST_Y_train_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_train_r2_score = r2_score(ST_Y_train,ST_Y_train_preds)\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "    # Change validation predictions to pandas series\n",
    "    ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "    ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_valid_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_valid_rmse = root_mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_valid_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "\n",
    "    # Predict the test set which is forecast data\n",
    "    ST_Y_test_preds = model.predict(ST_X_test)\n",
    "    # Changes the predicted array values to pandas series\n",
    "    ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "    ST_Y_test_preds_series.index =ST_Y_test.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_test_sorted = ST_Y_test.sort_values().reset_index(drop=True)\n",
    "    ST_Y_test_preds_sorted = ST_Y_test_preds_series[ST_Y_test.index].sort_values().reset_index(drop=True)\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    ST_test_mae = mean_absolute_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate mean squared error\n",
    "    ST_test_rmse = root_mean_squared_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_test_r2_score = r2_score(ST_Y_test,ST_Y_test_preds)\n",
    "    \n",
    "    # Convert the Series to a DataFrame to return as dataframe\n",
    "    ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "    ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "\n",
    "\n",
    "     # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_train_sorted.index,ST_Y_train_sorted, color='blue', label=f'{name} Training Observed Values')\n",
    "    plt.plot(ST_Y_train_preds_sorted.index,ST_Y_train_preds_sorted, color='red', label=f'{name} Training Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_train_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_train_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_train_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 50 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Training Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_train_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Validation Set Observed Values')\n",
    "    plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Validation Set Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_valid_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_valid_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_valid_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 50 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Validation Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_valid_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the test sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_test_sorted.index,ST_Y_test_sorted, color='blue', label=f'{name} Test Observed Values')\n",
    "    plt.plot(ST_Y_test_preds_sorted.index,ST_Y_test_preds_sorted, color='red', label=f'{name} Test Predicted Values')\n",
    "    # Display the metrics as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_test_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_test_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_test_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 50 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Final Test Scores For {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_test_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()    \n",
    "    return ST_Y_test_preds_df\n",
    "    \n",
    "std_deviation = dataset_denormalized_outlier_filtered['ST50'].std()\n",
    "# Split the dataset into features and target\n",
    "many_features_dropped =  ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','ST100','ST50','ID']\n",
    "soil_features_dropped = ['ST50','ST100','ID']\n",
    "uncorrelated_dropped = ['ST50','ID']\n",
    "# ST50_X = dataset_shuffled.drop(many_features_dropped, axis=1)\n",
    "ST50_X = dataset_shuffled.drop(soil_features_dropped, axis=1)\n",
    "ST50_Y = dataset_shuffled['ST50']\n",
    "\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_2cm ).\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(dataset_shuffled))\n",
    "ST50_X_train, ST50_Y_train = ST50_X[:train_split], ST50_Y[:train_split]\n",
    "ST50_X_valid, ST50_Y_valid =ST50_X[train_split:valid_split], ST50_Y[train_split:valid_split]\n",
    "ST50_X_test, ST50_Y_test = ST50_X[valid_split:], ST50_Y[valid_split:]\n",
    "\n",
    "# A. CatBoostRegressor (CB)\n",
    "# Create CB model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_cb_model_stack = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "# Fit the model for ST50 to start with\n",
    "ST50_cb_model_stack.fit(ST50_X_train, ST50_Y_train, eval_set=(ST50_X_valid, ST50_Y_valid), early_stopping_rounds=100)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================CatBoost The Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST50_cb_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'CB'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# B. RandomForestRegressor\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_rf_model_stack = RandomForestRegressor(n_estimators=300, \n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         max_features='sqrt',\n",
    "                                         max_depth=None,\n",
    "                                         bootstrap=False,\n",
    "                                         random_state=42)\n",
    "# Fit the model for ST50 to start with\n",
    "ST50_rf_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================Random Forest The Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST50_rf_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'RF'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# C. Histogram Based Gradient Boosting Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_gbr_model_stack = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "# Fit the ST50 model for soil temp at 100 cm\n",
    "ST50_gbr_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Histogram-Based Gradient Boosting Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_gbr_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'HGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# D. XGBoost Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create XGBoost for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_xgb_model_stack = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "# Fit the ST50 model for soil temp at 100 cm\n",
    "ST50_xgb_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The XGBoost Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_xgb_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'XGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# E. AdaBoostRegressor \n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create AdaBoost Regressor for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_adb_model_stack = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "# Fit the ST50 model for soil temp at 100 cm\n",
    "ST50_adb_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The AdaBoost Regressor Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_adb_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'ADB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# F. Ridge Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_rg_model_stack = Ridge(random_state=42)\n",
    "# Fit the ST50 model for soil temp at 100 cm\n",
    "ST50_rg_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Ridge Regressor Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_rg_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'RR'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# G. Lasso Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create Lasso model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_la_model_stack = Lasso(random_state=42)\n",
    "# Fit the ST50 model for soil temp at 100cm\n",
    "ST50_la_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Lasso Regressor Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_la_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'LA'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# H. ElasticNet Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_en_model_stack = ElasticNet(random_state=42)\n",
    "# Fit the ST50 model for soil temp at 100cm\n",
    "ST50_en_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The ElasticNet Regressor Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_en_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'EN'))\n",
    "print(\"=========================================================================================================\\n\")\n",
    "\n",
    "# I. SVR-L Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-L model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_svrl_model_stack = SVR(kernel='linear')\n",
    "# Fit the ST50 model for soil temp at 100cm\n",
    "ST50_svrl_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with linear model Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_svrl_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'SVR-L'))\n",
    "print(\"==========================================================================================================\\n\")\n",
    "\n",
    "# J. SVR-R Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-R model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST50_svrr_model_stack = SVR(kernel='rbf')\n",
    "# Fit the ST50 model for soil temp at 100cm\n",
    "ST50_svrr_model_stack.fit(ST50_X_train, ST50_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with rfb model Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_svrr_model_stack, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'SVR-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# Stack of predictors on a single data set\n",
    "ST50_rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_samples_split=2,\n",
    "                                     max_features='sqrt',\n",
    "                                     max_depth=None,\n",
    "                                     bootstrap=False,\n",
    "                                     random_state=42)\n",
    "ST50_gbdt_regresssor = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "ST50_xgb_model = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "ST50_cb_regressor = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "ST50_adb_regressor = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    (\"RandomForest\", ST50_rf_regressor),\n",
    "    (\"CatBoost\", ST50_cb_regressor),\n",
    "    (\"HistGradientBoosting\", ST50_gbdt_regresssor),\n",
    "    (\"XGBoost\", ST50_xgb_model)\n",
    "]\n",
    "ST50_stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\n",
    "\n",
    "# Measure and plot the results\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for ax, (name, est) in zip(axs, estimators + [(\"Stacking Regressor\", ST5_stacking_regressor)]):\n",
    "    scorers = {\"R^2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\", \"RMSE\": \"neg_root_mean_squared_error\"}\n",
    "\n",
    "    start_time = time.time()\n",
    "    scores = cross_validate(est, ST50_X_train, ST50_Y_train, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_pred = cross_val_predict(est, ST50_X_valid, ST50_Y_valid, n_jobs=-1, verbose=0)\n",
    "    y_test = cross_val_predict(est, ST50_X_test, ST50_Y_test, n_jobs=-1, verbose=0)\n",
    "    \n",
    "    # Calculate mean and std_deviation for each scorer\n",
    "    scores_mean_std = {\n",
    "        key: (np.abs(np.mean(scores[f'test_{value}'])), np.std(scores[f'test_{value}']))\n",
    "        for key, value in scorers.items()\n",
    "    }\n",
    "\n",
    "    # Format the scores\n",
    "    formatted_scores = {\n",
    "        key: f\"{mean:.4f} ± {std_dev:.4f}\"\n",
    "        for key, (mean, std_dev) in scores_mean_std.items()\n",
    "    }\n",
    "\n",
    "    display = PredictionErrorDisplay.from_predictions(\n",
    "        y_true=ST50_Y_valid,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        ax=ax,\n",
    "        scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "        line_kwargs={\"color\": \"tab:red\"},\n",
    "    )\n",
    "    ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.4f} seconds\", fontsize=14)\n",
    "    # Set custom x-label and y-label\n",
    "    ax.set_xlabel(\"Predicted Soil Temperature at 50 cm (°C)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Observed Soil Temperature at 50 cm (°C)\", fontsize=14)\n",
    "\n",
    "    for metric_name, (mean, std_dev) in scores_mean_std.items():\n",
    "        if metric_name == 'R^2':\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {formatted_scores[metric_name]}\")\n",
    "        else:\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {mean:.4f} ± {std_dev:.4f}\")\n",
    "    \n",
    "    ax.legend(loc=\"best\", fontsize='small')\n",
    "    # Save the mean and std scores to an Excel file\n",
    "    df_scores_summary = pd.DataFrame(scores_mean_std).T\n",
    "    df_scores_summary.columns = ['Train Mean', 'Train Std Dev']\n",
    "    df_scores_summary.to_excel(f'data/results/ST50/{name}_cv_scores.xlsx', index=True)\n",
    "# Hide any unused subplots\n",
    "for i in range(len(estimators)+1, len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "# Apply tight layout\n",
    "plt.tight_layout()\n",
    "# Save the entire figure with all subplots to a file\n",
    "fig.savefig('data/results/ST50/stacked_regressors_prediction_error_plots.png', bbox_inches='tight')\n",
    "\n",
    "# Sort actual values and get sorted indices\n",
    "ST50_Y_valid_sorted = ST50_Y_valid.sort_values()\n",
    "sorted_indices = ST50_Y_valid_sorted.index\n",
    "\n",
    "# Reorder y_pred using the sorted indices\n",
    "y_pred_sorted = pd.Series(y_pred, index=ST50_Y_valid.index).loc[sorted_indices]\n",
    "\n",
    "# Calculate metrics for the validation set predictions\n",
    "mae_valid = mean_absolute_error(ST50_Y_valid, y_pred)\n",
    "rmse_valid = np.sqrt(mean_squared_error(ST50_Y_valid, y_pred))\n",
    "r2_valid = r2_score(ST50_Y_valid, y_pred)\n",
    "\n",
    "# Calculate metrics for the test set predictions\n",
    "mae_test = mean_absolute_error(ST50_Y_test, y_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(ST50_Y_test, y_test))\n",
    "r2_test = r2_score(ST50_Y_test, y_test)\n",
    "\n",
    "# Save the validation metrics to an Excel file\n",
    "validation_test_metrics = {\n",
    "    'V_R^2': [r2_valid],\n",
    "    'V_MAE': [mae_valid],\n",
    "    'V_RMSE': [rmse_valid],\n",
    "    'T_R^2': [r2_test],\n",
    "    'T_MAE': [mae_test],\n",
    "    'T_RMSE': [rmse_test]    \n",
    "}\n",
    "# Save the validation metrics to an Excel file\n",
    "\n",
    "df_metrics = pd.DataFrame(validation_test_metrics)\n",
    "df_metrics.to_excel(f'data/results/ST50/{name}_validation_test_metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the sorted actual values and corresponding predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ST50_Y_valid_sorted.values, color='blue', label='Observed Values')\n",
    "plt.plot(y_pred_sorted.values, color='red', label='Predicted Values')\n",
    "\n",
    "# Display the metrics as text annotation\n",
    "plt.text(0.1, 0.75, f'MAE: {mae_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.3, 0.75, f'RMSE: {rmse_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.5, 0.75, f'R^2: {r2_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel('Index', fontsize=14)\n",
    "plt.ylabel('Soil Temperature at 50 cm (°C)', fontsize=14)\n",
    "plt.title(f'STACK-R model\\'s validation set\\'s predicted vs observed values for ST50', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'data/results/ST50/{name}_cross_validation_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "dump(ST50_stacking_regressor, filename=\"data/results/ST50/ST50_STACK-R_trained_model.joblib\");\n",
    "# Fit the stacking regressor for direct fitting and prediction for all sets at default CV=5\n",
    "ST50_stacking_regressor.fit(ST50_X_train, ST50_Y_train)\n",
    "print(\"====================The Stacking Regressor Evaluation Metrics Results For ST50 Denormalized =======================\\n\")\n",
    "print(show_scores(ST50_stacking_regressor, ST50_X_train, ST50_X_valid, ST50_X_test, ST50_Y_train, ST50_Y_valid, ST50_Y_test, std_deviation,'ST50', 'STACK-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "# ST50_Y_test_preds_df = predict_plot(ST50_stacking_regressor, ST50_X_train, ST50_Y_train, ST50_X_test, ST50_Y_test, ST50_X_valid, ST50_Y_valid, 'ST50', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c9789-7dc1-4056-a4b2-d1097f9fbe9a",
   "metadata": {},
   "source": [
    "### Cross-validation to check stability of the stacking regressor for ST50\n",
    "### NOTE: This will take time!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834750a-513d-4810-8d04-c07edaba0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "ST50_cv_scores = cross_val_score(ST50_stacking_regressor, ST50_X_train, ST50_Y_train, cv=kf, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive\n",
    "ST50_cv_scores = -ST50_cv_scores\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-Validation Scores (MSE):\", ST50_cv_scores)\n",
    "print(\"Mean CV Score (MSE):\", np.mean(ST50_cv_scores))\n",
    "print(\"Standard Deviation of CV Scores:\", np.std(ST50_cv_scores))\n",
    "# Save the scores to an Excel file\n",
    "ST50_cv_scores_df = pd.DataFrame(ST50_cv_scores, columns=['MSE'])\n",
    "ST50_cv_scores_df.to_excel('data/results/ST50/ST50_10_fold_cv_scores.xlsx', index=False)\n",
    "\n",
    "##=========== Visualize the problematic Fold using histogram==================\n",
    "# Calculate mean MSE\n",
    "ST50_mean_mse = np.mean(ST50_cv_scores)\n",
    "# Identify the problematic fold\n",
    "ST50_problematic_fold_index = np.argmax(np.abs(ST50_cv_scores - ST50_mean_mse))\n",
    "# Get the indices of the data points in the problematic fold\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(ST50_X_train)):\n",
    "    if fold_index == ST50_problematic_fold_index:\n",
    "        problematic_fold_train_indices = train_index\n",
    "        problematic_fold_test_indices = test_index\n",
    "\n",
    "# Subset the data for the problematic fold\n",
    "X_problematic_fold = ST50_X_train.iloc[problematic_fold_test_indices]\n",
    "y_problematic_fold = ST50_Y_train.iloc[problematic_fold_test_indices]\n",
    "# Visualize or analyze features for the problematic fold\n",
    "for feature in ST50_X_train.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Histogram for the problematic fold\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_problematic_fold[feature], kde=True, bins=20, color='red')\n",
    "    plt.title(f'{feature} - Problematic Fold')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    # Histogram for the entire dataset\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(ST50_X_train[feature], kde=True, bins=20, color='blue')\n",
    "    plt.title(f'{feature} - Entire Dataset')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/results/ST50/ST50_CV_problematic_10_fold_vs_main_dataset_histograms.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26f6d6-dcb3-4741-9a2c-87235ac6db9b",
   "metadata": {},
   "source": [
    "### Partial Dependence, Individual Conditional Expectation and Residual Analysis Plots for ST50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b19a0-2cdf-4046-8017-4eba635707c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import statsmodels.api as sm\n",
    "from pycebox.ice import ice, ice_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# # Partial Dependence Plot\n",
    "# print('====================================================== ST50 Partial Dependence Plot')\n",
    "# ST50_feature_names = ST50_X_train.columns.tolist()\n",
    "# n_features = len(ST50_feature_names)\n",
    "# n_cols = 2\n",
    "# n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "# fig1, ax1 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat = ax1.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST50_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST50_stacking_regressor, ST50_X_train, features=[feature])\n",
    "#     display.plot(ax=axes_flat[idx])\n",
    "#     axes_flat[idx].set_title(f'Partial Dependence (PD) Plot for {feature}')\n",
    "#     axes_flat[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat)):\n",
    "#     fig1.delaxes(axes_flat[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "# plt.suptitle('Partial Dependence (PD) Plot', fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# # plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST50/ST50_final_partial_dependence_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Individual Conditional Expectation Plot (ICE)\n",
    "# print('======================== ST50 Individual Conditional Expectation Plot ===============================')\n",
    "# fig2, axes2 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat2 = axes2.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST50_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST50_stacking_regressor, ST50_X_train, features=[feature], kind='individual')\n",
    "#     display.plot(ax=axes_flat2[idx])\n",
    "#     axes_flat2[idx].set_title(f'Individual Conditional Expectation (ICE) Plot for {feature}')\n",
    "#     axes_flat2[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat2)):\n",
    "#     fig2.delaxes(axes_flat2[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.7)\n",
    "# plt.suptitle('Individual Conditional Expectation (ICE) Plot', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST50/ST50_final_individual_conditional_expectation_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# Residual Analysis\n",
    "print('========================= ST50 Residual Analysis Plot ==============================')\n",
    "ST50_Y_predictions = ST50_stacking_regressor.predict(ST50_X_test)\n",
    "ST50_residuals = ST50_Y_test - ST50_Y_predictions\n",
    "\n",
    "# Calculate the interquartile range (IQR)\n",
    "Q1 = np.percentile(ST50_residuals, 25)\n",
    "Q3 = np.percentile(ST50_residuals, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the whisker range\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Count outliers\n",
    "outliers = np.sum((ST50_residuals < lower_bound) | (ST50_residuals > upper_bound))\n",
    "total_residuals = len(ST50_residuals)\n",
    "outlier_percentage = (outliers / total_residuals) * 100\n",
    "\n",
    "# Plotting the residuals scatter plot and box-and-whisker plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Residuals analysis plot\n",
    "ax[0].scatter(ST50_Y_predictions, ST50_residuals)\n",
    "ax[0].set_xlabel('Predictions', fontsize=14)\n",
    "ax[0].set_ylabel('Residuals', fontsize=14)\n",
    "ax[0].set_title('ST50 Residuals Analysis Plot', fontsize=14)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax[0].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# Box-and-whisker plot for residuals\n",
    "sns.boxplot(y=ST50_residuals, ax=ax[1])\n",
    "ax[1].set_title('ST50 Box-and-Whisker Plot of Residuals', fontsize=14)\n",
    "ax[1].set_ylabel('ST50 Residuals', fontsize=14)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# Annotate the plot with the number of outliers and total residuals\n",
    "annotation_text = (f'Total Residuals: {total_residuals}\\n'\n",
    "                   f'Number of Outliers: {outliers}\\n'\n",
    "                   f'Percentage of Outliers: {outlier_percentage:.2f}%')\n",
    "ax[1].annotate(annotation_text, xy=(0.8, 0.87), xycoords='axes fraction',\n",
    "               fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST50/ST50_final_residual_and_boxplot_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predictor Variables\n",
    "print('========================= Residuals vs. Predictor Variables ==============================')\n",
    "for column in ST50_X_test.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.scatter(ST50_X_test[column], ST50_residuals)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel(column, fontsize=14)\n",
    "    ax.set_ylabel('Residuals', fontsize=14)\n",
    "    ax.set_title(f'Residuals vs. {column}', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/results/ST50_final_residuals_vs_{column}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the Box Plot of all features\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Number of features in the DataFrame\n",
    "num_features = dataset_denormalized_outlier_filtered.shape[1]\n",
    "# Calculate the number of rows needed to plot all features in 3 columns\n",
    "num_cols = 3\n",
    "num_rows = math.ceil(num_features / num_cols)\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "# Define colors for each column\n",
    "colors = ['green', 'purple', 'red']\n",
    "# Create a Box Plot for each feature\n",
    "for i, column in enumerate(dataset_denormalized_outlier_filtered.columns):\n",
    "    col_index = i % num_cols  # Determine the column index (0, 1, or 2)\n",
    "    sns.boxplot(data=dataset_denormalized_outlier_filtered[column], ax=axes[i], color=colors[col_index])\n",
    "    axes[i].set_title(f'Box Plot for {column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Values', fontsize=14)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[i].tick_params(axis='both', which='minor', labelsize=12)\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST50/ST50_Box_plot_of_features.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# # Q-Q Plot\n",
    "# print('========================= ST50 Q-Q Plot ==============================')\n",
    "# fig5, ax5 = plt.subplots(figsize=(10, 7))\n",
    "# sm.qqplot(ST50_residuals, line='45', ax=ax5)\n",
    "# ax5.set_title('Q-Q Plot of Residuals')\n",
    "# plt.savefig('data/results/ST50/ST50_final_Q-Q_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Histogram of residuals\n",
    "# fig6, ax6 = plt.subplots(figsize=(10, 7))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(residuals, kde=True, ax=ax6)\n",
    "# plt.xlabel('Residuals')\n",
    "# plt.title('Histogram of Residuals')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc1ae4-c7da-4e45-a281-7fd66b3dca7e",
   "metadata": {},
   "source": [
    "### Feature importance analysis for ST50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56402ef8-035c-433d-b514-148a6be78841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ST50_X_train and ST50_Y_train are your training data\n",
    "\n",
    "# Fit the stacking regressor\n",
    "ST50_stacking_regressor.fit(ST50_X_train, ST50_Y_train)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = ST50_X_train.columns\n",
    "\n",
    "# Initialize an array to store feature importances\n",
    "feature_importances = np.zeros(ST50_X_train.shape[1])\n",
    "\n",
    "# Function to extract feature importances\n",
    "def get_feature_importance(model, X, y):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return np.abs(model.coef_)\n",
    "    elif isinstance(model, CatBoostRegressor):\n",
    "        return model.get_feature_importance()\n",
    "    else:\n",
    "        # Use permutation importance as a fallback for models without direct attribute\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        return result.importances_mean\n",
    "\n",
    "# Aggregate feature importances\n",
    "for name, model in ST50_stacking_regressor.named_estimators_.items():\n",
    "    importances = get_feature_importance(model, ST50_X_train, ST50_Y_train)\n",
    "    feature_importances += importances\n",
    "\n",
    "# Normalize the aggregated feature importances\n",
    "feature_importances /= len(ST50_stacking_regressor.named_estimators_)\n",
    "\n",
    "# Convert importances to percentage\n",
    "feature_importances_percentage = 100 * (feature_importances / np.sum(feature_importances))\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances_percentage\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('ST50 Stacking Regressor Feature Importances')\n",
    "\n",
    "# Add annotations\n",
    "for index, value in enumerate(importance_df['Importance']):\n",
    "    plt.text(value, index, f'{value:.2f}%', va='center')\n",
    "\n",
    "plt.savefig('data/results/ST50/ST50_stacking_regressor_feature_importances.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e54ea-d434-477f-a2f2-eed148a95bf9",
   "metadata": {},
   "source": [
    "### Learning curves for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a43945-d4c2-402c-ac05-10989171143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def plot_learning_curves(model, X_train, Y_train, X_valid, Y_valid, feature):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MAE to positive\n",
    "    train_errors_mae = -train_scores.mean(axis=1)\n",
    "    val_errors_mae = -valid_scores.mean(axis=1)\n",
    "    \n",
    "    train_sizes_mse, train_scores_mse, valid_scores_mse = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MSE to positive\n",
    "    train_errors_mse = -train_scores_mse.mean(axis=1)\n",
    "    val_errors_mse = -valid_scores_mse.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot MAE learning curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_sizes, train_errors_mae, \"r-\", label=\"Training MAE\")\n",
    "    plt.plot(train_sizes, val_errors_mae, \"b-\", label=\"Validation MAE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"MAE Learning Curve\")\n",
    "    \n",
    "    # Plot MSE learning curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_sizes, train_errors_mse, \"r-\", label=\"Training RMSE\")\n",
    "    plt.plot(train_sizes, val_errors_mse, \"b-\", label=\"Validation RMSE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE Learning Curve\")    \n",
    "    plt.savefig('data/results/ST50/ST50_learning_curves.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example call (ensure your data and model are defined correctly)\n",
    "plot_learning_curves(ST50_stacking_regressor, ST50_X_train, ST50_Y_train, ST50_X_valid, ST50_Y_valid, \"ST50\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad9e3a-e302-421c-b6c2-22b86ca0f078",
   "metadata": {},
   "source": [
    "### GridSearhCV Evaluation for all models used in the stacked regressor for ST50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1ad59-b12f-4cc5-a8f9-c5fc2382be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_cb = {\n",
    "    'iterations': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'depth': [4, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 50, 100]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    'max_leaf_nodes': [31, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [20, 50, 100],\n",
    "    'l2_regularization': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [1.0, 0.8, 0.6],\n",
    "    'colsample_bytree': [1.0, 0.8, 0.6],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 0.1, 0.01],\n",
    "    'tree_method': ['gpu_hist']  # Use GPU\n",
    "}\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# stacking_param_grid ={\n",
    "#     'rf__n_estimators': [100, 300, 500],\n",
    "#     'rf__max_depth': [None, 10, 20, 30],\n",
    "#     'hgb__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'hgb__max_iter': [100, 200, 500],\n",
    "#     'catboost__iterations': [100, 200, 500],\n",
    "#     'catboost__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'catboost__depth': [4, 6, 10],\n",
    "#     'xgb__n_estimators': [100, 200],\n",
    "#     'xgb__max_depth': [3, 5]\n",
    "# }\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "ada = AdaBoostRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestRegressor()),\n",
    "#     ('cb', CatBoostRegressor()),\n",
    "#     ('hgb', HistGradientBoostingRegressor()),    \n",
    "#     ('xgb', XGBRegressor())\n",
    "# ]\n",
    "\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=RidgeCV()\n",
    "# )\n",
    "\n",
    "# Initialize GridSearchCV for RF model\n",
    "start_time_rf = time.time()\n",
    "ST50_grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_rf_search = time.time()\n",
    "ST50_grid_search_rf.fit(ST50_X_train, ST50_Y_train)\n",
    "end_time_rf_fit = time.time()\n",
    "elapsed_time_search_rf = end_time_rf_search - start_time_rf\n",
    "elapsed_time_fit_rf = end_time_rf_fit - end_time_rf_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for RandomForestRegressor:\", ST50_grid_search_rf.best_params_)\n",
    "print(\"Best score for RandomForestRegressor:\", -ST50_grid_search_rf.best_score_)\n",
    "print(\"RandomForestRegressor GridSearchCV Time:\", elapsed_time_search_rf)\n",
    "print(\"RandomForestRegressor Fitting Time:\", elapsed_time_fit_rf)\n",
    "\n",
    "# Initialize GridSearchCV for HGB model\n",
    "start_time_hgb = time.time()\n",
    "ST50_grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=param_grid_hgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_hgb_search = time.time()\n",
    "ST50_grid_search_hgb.fit(ST50_X_train, ST50_Y_train)\n",
    "end_time_hgb_fit = time.time()\n",
    "elapsed_time_search_hgb = end_time_hgb_search - start_time_hgb\n",
    "elapsed_time_fit_hgb = end_time_hgb_fit - end_time_hgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for HistGradientBoostingRegressor:\", ST50_grid_search_hgb.best_params_)\n",
    "print(\"Best score for HistGradientBoostingRegressor:\", -ST50_grid_search_hgb.best_score_)\n",
    "print(\"HistGradientBoostingRegressor GridSearchCV Time:\", elapsed_time_search_hgb)\n",
    "print(\"HistGradientBoostingRegressor Fitting Time:\", elapsed_time_fit_hgb)\n",
    "\n",
    "# Initialize GridSearchCV for XGB model\n",
    "start_time_xgb = time.time()\n",
    "ST50_grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_xgb_search = time.time()\n",
    "ST50_grid_search_xgb.fit(ST50_X_train, ST50_Y_train)\n",
    "end_time_xgb_fit = time.time()\n",
    "elapsed_time_search_xgb = end_time_xgb_search - start_time_xgb\n",
    "elapsed_time_fit_xgb = end_time_xgb_fit - end_time_xgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for XGBRegressor:\", ST50_grid_search_xgb.best_params_)\n",
    "print(\"Best score for XGBRegressor:\", -ST50_grid_search_xgb.best_score_)\n",
    "print(\"XGBRegressor GridSearchCV Time:\", elapsed_time_search_xgb)\n",
    "print(\"XGBRegressor Fitting Time:\", elapsed_time_fit_xgb)\n",
    "\n",
    "# Initialize GridSearchCV for ADA model\n",
    "start_time_ada = time.time()\n",
    "ST50_grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_ada_search = time.time()\n",
    "ST50_grid_search_ada.fit(ST50_X_train, ST50_Y_train)\n",
    "end_time_ada_fit = time.time()\n",
    "elapsed_time_search_ada = end_time_ada_search - start_time_ada\n",
    "elapsed_time_fit_ada = end_time_ada_fit - end_time_ada_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for AdaBoostRegressor:\", ST50_grid_search_ada.best_params_)\n",
    "print(\"Best score for AdaBoostRegressor:\", -ST50_grid_search_ada.best_score_)\n",
    "print(\"AdaBoostRegressor GridSearchCV Time:\", elapsed_time_search_ada)\n",
    "print(\"AdaBoostRegressor Fitting Time:\", elapsed_time_fit_ada)\n",
    "\n",
    "# Initialize GridSearchCV for CB model\n",
    "start_time_cb = time.time()\n",
    "ST50_grid_search_cb = GridSearchCV(estimator=cb, param_grid=param_grid_cb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_cb_search = time.time()\n",
    "ST50_grid_search_cb.fit(ST50_X_train, ST50_Y_train)\n",
    "end_time_cb_fit = time.time()\n",
    "elapsed_time_search_cb = end_time_cb_search - start_time_cb\n",
    "elapsed_time_fit_cb = end_time_cb_fit - end_time_cb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for CatBoostRegressor:\", ST50_grid_search_cb.best_params_)\n",
    "print(\"Best score for CatBoost:\", -ST50_grid_search_cb.best_score_)\n",
    "print(\"CatBoostRegressor GridSearchCV Time:\", elapsed_time_search_cb)\n",
    "print(\"CatBoostRegressor Fitting Time:\", elapsed_time_fit_cb)\n",
    "\n",
    "# # Initialize GridSearchCV for Stacking model\n",
    "# start_time_stacking = time.time()\n",
    "# ST50_grid_search_stacking = GridSearchCV(estimator=stacking_regressor, param_grid=stacking_param_grid, cv=5, n_jobs=-1, verbose=0,scoring='neg_mean_absolute_error')\n",
    "# end_time_stacking_search = time.time()\n",
    "# ST50_grid_search_stacking.fit(ST50_X_train, ST50_Y_train)\n",
    "# end_time_stacking = time.time()\n",
    "# elapsed_time_search_stacking = end_time_stacking_search - start_time_stacking\n",
    "# elapsed_time_fit_stacking = end_time_stacking_fit - end_time_stacking_search\n",
    "\n",
    "# print(\"Best parameters for StackingRegressor:\", ST50_grid_search_stacking.best_params_)\n",
    "# print(\"Best score for StackingRegressor:\", -ST50_grid_search_stacking.best_score_)\n",
    "# print(\"StackingRegressor GridSearchCV Time:\", elapsed_time_search_stacking)\n",
    "# print(\"StackingRegressor Fitting Time:\", elapsed_time_fit_stacking)\n",
    "\n",
    "# Define the results of print statements as variables\n",
    "ST50_grid_search_and_fitting_results = {\n",
    "    'Model': ['RandomForestRegressor', 'HistGradientBoostingRegressor', 'AdaBoostRegressor', 'XGBRegressor'],\n",
    "    'Best Parameters': [ST50_grid_search_rf.best_params_, ST50_grid_search_hgb.best_params_, ST50_grid_search_ada.best_params_, ST50_grid_search_xgb.best_params_],\n",
    "    'Best Score': [-ST50_grid_search_rf.best_score_, -ST50_grid_search_hgb.best_score_, -ST50_grid_search_ada.best_score_, -ST50_grid_search_xgb.best_score_],\n",
    "    'GridSearchCV Time': [elapsed_time_search_rf, elapsed_time_search_hgb, elapsed_time_search_ada, elapsed_time_search_xgb],\n",
    "    'Fitting Time': [elapsed_time_fit_rf, elapsed_time_fit_hgb, elapsed_time_fit_ada, elapsed_time_fit_xgb]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_results = pd.DataFrame(ST50_grid_search_and_fitting_results)\n",
    "\n",
    "# Export DataFrame to Excel\n",
    "df_results.to_excel('data/results/ST50/ST50_grid_search_and_fitting_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e70a11-644a-47c9-be19-28b6d67a979c",
   "metadata": {},
   "source": [
    "### F. Stacking Regressor for Soil temperature at 100cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943856d2-583b-4239-9fda-9d4b62228f91",
   "metadata": {},
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ce228-5694-4c82-b125-26fa0bc5d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the covariance matrix for target ST100\n",
    "ST100_dataset_correlation = dataset_denormalized_outlier_filtered.drop(['ST100'], axis=1)\n",
    "ST100_covariance_matrix = ST100_dataset_correlation.cov()\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "ST100_correlation_matrix = ST100_dataset_correlation.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST100_correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=12)\n",
    "plt.savefig(\"data/results/ST100/ST100_denormalized_before_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Set the threshold\n",
    "threshold = 0.95\n",
    "# Find pairs of features with correlation above the threshold\n",
    "highly_correlated = np.where(np.abs(ST100_correlation_matrix) > threshold)\n",
    "highly_correlated_pairs = [(ST100_correlation_matrix.index[x], ST100_correlation_matrix.columns[y]) \n",
    "                           for x, y in zip(*highly_correlated) if x != y and x < y]\n",
    "\n",
    "print(\"Highly correlated pairs (above threshold):\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(pair)\n",
    "# Example: Removing one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for pair in highly_correlated_pairs:\n",
    "    features_to_remove.add(pair[1])  # You can choose to remove pair[0] or pair[1]\n",
    "\n",
    "# Drop the features from the dataframe\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated = ST100_dataset_correlation.drop(columns=features_to_remove)\n",
    "\n",
    "print(f\"Removed features: {features_to_remove}\")\n",
    "print(\"Shape of the reduced dataset:\", ST100_dataset_denormalized_outlier_filtered_uncorrelated.shape)\n",
    "\n",
    "# After removing the correlated features\n",
    "# Calculate the correlation matrix\n",
    "ST100_correlation_matrix_new = ST100_dataset_denormalized_outlier_filtered_uncorrelated.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(ST100_correlation_matrix_new, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=12)\n",
    "plt.savefig(\"data/results/ST100/ST100_denormalized_after_correlation_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Assuming dataset_denormalized_outlier_filtered is your DataFrame\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST100_dataset_denormalized_outlier_filtered_uncorrelated.copy()\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = sm.add_constant(ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(data):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = data.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Iteratively remove features with VIF above the threshold\n",
    "def remove_high_vif_features(data, threshold=40.0):\n",
    "    while True:\n",
    "        vif_data = calculate_vif(data)\n",
    "        max_vif = vif_data['VIF'].max()\n",
    "        if max_vif > threshold:\n",
    "            # Identify the feature with the highest VIF\n",
    "            feature_to_remove = vif_data.sort_values('VIF', ascending=False)['feature'].iloc[0]\n",
    "            print(f\"Removing feature '{feature_to_remove}' with VIF: {max_vif}\")\n",
    "            data = data.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "    return data, vif_data\n",
    "\n",
    "# Remove high VIF features\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif, ST100_final_vif_data = remove_high_vif_features(ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif)\n",
    "\n",
    "print(\"Final VIF data:\")\n",
    "print(ST100_final_vif_data)\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID'] = dataset_denormalized_outlier_filtered['ID']\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST100'] = dataset_denormalized_outlier_filtered['ST100']\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated['ID'] = dataset_denormalized_outlier_filtered['ID']\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated['ST100'] = dataset_denormalized_outlier_filtered['ST100']\n",
    "# Remove the constant term before creating the final DataFrame\n",
    "if 'const' in ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.columns:\n",
    "    ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif = ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(columns=['const'])\n",
    "\n",
    "# Store the 'ID' and 'ST100' columns with their corresponding index before PCA\n",
    "ID_index_mapping = ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ID']\n",
    "ST100_index_mapping = ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif['ST100']\n",
    "\n",
    "# Assume X is your feature dataframe\n",
    "ST100_X_pca = ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif.drop(['ST100', 'ID'], axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(ST100_X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.99)  # Choose the number of components\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "ST100_pca_df = pd.DataFrame(data=principal_components, columns=[f\"PC{i}\" for i in range(principal_components.shape[1])])\n",
    "\n",
    "# Merge PCA DataFrame with original DataFrame to maintain original index order\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ID_index_mapping, ST100_index_mapping, left_index=True, right_index=True)\n",
    "ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca = pd.merge(ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca, ST100_pca_df, left_index=True, right_index=True)\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Explained Variance', fontsize=14)\n",
    "plt.title('Explained Variance by Principal Components', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=12)\n",
    "plt.savefig('data/results/ST100/ST100_PCA_analysis.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c524b2-5924-48cf-b12a-0a599f7f3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_denormalized_outlier_filtered['mean_air_temperature_2m'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826cd35-e9d0-481d-96b9-a43601e869d1",
   "metadata": {},
   "source": [
    "### Option 1:  ST100 Prediction by varying the dataset cases\n",
    "#### Note: Choose the dataset case at this line of the code: dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1)\n",
    "#### Dataset Cases:\n",
    "##### Case 1. dataset_denormalized_outlier_filtered\n",
    "##### case 2. ST100_clean_dataset_denormalized\n",
    "##### case 3. ST100_dataset_denormalized_outlier_filtered_uncorrelated\n",
    "##### case 4. ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif\n",
    "##### case 5. ST100_dataset_denormalized_outlier_filtered_uncorrelated_after_vif_after_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045317e4-4200-40ea-a8f8-5186528674bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR  \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Choose any of the time-independent dataset cases\n",
    "# Select the dataset case here\n",
    "dataset_shuffled = dataset_denormalized_outlier_filtered.sample(frac=1) # Choose dataset case here\n",
    "  \n",
    "# Create function to evaluate model on few different levels\n",
    "def show_scores(model, X_train, X_valid, X_test, Y_train, Y_valid, Y_test, std, target='ST100', model_name='RF'):\n",
    "    \"\"\"\n",
    "    Calculates and shows the different sklearn evaluation metrics\n",
    "        \n",
    "    Parameters:\n",
    "        model: the model fitted.\n",
    "        X_train: the input training set.\n",
    "        X_valid: the input validation or test set.\n",
    "        Y_train: the target training set.\n",
    "        Y_valid: the target validation or test set.\n",
    "            \n",
    "    Returns:\n",
    "        scores: the dictionary of the calculated sklearn metrics for train and valid sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    test_preds = model.predict(X_test)\n",
    "    scores = {\n",
    "              # \"Validation Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Validation Set R^2 Score\":r2_score(Y_test, test_preds),   \n",
    "              # \"Validation Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set MAE\": mean_absolute_error(Y_valid, val_preds), \n",
    "              # \"Validation Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set RMSE\": root_mean_squared_error(Y_valid, val_preds),\n",
    "              # \"Test Set R^2 Score\": r2_score(Y_train, train_preds),\n",
    "              \"Test Set R^2 Score\":r2_score(Y_valid, val_preds),  \n",
    "              # \"Test Set MAE\": mean_absolute_error(Y_train, train_preds),\n",
    "              \"Test Set MAE\": mean_absolute_error(Y_test, test_preds), \n",
    "              # \"Test Set RMSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Tes Set RMSE\": root_mean_squared_error(Y_test, test_preds),\n",
    "              # \"Validation Set MSE\": mean_squared_error(Y_train, train_preds),\n",
    "              \"Validation Set MSE\": mean_squared_error(Y_valid, val_preds),             \n",
    "              # \"Validation Set Median Absolute Error\": median_absolute_error(Y_train, train_preds),\n",
    "              \"Validation Set Median Absolute Error\": median_absolute_error(Y_valid, val_preds),\n",
    "              # \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_train, train_preds),\n",
    "              \"Validation Set MA Percentage Error\": mean_absolute_percentage_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Max Error\": max_error(Y_train, train_preds),\n",
    "              \"Validation Set Max Error\": max_error(Y_valid, val_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_train, train_preds),\n",
    "              # \"Validation Set Explained Variance Score\": explained_variance_score(Y_valid, val_preds)\n",
    "    }\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(scores.items()), columns=['Metric', 'Value'])    \n",
    "    # Export the DataFrame to an Excel file\n",
    "    df.to_excel(f'data/results/{target}/{model_name}_scores.xlsx', index=False)\n",
    "    return scores\n",
    "\n",
    "# Define a function that takes test set and validation sets as input and generates prediction curve and returns test set prediction data \n",
    "def predict_plot(model, ST_X_train, ST_Y_train, ST_X_test, ST_Y_test, ST_X_validation, ST_Y_validation, name, std):\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_train_preds = model.predict(ST_X_train)\n",
    "    # Change train predictions to pandas series\n",
    "    ST_Y_train_preds_series = pd.Series(ST_Y_train_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_train_preds_series.index = ST_Y_train.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_train_sorted = ST_Y_train.sort_values().reset_index(drop=True)\n",
    "    ST_Y_train_preds_sorted = ST_Y_train_preds_series[ST_Y_train.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_train_mae = mean_absolute_error(ST_Y_train, ST_Y_train_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_train_rmse = root_mean_squared_error(ST_Y_train,ST_Y_train_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_train_r2_score = r2_score(ST_Y_train,ST_Y_train_preds)\n",
    "    \n",
    "    # Predict the validation set\n",
    "    ST_Y_validation_preds = model.predict(ST_X_validation)\n",
    "    # Change validation predictions to pandas series\n",
    "    ST_Y_validation_preds_series = pd.Series(ST_Y_validation_preds)\n",
    "    # Make the original and predicted series to have the same index\n",
    "    ST_Y_validation_preds_series.index =ST_Y_validation.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_validation_sorted = ST_Y_validation.sort_values().reset_index(drop=True)\n",
    "    ST_Y_validation_preds_sorted = ST_Y_validation_preds_series[ST_Y_validation.index].sort_values().reset_index(drop=True)\n",
    "  \n",
    "    # Calculate mean absolute error\n",
    "    ST_valid_mae = mean_absolute_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate root mean squared error\n",
    "    ST_valid_rmse = root_mean_squared_error(ST_Y_validation,ST_Y_validation_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_valid_r2_score = r2_score(ST_Y_validation,ST_Y_validation_preds)\n",
    "\n",
    "    # Predict the test set which is forecast data\n",
    "    ST_Y_test_preds = model.predict(ST_X_test)\n",
    "    # Changes the predicted array values to pandas series\n",
    "    ST_Y_test_preds_series = pd.Series(ST_Y_test_preds, name=name) \n",
    "    ST_Y_test_preds_series.index =ST_Y_test.index\n",
    "    # Sort Y_valid and Y_valid_preds in ascending order and reset indices\n",
    "    ST_Y_test_sorted = ST_Y_test.sort_values().reset_index(drop=True)\n",
    "    ST_Y_test_preds_sorted = ST_Y_test_preds_series[ST_Y_test.index].sort_values().reset_index(drop=True)\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    ST_test_mae = mean_absolute_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate mean squared error\n",
    "    ST_test_rmse = root_mean_squared_error(ST_Y_test,ST_Y_test_preds)\n",
    "    # Calculate the R^2 score\n",
    "    ST_test_r2_score = r2_score(ST_Y_test,ST_Y_test_preds)\n",
    "    \n",
    "    # Convert the Series to a DataFrame to return as dataframe\n",
    "    ST_Y_test_preds_df = ST_Y_test_preds_series.to_frame()\n",
    "    ST_Y_test_preds_df.index =  ST_X_test.index\n",
    "\n",
    "\n",
    "     # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_train_sorted.index,ST_Y_train_sorted, color='blue', label=f'{name} Training Observed Values')\n",
    "    plt.plot(ST_Y_train_preds_sorted.index,ST_Y_train_preds_sorted, color='red', label=f'{name} Training Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_train_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_train_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_train_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 100 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Training Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_train_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the validation sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_validation_sorted.index,ST_Y_validation_sorted, color='blue', label=f'{name} Validation Set Observed Values')\n",
    "    plt.plot(ST_Y_validation_preds_sorted.index,ST_Y_validation_preds_sorted, color='red', label=f'{name} Validation Set Predicted Values')\n",
    "    # Display the mean absolute error as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_valid_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_valid_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_valid_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 100 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Validation Set {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_valid_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the test sorted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ST_Y_test_sorted.index,ST_Y_test_sorted, color='blue', label=f'{name} Test Observed Values')\n",
    "    plt.plot(ST_Y_test_preds_sorted.index,ST_Y_test_preds_sorted, color='red', label=f'{name} Test Predicted Values')\n",
    "    # Display the metrics as text annotation\n",
    "    plt.text(0.1, 0.75, f'MAE: {ST_test_mae:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.3, 0.75, f'RMSE: {ST_test_rmse:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.text(0.5, 0.75, f'R^2: {ST_test_r2_score:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel(f'Soil Temperature at 100 cm (°C)', fontsize=14)\n",
    "    plt.title(f'Final Test Scores For {name} Observed vs Predicted Values', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'data/results/{name}_test_set_predicted_vs_Observed_values_line_plot.png', bbox_inches='tight')  # Save as PNG format\n",
    "    plt.show()    \n",
    "    return ST_Y_test_preds_df\n",
    "    \n",
    "std_deviation = dataset_denormalized_outlier_filtered['ST100'].std()\n",
    "# Split the dataset into features and target\n",
    "many_features_dropped =  ['earth_heat_flux_MJ_m2','radiation_balance_w_m2','phosynthetic_active_radiation_mE_m2','albedo_RR_GR','ST100','ID']\n",
    "soil_features_dropped = ['ST100','ID']\n",
    "uncorrelated_dropped = ['ST100','ID']\n",
    "# ST100_X = dataset_shuffled.drop(many_features_dropped, axis=1)\n",
    "ST100_X = dataset_shuffled.drop(soil_features_dropped, axis=1)\n",
    "ST100_Y = dataset_shuffled['ST100']\n",
    "\n",
    "# Split the dataset in to features (independent variables) and labels(dependent variable = target_soil_temperature_2cm ).\n",
    "# Then split into train, validation and test sets\n",
    "train_split = round(0.7*len(dataset_shuffled)) # 70% for train set\n",
    "valid_split = round(train_split + 0.15*len(dataset_shuffled))\n",
    "ST100_X_train, ST100_Y_train = ST100_X[:train_split], ST100_Y[:train_split]\n",
    "ST100_X_valid, ST100_Y_valid =ST100_X[train_split:valid_split], ST100_Y[train_split:valid_split]\n",
    "ST100_X_test, ST100_Y_test = ST100_X[valid_split:], ST100_Y[valid_split:]\n",
    "\n",
    "# A. CatBoostRegressor (CB)\n",
    "# Create CB model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_cb_model_stack = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "# Fit the model for ST100 to start with\n",
    "ST100_cb_model_stack.fit(ST100_X_train, ST100_Y_train, eval_set=(ST100_X_valid, ST100_Y_valid), early_stopping_rounds=100)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================CatBoost The Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST100_cb_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'CB'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# B. RandomForestRegressor\n",
    "# Create RF model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_rf_model_stack = RandomForestRegressor(n_estimators=300, \n",
    "                                         min_samples_leaf=1,\n",
    "                                         min_samples_split=2,\n",
    "                                         max_features='sqrt',\n",
    "                                         max_depth=None,\n",
    "                                         bootstrap=False,\n",
    "                                         random_state=42)\n",
    "# Fit the model for ST100 to start with\n",
    "ST100_rf_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================Random Forest The Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "\n",
    "print(show_scores(ST100_rf_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'RF'))\n",
    "print(\"==================================================================================================\\n\")\n",
    "\n",
    "# C. Histogram Based Gradient Boosting Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_gbr_model_stack = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "# Fit the ST100 model for soil temp at 100 cm\n",
    "ST100_gbr_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Histogram-Based Gradient Boosting Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_gbr_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'HGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# D. XGBoost Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create XGBoost for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_xgb_model_stack = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "# Fit the ST100 model for soil temp at 100 cm\n",
    "ST100_xgb_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The XGBoost Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_xgb_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'XGB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# E. AdaBoostRegressor \n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create AdaBoost Regressor for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_adb_model_stack = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "# Fit the ST100 model for soil temp at 100 cm\n",
    "ST100_adb_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The AdaBoost Regressor Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_adb_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'ADB'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# F. Ridge Regressor\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "# Create Ridge model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_rg_model_stack = Ridge(random_state=42)\n",
    "# Fit the ST100 model for soil temp at 100 cm\n",
    "ST100_rg_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Ridge Regressor Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_rg_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'RR'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# G. Lasso Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create Lasso model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_la_model_stack = Lasso(random_state=42)\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_la_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The Lasso Regressor Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_la_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'LA'))\n",
    "print(\"====================================================================================================\\n\")\n",
    "\n",
    "# H. ElasticNet Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create ElasticNet model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_en_model_stack = ElasticNet(random_state=42)\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_en_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The ElasticNet Regressor Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_en_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'EN'))\n",
    "print(\"=========================================================================================================\\n\")\n",
    "\n",
    "# I. SVR-L Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-L model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_svrl_model_stack = SVR(kernel='linear')\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_svrl_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with linear model Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_svrl_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'SVR-L'))\n",
    "print(\"==========================================================================================================\\n\")\n",
    "\n",
    "# J. SVR-R Regressor\n",
    "# Set up a radom seed\n",
    "np.random.seed(42)\n",
    "# Create SVR-R model for all possible target variables (soil temperature at 2cm, 5cm, 10cm, 20cm, 50cm, 100cm) to be used later\n",
    "ST100_svrr_model_stack = SVR(kernel='rbf')\n",
    "# Fit the ST100 model for soil temp at 100cm\n",
    "ST100_svrr_model_stack.fit(ST100_X_train, ST100_Y_train)\n",
    "# Show the scoring metrics for this model\n",
    "print(\"====================The SVR with rfb model Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_svrr_model_stack, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'SVR-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "\n",
    "\n",
    "# Stack of predictors on a single data set\n",
    "ST100_rf_regressor = RandomForestRegressor(n_estimators=300, \n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_samples_split=2,\n",
    "                                     max_features='sqrt',\n",
    "                                     max_depth=None,\n",
    "                                     bootstrap=False,\n",
    "                                     random_state=42)\n",
    "ST100_gbdt_regresssor = HistGradientBoostingRegressor(learning_rate=0.1, \n",
    "                                              max_iter=300, \n",
    "                                              max_leaf_nodes=41,\n",
    "                                              random_state=42)\n",
    "ST100_xgb_model = XGBRegressor(objective='reg:squarederror',\n",
    "                             learning_rate=0.1, \n",
    "                             max_depth=6, \n",
    "                             n_estimators=200, \n",
    "                             subsample=0.8, \n",
    "                             random_state=42)\n",
    "ST100_cb_regressor = CatBoostRegressor(iterations=500,\n",
    "                                learning_rate=0.1,\n",
    "                                depth=6,\n",
    "                                l2_leaf_reg=3,\n",
    "                                loss_function='RMSE',\n",
    "                                silent=True,\n",
    "                                random_state=42)\n",
    "ST100_adb_regressor = AdaBoostRegressor(learning_rate=0.1, \n",
    "                                  n_estimators=100,\n",
    "                                  random_state=42)\n",
    "\n",
    "estimators = [\n",
    "    (\"RandomForest\", ST100_rf_regressor),\n",
    "    (\"CatBoost\", ST100_cb_regressor),\n",
    "    (\"HistGradientBoosting\", ST100_gbdt_regresssor),\n",
    "    (\"XGBoost\", ST100_xgb_model)\n",
    "]\n",
    "ST100_stacking_regressor = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\n",
    "\n",
    "# Measure and plot the results\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for ax, (name, est) in zip(axs, estimators + [(\"Stacking Regressor\", ST5_stacking_regressor)]):\n",
    "    scorers = {\"R^2\": \"r2\", \"MAE\": \"neg_mean_absolute_error\", \"RMSE\": \"neg_root_mean_squared_error\"}\n",
    "\n",
    "    start_time = time.time()\n",
    "    scores = cross_validate(est, ST100_X_train, ST100_Y_train, scoring=list(scorers.values()), n_jobs=-1, verbose=0)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_pred = cross_val_predict(est, ST100_X_valid, ST100_Y_valid, n_jobs=-1, verbose=0)\n",
    "    y_test = cross_val_predict(est, ST100_X_test, ST100_Y_test, n_jobs=-1, verbose=0)\n",
    "    \n",
    "    # Calculate mean and std_deviation for each scorer\n",
    "    scores_mean_std = {\n",
    "        key: (np.abs(np.mean(scores[f'test_{value}'])), np.std(scores[f'test_{value}']))\n",
    "        for key, value in scorers.items()\n",
    "    }\n",
    "\n",
    "    # Format the scores\n",
    "    formatted_scores = {\n",
    "        key: f\"{mean:.4f} ± {std_dev:.4f}\"\n",
    "        for key, (mean, std_dev) in scores_mean_std.items()\n",
    "    }\n",
    "\n",
    "    display = PredictionErrorDisplay.from_predictions(\n",
    "        y_true=ST100_Y_valid,\n",
    "        y_pred=y_pred,\n",
    "        kind=\"actual_vs_predicted\",\n",
    "        ax=ax,\n",
    "        scatter_kwargs={\"alpha\": 0.2, \"color\": \"tab:blue\"},\n",
    "        line_kwargs={\"color\": \"tab:red\"},\n",
    "    )\n",
    "    ax.set_title(f\"{name}\\nEvaluation in {elapsed_time:.4f} seconds\", fontsize=14)\n",
    "    # Set custom x-label and y-label\n",
    "    ax.set_xlabel(\"Predicted Soil Temperature at 100 cm (°C)\", fontsize=14)\n",
    "    ax.set_ylabel(\"Observed Soil Temperature at 100 cm (°C)\", fontsize=14)\n",
    "\n",
    "    for metric_name, (mean, std_dev) in scores_mean_std.items():\n",
    "        if metric_name == 'R^2':\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {formatted_scores[metric_name]}\")\n",
    "        else:\n",
    "            ax.plot([], [], \" \", label=f\"{metric_name}: {mean:.4f} ± {std_dev:.4f}\")\n",
    "    \n",
    "    ax.legend(loc=\"best\", fontsize='small')\n",
    "    # Save the mean and std scores to an Excel file\n",
    "    df_scores_summary = pd.DataFrame(scores_mean_std).T\n",
    "    df_scores_summary.columns = ['Train Mean', 'Train Std Dev']\n",
    "    df_scores_summary.to_excel(f'data/results/ST100/{name}_cv_scores.xlsx', index=True)\n",
    "# Hide any unused subplots\n",
    "for i in range(len(estimators)+1, len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "# Apply tight layout\n",
    "plt.tight_layout()\n",
    "# Save the entire figure with all subplots to a file\n",
    "fig.savefig('data/results/ST100/stacked_regressors_prediction_error_plots.png', bbox_inches='tight')\n",
    "\n",
    "# Sort actual values and get sorted indices\n",
    "ST100_Y_valid_sorted = ST100_Y_valid.sort_values()\n",
    "sorted_indices = ST100_Y_valid_sorted.index\n",
    "\n",
    "# Reorder y_pred using the sorted indices\n",
    "y_pred_sorted = pd.Series(y_pred, index=ST100_Y_valid.index).loc[sorted_indices]\n",
    "\n",
    "# Calculate metrics for the validation set predictions\n",
    "mae_valid = mean_absolute_error(ST100_Y_valid, y_pred)\n",
    "rmse_valid = np.sqrt(mean_squared_error(ST100_Y_valid, y_pred))\n",
    "r2_valid = r2_score(ST100_Y_valid, y_pred)\n",
    "\n",
    "# Calculate metrics for the test set predictions\n",
    "mae_test = mean_absolute_error(ST100_Y_test, y_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(ST100_Y_test, y_test))\n",
    "r2_test = r2_score(ST100_Y_test, y_test)\n",
    "\n",
    "# Save the validation metrics to an Excel file\n",
    "validation_test_metrics = {\n",
    "    'V_R^2': [r2_valid],\n",
    "    'V_MAE': [mae_valid],\n",
    "    'V_RMSE': [rmse_valid],\n",
    "    'T_R^2': [r2_test],\n",
    "    'T_MAE': [mae_test],\n",
    "    'T_RMSE': [rmse_test]    \n",
    "}\n",
    "# Save the validation metrics to an Excel file\n",
    "\n",
    "df_metrics = pd.DataFrame(validation_test_metrics)\n",
    "df_metrics.to_excel(f'data/results/ST100/{name}_validation_test_metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the sorted actual values and corresponding predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ST100_Y_valid_sorted.values, color='blue', label='Observed Values')\n",
    "plt.plot(y_pred_sorted.values, color='red', label='Predicted Values')\n",
    "\n",
    "# Display the metrics as text annotation\n",
    "plt.text(0.1, 0.75, f'MAE: {mae_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.3, 0.75, f'RMSE: {rmse_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(0.5, 0.75, f'R^2: {r2_valid:.4f}', ha='center', va='center', transform=plt.gca().transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel('Index', fontsize=14)\n",
    "plt.ylabel('Soil Temperature at 100 cm (°C)', fontsize=14)\n",
    "plt.title(f'STACK-R model\\'s validation set\\'s predicted vs observed values for ST100', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'data/results/ST100/{name}_cross_validation_predicted_vs_observed_values_line_plot.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "dump(ST100_stacking_regressor, filename=\"data/results/ST100/ST100_STACK-R_trained_model.joblib\");\n",
    "# Fit the stacking regressor for direct fitting and prediction for all sets at default CV=5\n",
    "ST100_stacking_regressor.fit(ST100_X_train, ST100_Y_train)\n",
    "print(\"====================The Stacking Regressor Evaluation Metrics Results For ST100 Denormalized =======================\\n\")\n",
    "print(show_scores(ST100_stacking_regressor, ST100_X_train, ST100_X_valid, ST100_X_test, ST100_Y_train, ST100_Y_valid, ST100_Y_test, std_deviation,'ST100', 'STACK-R'))\n",
    "print(\"=======================================================================================================\\n\")\n",
    "# ST100_Y_test_preds_df = predict_plot(ST100_stacking_regressor, ST100_X_train, ST100_Y_train, ST100_X_test, ST100_Y_test, ST100_X_valid, ST100_Y_valid, 'ST100', std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de1862-03fe-449e-9552-20a29115e225",
   "metadata": {},
   "source": [
    "### Cross-validation to check stability of the stacking regressor for ST100\n",
    "### NOTE: This will take time!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b132726-6f63-4b57-9c47-b0568d6a7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "ST100_cv_scores = cross_val_score(ST100_stacking_regressor, ST100_X_train, ST100_Y_train, cv=kf, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive\n",
    "ST100_cv_scores = -ST100_cv_scores\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-Validation Scores (MSE):\", ST100_cv_scores)\n",
    "print(\"Mean CV Score (MSE):\", np.mean(ST100_cv_scores))\n",
    "print(\"Standard Deviation of CV Scores:\", np.std(ST100_cv_scores))\n",
    "# Save the scores to an Excel file\n",
    "ST100_cv_scores_df = pd.DataFrame(ST100_cv_scores, columns=['MSE'])\n",
    "ST100_cv_scores_df.to_excel('data/results/ST100/ST100_10_fold_cv_scores.xlsx', index=False)\n",
    "\n",
    "##=========== Visualize the problematic Fold using histogram==================\n",
    "# Calculate mean MSE\n",
    "ST100_mean_mse = np.mean(ST100_cv_scores)\n",
    "# Identify the problematic fold\n",
    "ST100_problematic_fold_index = np.argmax(np.abs(ST100_cv_scores - ST100_mean_mse))\n",
    "# Get the indices of the data points in the problematic fold\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(ST100_X_train)):\n",
    "    if fold_index == ST100_problematic_fold_index:\n",
    "        problematic_fold_train_indices = train_index\n",
    "        problematic_fold_test_indices = test_index\n",
    "\n",
    "# Subset the data for the problematic fold\n",
    "X_problematic_fold = ST100_X_train.iloc[problematic_fold_test_indices]\n",
    "y_problematic_fold = ST100_Y_train.iloc[problematic_fold_test_indices]\n",
    "# Visualize or analyze features for the problematic fold\n",
    "for feature in ST100_X_train.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Histogram for the problematic fold\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(X_problematic_fold[feature], kde=True, bins=20, color='red')\n",
    "    plt.title(f'{feature} - Problematic Fold')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    # Histogram for the entire dataset\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(ST100_X_train[feature], kde=True, bins=20, color='blue')\n",
    "    plt.title(f'{feature} - Entire Dataset')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/results/ST100/ST100_CV_problematic_10_fold_vs_main_dataset_histograms.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da1fc4-594c-4624-89c9-c2b8687cce2b",
   "metadata": {},
   "source": [
    "### Partial Dependence, Individual Conditional Expectation and Residual Analysis Plots for ST100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a99e9e-f9ab-437a-a49d-c07ff28b1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import statsmodels.api as sm\n",
    "from pycebox.ice import ice, ice_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# # Partial Dependence Plot\n",
    "# print('====================================================== ST100 Partial Dependence Plot')\n",
    "# ST100_feature_names = ST100_X_train.columns.tolist()\n",
    "# n_features = len(ST100_feature_names)\n",
    "# n_cols = 2\n",
    "# n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "# fig1, ax1 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat = ax1.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST100_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST100_stacking_regressor, ST100_X_train, features=[feature])\n",
    "#     display.plot(ax=axes_flat[idx])\n",
    "#     axes_flat[idx].set_title(f'Partial Dependence (PD) Plot for {feature}')\n",
    "#     axes_flat[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat)):\n",
    "#     fig1.delaxes(axes_flat[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "# plt.suptitle('Partial Dependence (PD) Plot', fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# # plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST100/ST100_final_partial_dependence_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Individual Conditional Expectation Plot (ICE)\n",
    "# print('======================== ST100 Individual Conditional Expectation Plot ===============================')\n",
    "# fig2, axes2 = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 15))\n",
    "# axes_flat2 = axes2.flatten()\n",
    "\n",
    "# for idx, feature in enumerate(ST100_feature_names):\n",
    "#     display = PartialDependenceDisplay.from_estimator(ST100_stacking_regressor, ST100_X_train, features=[feature], kind='individual')\n",
    "#     display.plot(ax=axes_flat2[idx])\n",
    "#     axes_flat2[idx].set_title(f'Individual Conditional Expectation (ICE) Plot for {feature}')\n",
    "#     axes_flat2[idx].set_xlabel(feature)\n",
    "\n",
    "# for idx in range(n_features, len(axes_flat2)):\n",
    "#     fig2.delaxes(axes_flat2[idx])\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.7)\n",
    "# plt.suptitle('Individual Conditional Expectation (ICE) Plot', fontsize=16)\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# plt.savefig('data/results/ST100/ST100_final_individual_conditional_expectation_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# Residual Analysis\n",
    "print('========================= ST100 Residual Analysis Plot ==============================')\n",
    "ST100_Y_predictions = ST100_stacking_regressor.predict(ST100_X_test)\n",
    "ST100_residuals = ST100_Y_test - ST100_Y_predictions\n",
    "\n",
    "# Calculate the interquartile range (IQR)\n",
    "Q1 = np.percentile(ST100_residuals, 25)\n",
    "Q3 = np.percentile(ST100_residuals, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the whisker range\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Count outliers\n",
    "outliers = np.sum((ST100_residuals < lower_bound) | (ST100_residuals > upper_bound))\n",
    "total_residuals = len(ST100_residuals)\n",
    "outlier_percentage = (outliers / total_residuals) * 100\n",
    "\n",
    "# Plotting the residuals scatter plot and box-and-whisker plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Residuals analysis plot\n",
    "ax[0].scatter(ST100_Y_predictions, ST100_residuals)\n",
    "ax[0].set_xlabel('Predictions', fontsize=14)\n",
    "ax[0].set_ylabel('Residuals', fontsize=14)\n",
    "ax[0].set_title('ST100 Residuals Analysis Plot', fontsize=14)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "ax[0].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "# Box-and-whisker plot for residuals\n",
    "sns.boxplot(y=ST100_residuals, ax=ax[1])\n",
    "ax[1].set_title('ST100 Box-and-Whisker Plot of Residuals', fontsize=14)\n",
    "ax[1].set_ylabel('ST100 Residuals', fontsize=14)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# Annotate the plot with the number of outliers and total residuals\n",
    "annotation_text = (f'Total Residuals: {total_residuals}\\n'\n",
    "                   f'Number of Outliers: {outliers}\\n'\n",
    "                   f'Percentage of Outliers: {outlier_percentage:.2f}%')\n",
    "ax[1].annotate(annotation_text, xy=(0.8, 0.87), xycoords='axes fraction',\n",
    "               fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST100/ST100_final_residual_and_boxplot_analysis.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predictor Variables\n",
    "print('========================= Residuals vs. Predictor Variables ==============================')\n",
    "for column in ST100_X_test.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.scatter(ST100_X_test[column], ST100_residuals)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel(column, fontsize=14)\n",
    "    ax.set_ylabel('Residuals', fontsize=14)\n",
    "    ax.set_title(f'Residuals vs. {column}', fontsize=14)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data/results/ST100_final_residuals_vs_{column}.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the Box Plot of all features\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Number of features in the DataFrame\n",
    "num_features = dataset_denormalized_outlier_filtered.shape[1]\n",
    "# Calculate the number of rows needed to plot all features in 3 columns\n",
    "num_cols = 3\n",
    "num_rows = math.ceil(num_features / num_cols)\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, num_rows * 5))\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "# Define colors for each column\n",
    "colors = ['green', 'purple', 'red']\n",
    "# Create a Box Plot for each feature\n",
    "for i, column in enumerate(dataset_denormalized_outlier_filtered.columns):\n",
    "    col_index = i % num_cols  # Determine the column index (0, 1, or 2)\n",
    "    sns.boxplot(data=dataset_denormalized_outlier_filtered[column], ax=axes[i], color=colors[col_index])\n",
    "    axes[i].set_title(f'Box Plot for {column}', fontsize=14)\n",
    "    axes[i].set_xlabel('Values', fontsize=14)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=14)\n",
    "    axes[i].tick_params(axis='both', which='minor', labelsize=12)\n",
    "# Remove any empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/results/ST100/ST100_Box_plot_of_features.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# # Q-Q Plot\n",
    "# print('========================= ST100 Q-Q Plot ==============================')\n",
    "# fig5, ax5 = plt.subplots(figsize=(10, 7))\n",
    "# sm.qqplot(ST100_residuals, line='45', ax=ax5)\n",
    "# ax5.set_title('Q-Q Plot of Residuals')\n",
    "# plt.savefig('data/results/ST100/ST100_final_Q-Q_plot.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# # Histogram of residuals\n",
    "# fig6, ax6 = plt.subplots(figsize=(10, 7))\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(residuals, kde=True, ax=ax6)\n",
    "# plt.xlabel('Residuals')\n",
    "# plt.title('Histogram of Residuals')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff63f6-7272-4160-8016-b8a15ed9b0eb",
   "metadata": {},
   "source": [
    "### Feature importance analysis for ST100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa6cff-750e-4e10-963f-8524a0070454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ST100_X_train and ST100_Y_train are your training data\n",
    "\n",
    "# Fit the stacking regressor\n",
    "ST100_stacking_regressor.fit(ST100_X_train, ST100_Y_train)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = ST100_X_train.columns\n",
    "\n",
    "# Initialize an array to store feature importances\n",
    "feature_importances = np.zeros(ST100_X_train.shape[1])\n",
    "\n",
    "# Function to extract feature importances\n",
    "def get_feature_importance(model, X, y):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return np.abs(model.coef_)\n",
    "    elif isinstance(model, CatBoostRegressor):\n",
    "        return model.get_feature_importance()\n",
    "    else:\n",
    "        # Use permutation importance as a fallback for models without direct attribute\n",
    "        result = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        return result.importances_mean\n",
    "\n",
    "# Aggregate feature importances\n",
    "for name, model in ST100_stacking_regressor.named_estimators_.items():\n",
    "    importances = get_feature_importance(model, ST100_X_train, ST100_Y_train)\n",
    "    feature_importances += importances\n",
    "\n",
    "# Normalize the aggregated feature importances\n",
    "feature_importances /= len(ST100_stacking_regressor.named_estimators_)\n",
    "\n",
    "# Convert importances to percentage\n",
    "feature_importances_percentage = 100 * (feature_importances / np.sum(feature_importances))\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances_percentage\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "plt.title('ST100 Stacking Regressor Feature Importances')\n",
    "\n",
    "# Add annotations\n",
    "for index, value in enumerate(importance_df['Importance']):\n",
    "    plt.text(value, index, f'{value:.2f}%', va='center')\n",
    "\n",
    "plt.savefig('data/results/ST100/ST100_stacking_regressor_feature_importances.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88868572-4daf-4dfe-87fb-2378d0aa11f5",
   "metadata": {},
   "source": [
    "### Learning curves for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9938ed-8a71-4ea9-95f5-1f67954ec440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def plot_learning_curves(model, X_train, Y_train, X_valid, Y_valid, feature):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MAE to positive\n",
    "    train_errors_mae = -train_scores.mean(axis=1)\n",
    "    val_errors_mae = -valid_scores.mean(axis=1)\n",
    "    \n",
    "    train_sizes_mse, train_scores_mse, valid_scores_mse = learning_curve(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=Y_train,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert negative MSE to positive\n",
    "    train_errors_mse = -train_scores_mse.mean(axis=1)\n",
    "    val_errors_mse = -valid_scores_mse.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot MAE learning curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_sizes, train_errors_mae, \"r-\", label=\"Training MAE\")\n",
    "    plt.plot(train_sizes, val_errors_mae, \"b-\", label=\"Validation MAE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.title(\"MAE Learning Curve\")\n",
    "    \n",
    "    # Plot MSE learning curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_sizes, train_errors_mse, \"r-\", label=\"Training RMSE\")\n",
    "    plt.plot(train_sizes, val_errors_mse, \"b-\", label=\"Validation RMSE\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(f\"{feature} Training set size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"RMSE Learning Curve\")    \n",
    "    plt.savefig('data/results/ST100/ST100_learning_curves.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example call (ensure your data and model are defined correctly)\n",
    "plot_learning_curves(ST100_stacking_regressor, ST100_X_train, ST100_Y_train, ST100_X_valid, ST100_Y_valid, \"ST100\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1454a-c1c5-452f-b8b1-373ea6ef4931",
   "metadata": {},
   "source": [
    "### GridSearhCV Evaluation for all models used in the stacked regressor for ST100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be4936c-9f13-4039-88ac-b3eae8c90120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grid_cb = {\n",
    "    'iterations': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'depth': [4, 6, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 50, 100]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_hgb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    'max_leaf_nodes': [31, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [20, 50, 100],\n",
    "    'l2_regularization': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [1.0, 0.8, 0.6],\n",
    "    'colsample_bytree': [1.0, 0.8, 0.6],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 0.1, 0.01],\n",
    "    'tree_method': ['gpu_hist']  # Use GPU\n",
    "}\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "# stacking_param_grid ={\n",
    "#     'rf__n_estimators': [100, 300, 500],\n",
    "#     'rf__max_depth': [None, 10, 20, 30],\n",
    "#     'hgb__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'hgb__max_iter': [100, 200, 500],\n",
    "#     'catboost__iterations': [100, 200, 500],\n",
    "#     'catboost__learning_rate': [0.01, 0.1, 0.05],\n",
    "#     'catboost__depth': [4, 6, 10],\n",
    "#     'xgb__n_estimators': [100, 200],\n",
    "#     'xgb__max_depth': [3, 5]\n",
    "# }\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "cb = CatBoostRegressor(random_state=42)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "ada = AdaBoostRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestRegressor()),\n",
    "#     ('cb', CatBoostRegressor()),\n",
    "#     ('hgb', HistGradientBoostingRegressor()),    \n",
    "#     ('xgb', XGBRegressor())\n",
    "# ]\n",
    "\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=estimators,\n",
    "#     final_estimator=RidgeCV()\n",
    "# )\n",
    "\n",
    "# Initialize GridSearchCV for RF model\n",
    "start_time_rf = time.time()\n",
    "ST100_grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_rf_search = time.time()\n",
    "ST100_grid_search_rf.fit(ST100_X_train, ST100_Y_train)\n",
    "end_time_rf_fit = time.time()\n",
    "elapsed_time_search_rf = end_time_rf_search - start_time_rf\n",
    "elapsed_time_fit_rf = end_time_rf_fit - end_time_rf_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for RandomForestRegressor:\", ST100_grid_search_rf.best_params_)\n",
    "print(\"Best score for RandomForestRegressor:\", -ST100_grid_search_rf.best_score_)\n",
    "print(\"RandomForestRegressor GridSearchCV Time:\", elapsed_time_search_rf)\n",
    "print(\"RandomForestRegressor Fitting Time:\", elapsed_time_fit_rf)\n",
    "\n",
    "# Initialize GridSearchCV for HGB model\n",
    "start_time_hgb = time.time()\n",
    "ST100_grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=param_grid_hgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_hgb_search = time.time()\n",
    "ST100_grid_search_hgb.fit(ST100_X_train, ST100_Y_train)\n",
    "end_time_hgb_fit = time.time()\n",
    "elapsed_time_search_hgb = end_time_hgb_search - start_time_hgb\n",
    "elapsed_time_fit_hgb = end_time_hgb_fit - end_time_hgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for HistGradientBoostingRegressor:\", ST100_grid_search_hgb.best_params_)\n",
    "print(\"Best score for HistGradientBoostingRegressor:\", -ST100_grid_search_hgb.best_score_)\n",
    "print(\"HistGradientBoostingRegressor GridSearchCV Time:\", elapsed_time_search_hgb)\n",
    "print(\"HistGradientBoostingRegressor Fitting Time:\", elapsed_time_fit_hgb)\n",
    "\n",
    "# Initialize GridSearchCV for XGB model\n",
    "start_time_xgb = time.time()\n",
    "ST100_grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_xgb_search = time.time()\n",
    "ST100_grid_search_xgb.fit(ST100_X_train, ST100_Y_train)\n",
    "end_time_xgb_fit = time.time()\n",
    "elapsed_time_search_xgb = end_time_xgb_search - start_time_xgb\n",
    "elapsed_time_fit_xgb = end_time_xgb_fit - end_time_xgb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for XGBRegressor:\", ST100_grid_search_xgb.best_params_)\n",
    "print(\"Best score for XGBRegressor:\", -ST100_grid_search_xgb.best_score_)\n",
    "print(\"XGBRegressor GridSearchCV Time:\", elapsed_time_search_xgb)\n",
    "print(\"XGBRegressor Fitting Time:\", elapsed_time_fit_xgb)\n",
    "\n",
    "# Initialize GridSearchCV for ADA model\n",
    "start_time_ada = time.time()\n",
    "ST100_grid_search_ada = GridSearchCV(estimator=ada, param_grid=param_grid_ada, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_ada_search = time.time()\n",
    "ST100_grid_search_ada.fit(ST100_X_train, ST100_Y_train)\n",
    "end_time_ada_fit = time.time()\n",
    "elapsed_time_search_ada = end_time_ada_search - start_time_ada\n",
    "elapsed_time_fit_ada = end_time_ada_fit - end_time_ada_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for AdaBoostRegressor:\", ST100_grid_search_ada.best_params_)\n",
    "print(\"Best score for AdaBoostRegressor:\", -ST100_grid_search_ada.best_score_)\n",
    "print(\"AdaBoostRegressor GridSearchCV Time:\", elapsed_time_search_ada)\n",
    "print(\"AdaBoostRegressor Fitting Time:\", elapsed_time_fit_ada)\n",
    "\n",
    "# Initialize GridSearchCV for CB model\n",
    "start_time_cb = time.time()\n",
    "ST100_grid_search_cb = GridSearchCV(estimator=cb, param_grid=param_grid_cb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "end_time_cb_search = time.time()\n",
    "ST100_grid_search_cb.fit(ST100_X_train, ST100_Y_train)\n",
    "end_time_cb_fit = time.time()\n",
    "elapsed_time_search_cb = end_time_cb_search - start_time_cb\n",
    "elapsed_time_fit_cb = end_time_cb_fit - end_time_cb_search\n",
    "# Get the best parameters and scores\n",
    "print(\"Best parameters for CatBoostRegressor:\", ST100_grid_search_cb.best_params_)\n",
    "print(\"Best score for CatBoost:\", -ST100_grid_search_cb.best_score_)\n",
    "print(\"CatBoostRegressor GridSearchCV Time:\", elapsed_time_search_cb)\n",
    "print(\"CatBoostRegressor Fitting Time:\", elapsed_time_fit_cb)\n",
    "\n",
    "# # Initialize GridSearchCV for Stacking model\n",
    "# start_time_stacking = time.time()\n",
    "# ST100_grid_search_stacking = GridSearchCV(estimator=stacking_regressor, param_grid=stacking_param_grid, cv=5, n_jobs=-1, verbose=0,scoring='neg_mean_absolute_error')\n",
    "# end_time_stacking_search = time.time()\n",
    "# ST100_grid_search_stacking.fit(ST100_X_train, ST100_Y_train)\n",
    "# end_time_stacking = time.time()\n",
    "# elapsed_time_search_stacking = end_time_stacking_search - start_time_stacking\n",
    "# elapsed_time_fit_stacking = end_time_stacking_fit - end_time_stacking_search\n",
    "\n",
    "# print(\"Best parameters for StackingRegressor:\", ST100_grid_search_stacking.best_params_)\n",
    "# print(\"Best score for StackingRegressor:\", -ST100_grid_search_stacking.best_score_)\n",
    "# print(\"StackingRegressor GridSearchCV Time:\", elapsed_time_search_stacking)\n",
    "# print(\"StackingRegressor Fitting Time:\", elapsed_time_fit_stacking)\n",
    "\n",
    "# Define the results of print statements as variables\n",
    "ST100_grid_search_and_fitting_results = {\n",
    "    'Model': ['RandomForestRegressor', 'HistGradientBoostingRegressor', 'AdaBoostRegressor', 'XGBRegressor'],\n",
    "    'Best Parameters': [ST100_grid_search_rf.best_params_, ST100_grid_search_hgb.best_params_, ST100_grid_search_ada.best_params_, ST100_grid_search_xgb.best_params_],\n",
    "    'Best Score': [-ST100_grid_search_rf.best_score_, -ST100_grid_search_hgb.best_score_, -ST100_grid_search_ada.best_score_, -ST100_grid_search_xgb.best_score_],\n",
    "    'GridSearchCV Time': [elapsed_time_search_rf, elapsed_time_search_hgb, elapsed_time_search_ada, elapsed_time_search_xgb],\n",
    "    'Fitting Time': [elapsed_time_fit_rf, elapsed_time_fit_hgb, elapsed_time_fit_ada, elapsed_time_fit_xgb]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_results = pd.DataFrame(ST100_grid_search_and_fitting_results)\n",
    "\n",
    "# Export DataFrame to Excel\n",
    "df_results.to_excel('data/results/ST100/ST100_grid_search_and_fitting_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a9b53-f4d2-441b-84dc-4ca079da1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > all_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3439300-aa87-4783-90a5-7e0b4878aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read all_requirements.txt and filter out non-standard lines\n",
    "# with open('all_requirements.txt', 'r') as infile, open('requirements.txt', 'w') as outfile:\n",
    "#     for line in infile:\n",
    "#         # Only write lines that match the standard format\n",
    "#         if '==' in line and '@' not in line:\n",
    "#             outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e848f6-9089-47b4-9e40-77bcc79a6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575437f9-58d5-499c-a3c2-e17a081b377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize GridSearchCV for RF model\n",
    "# start_time_rf = time.time()\n",
    "# ST2_grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "# end_time_rf_search = time.time()\n",
    "# ST2_grid_search_rf.fit(ST2_X_train, ST2_Y_train)\n",
    "# end_time_rf_fit = time.time()\n",
    "# elapsed_time_search_rf = end_time_rf_search - start_time_rf\n",
    "# elapsed_time_fit_rf = end_time_rf_fit - end_time_rf_search\n",
    "# # Get the best parameters and scores\n",
    "# print(\"Best parameters for RandomForestRegressor:\", ST2_grid_search_rf.best_params_)\n",
    "# print(\"Best score for RandomForestRegressor:\", -ST2_grid_search_rf.best_score_)\n",
    "# print(\"RandomForestRegressor GridSearchCV Time:\", elapsed_time_search_rf)\n",
    "# print(\"RandomForestRegressor Fitting Time:\", elapsed_time_fit_rf)\n",
    "\n",
    "# # Initialize GridSearchCV for HGB model\n",
    "# start_time_hgb = time.time()\n",
    "# ST2_grid_search_hgb = GridSearchCV(estimator=hgb, param_grid=param_grid_hgb, cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "# end_time_hgb_search = time.time()\n",
    "# ST2_grid_search_hgb.fit(ST2_X_train, ST2_Y_train)\n",
    "# end_time_hgb_fit = time.time()\n",
    "# elapsed_time_search_hgb = end_time_hgb_search - start_time_hgb\n",
    "# elapsed_time_fit_hgb = end_time_hgb_fit - end_time_hgb_search\n",
    "# # Get the best parameters and scores\n",
    "# print(\"Best parameters for HistGradientBoostingRegressor:\", ST2_grid_search_hgb.best_params_)\n",
    "# print(\"Best score for HistGradientBoostingRegressor:\", -ST2_grid_search_hgb.best_score_)\n",
    "# print(\"HistGradientBoostingRegressor GridSearchCV Time:\", elapsed_time_search_hgb)\n",
    "# print(\"HistGradientBoostingRegressor Fitting Time:\", elapsed_time_fit_hgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92ee50-851a-4d0f-b812-bb9c635aa967",
   "metadata": {},
   "source": [
    "### Training and Test Scores variation with training data size ( Learning curve) for ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede930f-7eee-491f-985d-4d6cf0148cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    ST2_stacking_regressor, ST2_X_train, ST2_Y_train)\n",
    "display = LearningCurveDisplay(train_sizes=train_sizes,\n",
    "    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
    "display.plot()\n",
    "plt.savefig('data/results/ST2/ST2_learning_curve_score.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8d112-bc3c-411f-aaac-aa2555211b6d",
   "metadata": {},
   "source": [
    "### Training and Test Scores variation with training data size ( Learning curve) for ST5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac5444-a3f4-4018-b52c-712f9f01e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    ST5_stacking_regressor, ST5_X_train, ST5_Y_train)\n",
    "display = LearningCurveDisplay(train_sizes=train_sizes,\n",
    "    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
    "display.plot()\n",
    "plt.savefig('data/results/ST5/ST5_learning_curve_score.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc7b3e-7744-4555-a719-ba765318ad1c",
   "metadata": {},
   "source": [
    "### Training and Test Scores variation with training data size ( Learning curve) for ST10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0e00c-fa50-4e57-b303-7ae9d69951fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    ST10_stacking_regressor, ST10_X_train, ST10_Y_train)\n",
    "display = LearningCurveDisplay(train_sizes=train_sizes,\n",
    "    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
    "display.plot()\n",
    "plt.savefig('data/results/ST10/ST10_learning_curve_score.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2a920-e18b-4e91-8b36-ae33f8287cbf",
   "metadata": {},
   "source": [
    "### Training and Test Scores variation with training data size ( Learning curve) for ST20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730da46-db04-44d7-8bc7-4a6e6ed0228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    ST20_stacking_regressor, ST20_X_train, ST20_Y_train)\n",
    "display = LearningCurveDisplay(train_sizes=train_sizes,\n",
    "    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
    "display.plot()\n",
    "plt.savefig('data/results/ST20/ST20_learning_curve_score.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2656b-b3ea-4cb3-bb2c-7ee1fce8cd52",
   "metadata": {},
   "source": [
    "### Training and Test Scores variation with training data size ( Learning curve) for ST50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc9f23-ba2c-47cc-b4b5-048dab9c3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    ST50_stacking_regressor, ST50_X_train, ST50_Y_train)\n",
    "display = LearningCurveDisplay(train_sizes=train_sizes,\n",
    "    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
    "display.plot()\n",
    "plt.savefig('data/results/ST50/ST50_learning_curve_score.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e92565-c006-4f52-82ff-0a1118315a51",
   "metadata": {},
   "source": [
    "### Training and Test Scores variation with training data size ( Learning curve) for ST100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d36760-a433-4144-8e18-0b45fee57efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    ST100_stacking_regressor, ST100_X_train, ST100_Y_train)\n",
    "display = LearningCurveDisplay(train_sizes=train_sizes,\n",
    "    train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
    "display.plot()\n",
    "plt.savefig('data/results/ST100/ST100_learning_curve_score.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7f083-d723-4de7-89fa-26bbd160fbc9",
   "metadata": {},
   "source": [
    "### ST50 and ST100 time series relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91d389-a9a4-4eb0-948a-3eb8bcbe8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(dataset['date'], dataset['ST50'], label='ST50', alpha=0.5)\n",
    "plt.scatter(dataset['date'], dataset['ST100'], label='ST100', alpha=0.5)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('ST50/ST100 Soil Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.savefig('data/results/ST50_vs_ST100_time_series.png', bbox_inches='tight')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
